<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>天道酬勤，厚德载物</title>
  
  
  <link href="https://cuiluyi.gitee.io/atom.xml" rel="self"/>
  
  <link href="https://cuiluyi.gitee.io/"/>
  <updated>2024-08-20T05:49:06.519Z</updated>
  <id>https://cuiluyi.gitee.io/</id>
  
  <author>
    <name>银杏枫树</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.gitee.io/2024/08/20/[object%20Object]/%E4%BF%9D%E7%A0%94/%E3%80%90%E4%BF%9D%E7%A0%94%E5%A4%A9%E6%A2%AF%E3%80%91/"/>
    <id>https://cuiluyi.gitee.io/2024/08/20/[object%20Object]/%E4%BF%9D%E7%A0%94/%E3%80%90%E4%BF%9D%E7%A0%94%E5%A4%A9%E6%A2%AF%E3%80%91/</id>
    <published>2024-08-20T05:40:51.045Z</published>
    <updated>2024-08-20T05:49:06.519Z</updated>
    
    <content type="html"><![CDATA[<figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201349184.jpg"alt="e42053ba94f0d26e9bbe1276d3f7006" /><figcaptionaria-hidden="true">e42053ba94f0d26e9bbe1276d3f7006</figcaption></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201341313.jpg"alt="7c64a3f6bf1aee01a1863e690a0484a" /><figcaptionaria-hidden="true">7c64a3f6bf1aee01a1863e690a0484a</figcaption></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201341136.jpg"alt="c002fed944e0a70de4320ea7378f8ee" /><figcaptionaria-hidden="true">c002fed944e0a70de4320ea7378f8ee</figcaption></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201341866.jpg"alt="fd8579f11f080d0838b4d187b8860ce" /><figcaptionaria-hidden="true">fd8579f11f080d0838b4d187b8860ce</figcaption></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201341112.jpg"alt="f577c6cd8448b671c53ce926ed2d4c4" /><figcaptionaria-hidden="true">f577c6cd8448b671c53ce926ed2d4c4</figcaption></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201341166.jpg"alt="8f283c6de1063b64e012aea5b9ee2e4" /><figcaptionaria-hidden="true">8f283c6de1063b64e012aea5b9ee2e4</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img
src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201349184.jpg&quot;
alt=&quot;e42053ba94f0d26e9bbe1276d3f7006&quot; /&gt;
&lt;figca</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.gitee.io/2024/08/20/[object%20Object]/%E4%BF%9D%E7%A0%94/%E3%80%90%E6%8A%A5%E8%80%83%E6%95%B0%E9%87%8F%E3%80%91/"/>
    <id>https://cuiluyi.gitee.io/2024/08/20/[object%20Object]/%E4%BF%9D%E7%A0%94/%E3%80%90%E6%8A%A5%E8%80%83%E6%95%B0%E9%87%8F%E3%80%91/</id>
    <published>2024-08-20T05:39:34.940Z</published>
    <updated>2024-08-20T05:40:00.782Z</updated>
    
    <content type="html"><![CDATA[<figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201339912.jpeg"alt="1b30d588c93c5675b29a09a480e43365" /><figcaptionaria-hidden="true">1b30d588c93c5675b29a09a480e43365</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure&gt;
&lt;img
src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408201339912.jpeg&quot;
alt=&quot;1b30d588c93c5675b29a09a480e43365&quot; /&gt;
&lt;fig</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Adam详解</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</id>
    <published>2024-08-13T02:02:27.000Z</published>
    <updated>2024-08-20T05:28:44.196Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adam算法详解">Adam算法详解</h1><p>Adam算法在RMSProp算法基础上对<ahref="https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;spm=1001.2101.3001.7020">小批量</a>随机梯度也做了指数加权移动平均[1]。下面我们来介绍这个算法。</p><blockquote><p>所以Adam算法可以看做是RMSProp算法与动量法的结合。adam算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶moment和二阶moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了Adagrad算法的处理稀疏型数据，又结合了RMSProp算法的可以处理非稳态的数据。</p></blockquote><p>小tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个<ahref="https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;vd_source=12c80a98ec9426002a2f54318421082c">优化算法系列合集</a>，个人觉得比较容易听懂</p><h2 id="算法">算法</h2><p>Adam算法使用了动量变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span>​和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量 <spanclass="math inline">\(\boldsymbol{s}_t\)</span> ​，并在时间步 <spanclass="math inline">\(0\)</span> 将它们中每个元素初始化为 <spanclass="math inline">\(0\)</span>。给定超参数 <spanclass="math inline">\(0 \leq \beta_1 &lt; 1\)</span> （算法作者建议设为<span class="math inline">\(0.9\)</span>），时间步 <spanclass="math inline">\(t\)</span> 的动量变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> ​即小批量随机梯度 <spanclass="math inline">\(\boldsymbol{g}_t\)</span> ​的指数加权移动平均：</p><p><span class="math display">\[\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1)\boldsymbol{g}_t\]</span> 和RMSProp算法中一样，给定超参数 <span class="math inline">\(0\leq \beta_2 &lt; 1\)</span> （算法作者建议设为0.999）</p><p>将小批量随机梯度按元素平方后的项 <spanclass="math inline">\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)</span>​做指数加权移动平均得到 <spanclass="math inline">\(\boldsymbol{s}_t\)</span>​： <spanclass="math display">\[\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2)\boldsymbol{g}_t \odot \boldsymbol{g}_t\]</span> 由于我们将 <spanclass="math inline">\(\boldsymbol{v}_0\)</span> 和 <spanclass="math inline">\(\boldsymbol{s}_0\)</span> 中的元素都初始化为 <spanclass="math inline">\(0\)</span></p><p>在时间步 <span class="math inline">\(t\)</span> 我们得到 <spanclass="math inline">\(\boldsymbol{v}_t = (1-\beta_1) \sum_{i=1}^t\beta_1^{t-i}\boldsymbol{g}_i\)</span>。将过去各时间步小批量随机梯度的权值相加，得到<span class="math inline">\((1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 -\beta_1^t\)</span>。需要注意的是，当 <spanclass="math inline">\(t\)</span>较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 <spanclass="math inline">\(\beta_1 = 0.9\)</span> 时，<spanclass="math inline">\(\boldsymbol{v}_1 =0.1\boldsymbol{g}_1\)</span>。为了消除这样的影响，对于任意时间步 <spanclass="math inline">\(t\)</span> ，我们可以将 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> 再除以 <spanclass="math inline">\(1 -\beta_1^t\)</span>，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作<strong>偏差修正</strong>。在Adam算法中，我们对变量<span class="math inline">\(\boldsymbol{v}_t\)</span> 和 <spanclass="math inline">\(\boldsymbol{s}_t\)</span> 均作偏差修正： <spanclass="math display">\[\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}\]</span></p><p><span class="math display">\[\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}\]</span></p><p>接下来，Adam算法使用以上偏差修正后的变量 $_t $ 和 <spanclass="math inline">\(\hat{\boldsymbol{s}}_t\)</span>，将模型参数中每个元素的学习率通过按元素运算重新调整：</p><p><span class="math display">\[\boldsymbol{g}_t&#39; \leftarrow \frac{\eta\hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}\]</span> 其中 <span class="math inline">\(\eta\)</span> 是学习率，<spanclass="math inline">\(\epsilon\)</span>是为了维持数值稳定性而添加的常数，如 <spanclass="math inline">\(10^{-8}\)</span>。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用<span class="math inline">\(\boldsymbol{g}_t&#39;\)</span>​迭代自变量：</p><p><span class="math display">\[\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t&#39;\]</span></p><h2 id="从零开始实现">从零开始实现</h2><p>我们按照Adam算法中的公式实现该算法。其中时间步 t tt通过<code>hyperparams</code>参数传入<code>adam</code>函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>():</span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>使用学习率为0.01的Adam算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.245370, 0.065155 sec per epoch</span><br></pre></td></tr></table></figure><figure><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/3e40be6e8cf9cc23f28c74f46c87fb79.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="简洁实现">简洁实现</h2><p>通过名称为“Adam”的优化器实例，我们便可使用PyTorch提供的Adam算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.242066, 0.056867 sec per epoch</span><br></pre></td></tr></table></figure><figure><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/3d2011d1884097856969211021d1ccff.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="小结">小结</h2><ul><li>Adam算法在RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。</li><li>Adam算法使用了偏差修正。</li></ul><h2 id="参考文献">参考文献</h2><p>[1] Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochasticoptimization. arXiv preprint arXiv:1412.6980.</p><hr /><blockquote><p>注：除代码外本节与原书此节基本相同，<ahref="https://zh.d2l.ai/chapter_optimization/adam.html">原书传送门</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;adam算法详解&quot;&gt;Adam算法详解&lt;/h1&gt;
&lt;p&gt;Adam算法在RMSProp算法基础上对&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;amp;spm=1001.2</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/</id>
    <published>2024-08-13T01:02:27.000Z</published>
    <updated>2024-08-20T05:28:49.430Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adam-算法">Adam 算法</h1><p>​接下来，我们将介绍目前常用的梯度下降法中的王者——Adam算法。Adam（AdaptiveMomentEstimation）是目前深度学习中最常用的优化算法之一。Adam算法的核心思想是<strong>利用梯度一阶动量和二阶动量来动态自适应调整学习率</strong>，既保持了<strong>Momentum收敛速度快</strong>的优点，又结合了<strong>RMSProp自适应学习率</strong> 的优点</p><h2 id="基本思想">基本思想</h2><p>Adam算法通过计算梯度的<strong>一阶动量</strong>（即<strong>梯度的指数加权移动平均）</strong>和梯度的<strong>二阶动量</strong>（即<strong>梯度平方的指数加权移动平均</strong>）来<strong>动态调整</strong>每个参数的<strong>学习率</strong>。具体公式如下：</p><ol type="1"><li>梯度的一阶动量：</li></ol><p><span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\]</span></p><ol start="2" type="1"><li>梯度的二阶动量：</li></ol><p><span class="math display">\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p><ol start="3" type="1"><li>偏差修正：</li></ol><p><span class="math display">\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}  \]</span></p><p><span class="math display">\[\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\]</span></p><ol start="4" type="1"><li>更新参数：</li></ol><p><span class="math display">\[\theta_{t+1} = \theta_t - \frac{\alpha \hat{m}_t}{\sqrt{\hat{v}_t} +\epsilon}\]</span></p><p>其中：<br />- <span class="math inline">\(\beta_1\)</span> 和 <spanclass="math inline">\(\beta_2\)</span>分别是<strong>动量</strong>和<strong>均方根动量</strong>的衰减率，常用值为<span class="math inline">\(\beta_1 = 0.9\)</span> 和 <spanclass="math inline">\(\beta_2 = 0.999\)</span> - <spanclass="math inline">\(\epsilon\)</span>是一个很小的常数，用于防止分母为零，常用值为 <spanclass="math inline">\(10^{-8}\)</span></p><h2 id="优缺点">优缺点</h2><p><strong>优点</strong>：</p><ul><li><strong>自适应调整学习率</strong>：根据一阶动量和二阶动量动态调整每个参数的学习率，使得训练过程更加稳定。</li><li><strong>收敛速度快</strong>：结合动量法的<strong>加速特性</strong>和RMSProp的<strong>平稳特性</strong>，能够快速收敛到最优解。</li><li>能处理<strong>稀疏梯度</strong>，适用于大规模数据和参数。</li></ul><p><strong>缺点</strong>：</p><ul><li>对于某些特定问题，Adam 可能会出现不稳定的收敛行为。<br /></li><li>参数较多：Adam 算法需要调整的<strong>超参数较多</strong>（例如 <spanclass="math inline">\(\beta_1\)</span> , <spanclass="math inline">\(\beta_2\)</span> , <spanclass="math inline">\(\epsilon\)</span>），调参复杂度高。</li></ul><h2 id="代码实现">代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义Adam优化器</span></span><br><span class="line">optimizer = torch.optim.Adam([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adam&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="adam-与其他算法的比较">Adam 与其他算法的比较</h2><p>Adam算法集成了SGD、动量法、Adagrad、Adadelta等多种优化算法的优点，具有快速收敛和稳定的特点。以下是它与其他算法的对比：</p><ol type="1"><li>SGD：基本的随机梯度下降法，收敛速度较慢，易陷入局部最优。</li><li>动量法：在SGD基础上加入一阶动量，加速收敛，但仍然可能陷入局部最优。</li><li>Adagrad：自适应学习率，但对历史梯度的累积会导致学习率不断减小，后期训练缓慢。</li><li>RMSProp：改进了Adagrad，通过引入衰减系数解决学习率不断减小的问题。</li><li>Adam：结合动量法和RMSProp的优点，具有快速收敛和稳定的特点，是目前最常用的优化算法。</li></ol><h2 id="小结">小结</h2><p>Adam 算法作为一种自适应的梯度下降优化算法，结合了动量法和 RMSProp的优点，能够有效地加速模型的收敛，同时保持稳定性。它通过计算一阶和二阶动量来动态调整学习率，使得模型在训练过程中能够快速收敛，并适应不同的优化问题。尽管Adam需要调整的超参数较多，但其优越的性能使得它成为深度学习中最广泛使用的优化算法之一。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;adam-算法&quot;&gt;Adam 算法&lt;/h1&gt;
&lt;p&gt;​
接下来，我们将介绍目前常用的梯度下降法中的王者——Adam算法。Adam（Adaptive
Moment
Estimation）是目前深度学习中最常用的优化算法之一。Adam算法的核心思想是&lt;stron</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>AdaGrad</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.AdaGrad/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.AdaGrad/</id>
    <published>2024-08-13T00:02:27.000Z</published>
    <updated>2024-08-20T05:29:13.252Z</updated>
    
    <content type="html"><![CDATA[<p># AdaGrad算法</p><p>在前面我们讲解了<ahref="https://so.csdn.net/so/search?q=%E5%8A%A8%E9%87%8F%E6%B3%95&amp;spm=1001.2101.3001.7020">动量法</a>（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的AdaGrad算法。二阶动量的出现，才意味着真正的<strong>自适应学习率</strong>优化算法时代的到来。</p><h2 id="adagrad算法的基本思想">AdaGrad算法的基本思想</h2><p>我们先回顾一下传统的<ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>（SGD）及其各种变种。它们都是以<strong>同样的学习率</strong>来更新<strong>每一个参数</strong>的。但深度神经网络往往包含大量参数，这些参数并不总是<strong>均匀更新</strong>的。有些参数更新得频繁，有些则很少更新。</p><ul><li>对于<strong>经常更新</strong>的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的<strong>学习率小一些</strong></li><li>对于<strong>偶尔更新</strong>的参数，我们了解的信息较少，希望从每一个样本中多学一些，即<strong>学习率大一些</strong></li></ul><p>要动态度量历史更新的频率，我们引入<strong>二阶动量</strong>。二阶动量通过将每一位各自的历史梯度的<strong>平方</strong>叠加起来来计算。具体公式如下：</p><p><span class="math display">\[v_t = v_{t-1} + g_t^2\]</span></p><p>其中，<span class="math inline">\(g_t\)</span> 是当前的梯度。</p><h2 id="算法流程">算法流程</h2><ol type="1"><li><strong>计算当前梯度 <span class="math inline">\(g_t\)</span></strong>：</li></ol><p><span class="math display">\[g_t = \nabla f(w_t)\]</span></p><ol start="2" type="1"><li><strong>更新二阶动量 <span class="math inline">\(v_t\)</span></strong>：</li></ol><p><span class="math display">\[v_t =  v_{t-1} + g_t^2\]</span></p><ol start="3" type="1"><li><strong>计算当前时刻的下降梯度</strong>：</li></ol><p><span class="math display">\[w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t} + \epsilon} g_t\]</span></p><p>其中，<span class="math inline">\(\alpha\)</span> 是学习率，<spanclass="math inline">\(\epsilon\)</span>是一个小的平滑项，防止分母为0。</p><h2 id="稀疏特征处理">稀疏特征处理</h2><p>AdaGrad算法主要针对<strong>稀疏特征</strong>进行了优化。<strong>稀疏特征</strong>在很多样本中只出现少数几次，在训练模型时，这些稀疏特征的更新很少，但每次更新可能带来较大影响。AdaGrad通过调整每个特征的学习率，针对这种情况进行了优化。</p><h3 id="优缺点">优缺点</h3><p><strong>优点</strong>：</p><ol type="1"><li><strong>有效处理稀疏特征</strong>：自动调整每个参数的学习率，使得稀疏特征的更新更少。<br /></li><li><strong>加速收敛</strong>：在自动调整学习率的同时，使得模型在训练过程中更快收敛。</li></ol><p><strong>缺点</strong>：<br />1.<strong>学习率逐渐减小</strong>：每次迭代中学习率都会减小，导致训练后期学习率变得非常小，从而使收敛速度变慢。<br />2.<strong>固定调整方式</strong>：对于不同参数，学习率调整方式是固定的，无法根据不同任务自动调整。</p><h2 id="代码实现">代码实现</h2><p>下面是一个简单的PyTorch实现AdaGrad算法的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成一些数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义AdaGrad优化器</span></span><br><span class="line">optimizer = torch.optim.Adagrad([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112049596.png"alt="image-20240811204910528" /><figcaption aria-hidden="true">image-20240811204910528</figcaption></figure><h2 id="总结">总结</h2><p>本节我们介绍了一种新的梯度下降算法变体——AdaGrad。与动量法相比，它最大的改进在于<strong>使用二阶动量来动态调整学习率</strong>，能够记住历史上的梯度信息，以动态调整学习率。其主要优点是能够处理稀疏特征问题，但也有学习率逐渐减小和调整方式固定的缺点。</p><p>到目前为止，我们一共讲了五种梯度下降算法。AdaGrad是2011年提出的，而动量法在1993年提出，SGD在1951年提出。通过时间轴的对比，我们可以看出人们在不断研究和改进梯度下降算法，从最早的梯度下降法到SGD，再到动量法、小批量梯度下降，最后到2011年的AdaGrad。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;# AdaGrad算法&lt;/p&gt;
&lt;p&gt;在前面我们讲解了&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E5%8A%A8%E9%87%8F%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;动量法&lt;/a&gt;（Mo</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>RMSProp 和 Adadelta</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.RMSProp%20%E5%92%8C%20Adadelta/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.RMSProp%20%E5%92%8C%20Adadelta/</id>
    <published>2024-08-12T23:02:27.000Z</published>
    <updated>2024-08-20T05:28:03.151Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rmsprop-和-adadelta-算法">RMSProp 和 Adadelta 算法</h1><p>在<ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>中，RMSProp和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad的基础上做了改进，以适应深度学习中的大规模参数优化需求。</p><h2 id="rmsprop-算法">RMSProp 算法</h2><h3 id="基本思想">基本思想</h3><p>RMSProp 对 AdaGrad进行改进，通过引入<strong>衰减率</strong>来调整二阶动量的累积。这样可以<strong>避免</strong>AdaGrad 中<strong>学习率减小过快</strong>的问题。</p><p>AdaGrad 的二阶动量计算公式如下：</p><p><span class="math display">\[v_t = v_{t-1} + g_t^2\]</span> 而 RMSProp 采用了带有衰减率的计算方式：</p><p><span class="math display">\[v_t = \beta v_{t-1} + (1 - \beta) g_t^2\]</span> 其中，<span class="math inline">\(\beta\)</span>是衰减率系数。</p><h3 id="优缺点">优缺点</h3><p><strong>优点：</strong></p><ul><li><strong>自动调整学习率</strong>，避免学习率过大或过小的问题</li><li><strong>加速收敛速度</strong></li><li><strong>简单适用</strong>，适用于各种优化问题</li></ul><p><strong>缺点：</strong></p><ul><li>在处理稀疏特征时不够优秀</li><li>需要调整的超参数较多（衰减率 <spanclass="math inline">\(\beta\)</span> i和学习率 <spanclass="math inline">\(\alpha\)</span> ）</li><li>收敛速度可能不如某些更先进的<ahref="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a></li></ul><h3 id="代码实现">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义RMSProp优化器</span></span><br><span class="line">optimizer = torch.optim.RMSprop([w, b], lr=learning_rate, alpha=beta)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with RMSProp&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="adadelta-算法">Adadelta 算法</h2><h3 id="基本思想-1">基本思想</h3><p>Adadelta 是对 RMSProp的进一步改进，旨在<strong>自动调整学习率</strong>，避免手动调参。它通过计算梯度和权重更新量的累积值来调整学习率，使得训练过程更加稳定。</p><p>Adadelta 的公式如下：</p><ol type="1"><li>梯度的累积：</li></ol><p><span class="math display">\[E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g_t^2\]</span></p><ol start="2" type="1"><li>权重更新量的累积：</li></ol><p><span class="math display">\[E[\Delta x^2]_t = \rho E[\Delta x^2]_{t-1} + (1 - \rho) (\Delta x_t)^2\]</span></p><ol start="3" type="1"><li>更新参数：</li></ol><p><span class="math display">\[\Delta x_t = -\frac{\sqrt{E[\Delta x^2]_{t-1} +\epsilon}}{\sqrt{E[g^2]_t + \epsilon}} g_t  \]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_t + \Delta x_t\]</span></p><h3 id="优缺点-1">优缺点</h3><p><strong>优点：</strong><br />- <strong>自动调整学习率</strong>，避免学习率过大或过小的问题 -避免出现学习率饱和现象，使得训练更加稳定</p><p><strong>缺点：</strong></p><ul><li>可能收敛较慢</li><li>需要维护梯度和权重更新量的累积值，增加了空间复杂度</li></ul><h3 id="代码实现-1">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">1.0</span>  <span class="comment"># Adadelta 不需要传统的学习率</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">rho = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">1e-6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义Adadelta优化器</span></span><br><span class="line">optimizer = torch.optim.Adadelta([w, b], rho=rho, eps=epsilon)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adadelta&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;rmsprop-和-adadelta-算法&quot;&gt;RMSProp 和 Adadelta 算法&lt;/h1&gt;
&lt;p&gt;在&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Momentum</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/</id>
    <published>2024-08-12T22:02:27.000Z</published>
    <updated>2024-08-20T05:27:59.151Z</updated>
    
    <content type="html"><![CDATA[<h1 id="动量法momentum">动量法（Momentum）</h1><h2 id="背景知识">背景知识</h2><p>在<ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>的优化过程中，梯度下降法（GradientDescent,GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在<strong>收敛速度慢</strong>、<strong>容易陷入局部最小值</strong>以及在<strong>高维空间中震荡较大</strong>的问题。为了解决这些问题，人们提出了动量法（Momentum）。</p><h2 id="动量法的概念">动量法的概念</h2><p>动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：<strong>在运动方向上保持运动的趋势，从而加速收敛</strong>。</p><h2 id="动量法在深度学习中的应用">动量法在深度学习中的应用</h2><p>在深度学习中，动量法通过记录<strong>梯度的增量</strong>并将其与<strong>当前梯度相加</strong>，来<strong>平滑梯度下降</strong>的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。</p><p>动量法的更新公式如下：</p><p><span class="math display">\[m_t = \beta m_{t-1} + \nabla L(w_t)\]</span> <span class="math display">\[w_{t+1} = w_t - \alpha m_t\]</span></p><p>其中：</p><ul><li><span class="math inline">\(m_t\)</span>是动量项，记录了之前梯度的累积。<br /></li><li><span class="math inline">\(\beta\)</span>是动量参数，控制<strong>动量项的衰减</strong>，一般取值为0.9。<br /></li><li><span class="math inline">\(\nabla L(w_t)\)</span>是当前参数的梯度。</li><li><span class="math inline">\(\alpha\)</span> 是学习率。</li></ul><h2 id="动量法的优点">动量法的优点</h2><ol type="1"><li><p><strong>加速收敛</strong>：动量法通过积累之前的梯度信息，使得优化过程更为顺畅，避免了曲折路径，提高了收敛速度。<br /></p></li><li><p><strong>跳过局部最小值</strong>：由于动量的累积作用，可以帮助优化算法跳过一些局部最小值，找到更优的解。<br /></p></li><li><p><strong>减少振荡</strong>：动量法可以有效减小学习过程中梯度震荡的现象，使得模型的训练更加稳定。## 动量法的缺点</p></li><li><p><strong>计算复杂度增加</strong>：由于需要维护动量项，会导致计算复杂度的增加。<br /></p></li><li><p><strong>参数调节</strong>：动量法引入了新的超参数（动量系数），需要在实际应用中进行调节。</p></li></ol><h2 id="动量法的改进及变种">动量法的改进及变种</h2><p>​ 在动量法的基础上，还有一些改进和变种，如Nesterov加速梯度（NesterovAccelerated <ahref="https://so.csdn.net/so/search?q=Gradient&amp;spm=1001.2101.3001.7020">Gradient</a>,NAG）、RMSprop、Adam等。这些方法在动量法的基础上进一步优化了收敛速度和稳定性。</p><h2 id="实验代码示例">实验代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据生成</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">X = torch.randn(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span> * X.squeeze() + <span class="number">2</span> + torch.randn(<span class="number">1000</span>) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同梯度下降方法的比较</span></span><br><span class="line">methods = &#123;</span><br><span class="line">    <span class="string">&#x27;SGD&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>),</span><br><span class="line">    <span class="string">&#x27;Momentum&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">losses = &#123;method: [] <span class="keyword">for</span> method <span class="keyword">in</span> methods&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> method_name, optimizer_fn <span class="keyword">in</span> methods.items():</span><br><span class="line">    model = LinearModel()</span><br><span class="line">    optimizer = optimizer_fn(model.parameters())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(X)</span><br><span class="line">        loss = criterion(outputs.squeeze(), y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        losses[method_name].append(loss.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line"><span class="keyword">for</span> method_name, loss_values <span class="keyword">in</span> losses.items():</span><br><span class="line">    plt.plot(loss_values, label=method_name)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Loss Curve Comparison&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112029254.png"alt="image-20240811202901191" /><figcaption aria-hidden="true">image-20240811202901191</figcaption></figure><h2 id="结论">结论</h2><p>动量法通过引入动量项，显著提高了<ahref="https://so.csdn.net/so/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">梯度下降法</a>的收敛速度和稳定性。尽管在实际应用中引入了额外的计算开销，但其在许多深度学习任务中的表现优异，已经成为常用的优化方法之一。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;动量法momentum&quot;&gt;动量法（Momentum）&lt;/h1&gt;
&lt;h2 id=&quot;背景知识&quot;&gt;背景知识&lt;/h2&gt;
&lt;p&gt;在&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>mini-batch GD</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3.Mini-batch%20GD/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3.Mini-batch%20GD/</id>
    <published>2024-08-12T21:02:27.000Z</published>
    <updated>2024-08-20T05:27:56.599Z</updated>
    
    <content type="html"><![CDATA[<h1id="小批量梯度下降法mini-batch-gradient-descent">小批量梯度下降法（Mini-batchGradient Descent）</h1><p>在<ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020">深度学习模型</a>的训练过程中，梯度下降法是最常用的优化算法之一。我们前面介绍了梯度下降法（BatchGradient Descent）和随机梯度下降法（Stochastic GradientDescent），两者各有优缺点。为了在<strong>计算速度</strong>和<strong>收敛稳定性</strong>之间找到<strong>平衡</strong>，<strong>小批量梯度下降法</strong>（Mini-batchGradientDescent）应运而生。下面我们详细介绍其基本思想、优缺点，并通过代码实现来比较三种梯度下降法。</p><h2 id="小批量梯度下降法的基本思想">小批量梯度下降法的基本思想</h2><p>​小批量梯度下降法在每次迭代中，使用<strong>一小部分随机样本（称为小批量）</strong>来计算梯度，并更新参数值。具体来说，算法步骤如下：</p><ol type="1"><li><p>初始化参数 <span class="math inline">\(w\)</span> 和 <spanclass="math inline">\(b\)</span></p></li><li><p>在每次迭代中，从训练集中随机抽取 <spanclass="math inline">\(m\)</span> 个样本。</p></li><li><p>使用这 <span class="math inline">\(m\)</span>个样本计算损失函数的梯度</p></li><li><p>更新参数 <span class="math inline">\(w\)</span> 和 <spanclass="math inline">\(b\)</span></p></li></ol><p>其梯度计算公式如下：</p><p><span class="math display">\[w_{t+1}=w_{t}-\alpha \cdot \frac{1}{m}\sum_{i=1}^m{\nabla_w}L(w_{t},b_{t},x_i,y_i),\\b_{t+1}=b_{t}-\alpha \cdot \frac{1}{m}\sum_{i=1}^m{\nabla_b}L(w_{t},b_{t},x_i,y_i),\]</span> 其中，<span class="math inline">\(\alpha\)</span> 是学习率，<span class="math inline">\(m\)</span> 是小批量的大小。</p><ul><li>当 <span class="math inline">\(m=1\)</span> 时，Mini-batch GradientDescent 转换为 SGD</li><li>当 <span class="math inline">\(m=n\)</span> 时，Mini-batch GradientDescent 转换为 GD</li></ul><h2 id="优缺点">优缺点</h2><h3 id="优点">优点</h3><ol type="1"><li><strong>计算速度快</strong>：与批量梯度下降法相比，每次迭代只需计算小批量样本的梯度，速度更快。<br /></li><li><strong>减少振荡</strong>：与<ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>相比，梯度的计算更加稳定，减少了参数更新时的振荡。<br /></li><li><strong>控制灵活</strong>：可以调整小批量的大小，使得训练速度和精度之间达到平衡。</li></ol><h3 id="缺点">缺点</h3><ol type="1"><li><strong>需要调整学习率和小批量大小</strong>：学习率决定每次更新的步长，小批量大小决定每次计算梯度使用的样本数量。<br /></li><li><strong>内存消耗</strong>：小批量大小的选择受限于内存容量，尤其在使用GPU运算时，需要选择合适的小批量大小。</li></ol><h3 id="代码实现">代码实现</h3><p>下面通过代码实现和比较三种梯度下降法的执行效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span>*X + <span class="number">2</span> + np.random.randn(<span class="number">1000</span>, <span class="number">1</span>) * <span class="number">0.1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 转换为tensor</span></span><br><span class="line">X_tensor = torch.tensor(X, dtype=torch.float32)</span><br><span class="line">y_tensor = torch.tensor(y, dtype=torch.float32)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 封装为数据集</span></span><br><span class="line">dataset = TensorDataset(X_tensor, y_tensor)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearRegressionModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义梯度下降法的批量大小</span></span><br><span class="line">batch_sizes = [<span class="number">1000</span>, <span class="number">1</span>, <span class="number">128</span>]</span><br><span class="line">batch_labels = [<span class="string">&#x27;Batch Gradient Descent&#x27;</span>, <span class="string">&#x27;Stochastic Gradient Descent&#x27;</span>, <span class="string">&#x27;Mini-batch Gradient Descent&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储损失值</span></span><br><span class="line">losses = &#123;label: [] <span class="keyword">for</span> label <span class="keyword">in</span> batch_labels&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> batch_size, label, color <span class="keyword">in</span> <span class="built_in">zip</span>(batch_sizes, batch_labels, colors):</span><br><span class="line">    model = LinearRegressionModel()</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">    </span><br><span class="line">    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs), desc=label):</span><br><span class="line">        epoch_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> data_loader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(batch_x)</span><br><span class="line">            loss = criterion(outputs, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        losses[label].append(epoch_loss / <span class="built_in">len</span>(data_loader))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制损失值变化曲线</span></span><br><span class="line"><span class="keyword">for</span> label, color <span class="keyword">in</span> <span class="built_in">zip</span>(batch_labels, colors):</span><br><span class="line">    plt.plot(losses[label], color=color, label=label)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="结果分析">结果分析</h3><p>运行上述代码后，会显示三种梯度下降法在每个迭代周期（epoch）中的损失变化曲线。可以看到：</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112012585.png"alt="image-20240811201255444" /><figcaption aria-hidden="true">image-20240811201255444</figcaption></figure><ol type="1"><li>批量梯度下降法：损失曲线平滑，但训练速度较慢。<br /></li><li>随机梯度下降法：训练速度快，但损失曲线波动较大。<br /></li><li>小批量梯度下降法：在训练速度和损失曲线的稳定性之间达到了平衡，效果较为理想。</li></ol><h2 id="总结">总结</h2><p>​小批量梯度下降法结合了批量梯度下降法和随机梯度下降法的优点，是深度学习中常用的<ahref="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a>。通过调整小<strong>批量大小</strong>和<strong>学习率</strong>，可以在<strong>训练速度</strong>和<strong>收敛稳定性</strong>之间找到最佳平衡。在实际应用中，小批量梯度下降法由于其较高的效率和较好的收敛效果，被广泛应用于各类深度学习模型的训练中。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1
id=&quot;小批量梯度下降法mini-batch-gradient-descent&quot;&gt;小批量梯度下降法（Mini-batch
Gradient Descent）&lt;/h1&gt;
&lt;p&gt;在&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SGD</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2.SGD/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2.SGD/</id>
    <published>2024-08-12T20:02:27.000Z</published>
    <updated>2024-08-20T05:26:30.439Z</updated>
    
    <content type="html"><![CDATA[<h1 id="随机梯度下降法sgd">随机梯度下降法（SGD）</h1><p>​ 在<ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>中，梯度下降法（GradientDescent）是最常用的模型参数优化方法。然而，传统的梯度下降法（Full BatchLearning）存在一些缺点，例如训练时间过长和容易陷入局部最小值。为了解决这些问题，随机梯度下降法（StochasticGradient Descent，简称SGD）应运而生。</p><h2 id="传统梯度下降法的问题">传统梯度下降法的问题</h2><ol type="1"><li><strong>训练时间长</strong>：传统梯度下降法需要使用所有训练数据来计算梯度，因此数据量大时耗时严重。<br /></li><li><strong>容易陷入局部最小值</strong>：复杂的损失函数可能会导致算法在局部最小值附近来回震荡，无法快速收敛。<br /></li><li><strong>对初始值敏感</strong>：初始值选择不当可能导致算法被卡在局部最小值。</li></ol><h2 id="随机梯度下降法的基本思想"><ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>的基本思想</h2><p>SGD每次迭代仅<strong>使用一个样本</strong>的<strong>损失值</strong>来计算梯度，而不是全数据集损失值的求和平均来计算梯度。SGD的优缺点 -------</p><h3 id="优点">优点</h3><ol type="1"><li><strong>速度快</strong>：每次迭代只需计算一个样本的梯度，速度比传统方法快很多。<br /></li><li><strong>避免局部最小值</strong>：因为每次更新参数只使用一个样本，随机性使得算法不容易陷入局部最小值。</li><li><strong>更易实现和调整</strong>：每个样本的梯度可以分别计算，并行处理更加高效。</li></ol><h3 id="缺点">缺点</h3><ol type="1"><li><strong>收敛不稳定</strong>：每次迭代梯度都会有噪声，可能导致收敛不稳定。</li><li><strong>方差较大</strong>：每次更新参数只使用一个样本的梯度，可能导致算法方差较大，难以收敛。</li></ol><h2 id="动态学习率">动态学习率</h2><p>为了提高SGD的收敛性，可以使用动态学习率。常见的动态学习率策略包括：</p><ol type="1"><li><p><strong>反比例学习率</strong>：初始学习率随着迭代次数增加而减小。</p></li><li><p><strong>反比例平方学习率</strong>：类似反比例学习率，但减小速度更快。</p></li></ol><p><span class="math display">\[\alpha_t = \frac{\alpha_0}{1 + k \cdot t^2}\]</span></p><ol start="3" type="1"><li><strong>指数衰减学习率</strong>：学习率以指数形式衰减。</li></ol><p><span class="math display">\[\alpha_t = \alpha_0 \cdot e^{-\lambda t}\]</span></p><h2 id="实现示例">实现示例</h2><p>下面是使用Python实现随机梯度下降法的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_descent</span>(<span class="params">X, y, lr=<span class="number">0.01</span>, epochs=<span class="number">1000</span></span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line">    w = np.zeros(n)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            random_index = np.random.randint(m)</span><br><span class="line">            xi = X[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            gradient_w = <span class="number">2</span> * xi.T.dot(xi.dot(w) + b - yi)</span><br><span class="line">            gradient_b = <span class="number">2</span> * (xi.dot(w) + b - yi)</span><br><span class="line">            w = w - lr * gradient_w</span><br><span class="line">            b = b - lr * gradient_b</span><br><span class="line">        <span class="comment"># 可选：动态学习率调整</span></span><br><span class="line">        lr = lr / (<span class="number">1</span> + epoch / epochs)</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">y = np.array([<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用SGD函数</span></span><br><span class="line">w, b = stochastic_gradient_descent(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;权重:&quot;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;偏置:&quot;</span>, b)</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><p>SGD通过每次迭代使用一个<strong>随机样本</strong>来计算梯度，从而加快了计算速度并避免陷入局部最小值。动态学习率的使用可以进一步提高SGD的收敛性。在实际应用中，SGD已成为深度学习领域最常用的优化算法之一。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;随机梯度下降法sgd&quot;&gt;随机梯度下降法（SGD）&lt;/h1&gt;
&lt;p&gt;​ 在&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.210</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>GD</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1.GD/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1.GD/</id>
    <published>2024-08-12T19:02:27.000Z</published>
    <updated>2024-08-20T05:26:27.544Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度下降法gradient-descent">梯度下降法（Gradient Descent）</h1><h2 id="引言">引言</h2><p>​ 在深度学习中，<ahref="https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">损失函数</a>的求解是一个关键步骤。损失函数通常没有解析解，因此需要通过最优化算法来逼近求解。其中，梯度下降法是最常用的优化算法之一。本文将详细介绍梯度下降法的基本概念、理论基础、及其在深度学习中的应用。</p><h2 id="梯度下降法的基本概念">梯度下降法的基本概念</h2><p>梯度下降法（GradientDescent）是一种基于一阶导数的优化算法，用于最小化目标函数。在深度学习中，目标函数通常是损失函数，其目的是通过调整参数来使损失最小化。</p><h3 id="损失函数的定义">损失函数的定义</h3><p>假设损失函数 <span class="math inline">\(L\)</span> 是参数 <spanclass="math inline">\(W\)</span> 的函数：<spanclass="math inline">\(L(W)\)</span>，我们的目标是找到参数 <spanclass="math inline">\(W\)</span> 使得 <spanclass="math inline">\(L(W)\)</span> 最小化。</p><h3 id="梯度的定义">梯度的定义</h3><p>梯度是损失函数的导数，表示函数在某一点处的最陡下降方向。对于参数<span class="math inline">\(W\)</span> 的每个分量 <spanclass="math inline">\(w_i\)</span> ，梯度表示为：</p><p><span class="math display">\[\nabla L(W)=\left[\frac{\partial L}{\partial w_1},\frac{\partialL}{\partial w_2},\ldots,\frac{\partial L}{\partial w_n}\right]\]</span></p><blockquote><p><strong>梯度的维度</strong>=<strong>参数的个数</strong></p></blockquote><h3 id="梯度下降算法">梯度下降算法</h3><p>梯度下降法通过以下步骤更新参数：</p><p><span class="math display">\[w_{t+1}=w_{t}-\alpha \cdot \frac{1}{n}\sum_{i=1}^n{\nabla_w}L(w_{t},b_{t},x_i,y_i),\\b_{t+1}=b_{t}-\alpha \cdot \frac{1}{n}\sum_{i=1}^n{\nabla_b}L(w_{t},b_{t},x_i,y_i),\]</span> 其中，<span class="math inline">\(\alpha\)</span>是学习率（Learning Rate），决定了每次更新的步长；<spanclass="math inline">\(n\)</span> 是样本大小。</p><h2 id="梯度下降法的应用">梯度下降法的应用</h2><h3 id="简单示例二次损失函数">简单示例：二次损失函数</h3><p>为了便于理解，我们假设损失函数是一个简单的二次函数：</p><p><span class="math display">\[L(W) = W^2\]</span> 梯度为：</p><p><span class="math display">\[\nabla L(W) = 2W\]</span> 根据梯度下降法的更新规则，参数更新为：</p><p><span class="math display">\[W_{t+1} = W_t - \alpha \cdot 2W_t = W_t(1 - 2\alpha)\]</span></p><h3 id="高维度情况下的梯度下降">高维度情况下的梯度下降</h3><p>​在实际应用中，损失函数往往是高维度的。梯度下降法可以扩展到高维度情况，其中<strong>梯度是一个向量，表示每个参数</strong>的导数。我们将梯度表示为一个向量，并对每个参数进行更新。</p><h3 id="学习率的选择">学习率的选择</h3><p>学习率 <span class="math inline">\(\alpha\)</span>对梯度下降法的收敛速度和稳定性有重大影响。选择合适的学习率非常重要。</p><ul><li>如果学习率过大，算法可能会在最小值附近来回<strong>震荡</strong>；</li><li>如果学习率过小，算法的<strong>收敛速度会非常慢</strong>。</li></ul><h2 id="梯度下降法的变体">梯度下降法的变体</h2><p>在实际应用中，梯度下降法有多种变体，以提高收敛速度和稳定性。常见的变体包括：</p><ul><li><strong>随机梯度下降法（SGD）</strong>：每次迭代使用一个或几个样本来更新参数，而不是使用整个训练集。这种方法可以显著加快计算速度。<br /></li><li><strong>动量法（Momentum）</strong>：在每次更新时，加入之前更新的动量，以加速收敛。<br /></li><li><strong>自适应学习率</strong>方法：例如Adagrad、RMSprop、Adam等，通过<strong>动态调整学习率</strong>来提高收敛效果。</li></ul><h2 id="总结">总结</h2><p>梯度下降法是深度学习中最常用的优化算法之一。通过计算损失函数的梯度，确定参数的更新方向和步长，不断逼近损失函数的最小值。<strong>选择合适的学习率和初始点是梯度下降法（GD）成功的关键。</strong>理解梯度下降法的基本概念和应用，对于深入学习<ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">深度学习算法</a>有重要意义。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;梯度下降法gradient-descent&quot;&gt;梯度下降法（Gradient Descent）&lt;/h1&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;​ 在深度学习中，&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的优化算法探讨</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%92%8C%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%92%8C%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</id>
    <published>2024-08-12T18:02:27.000Z</published>
    <updated>2024-08-20T05:25:37.867Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习中的优化算法探讨">深度学习中的优化算法探讨</h1><p>​在深度学习的过程中，<strong>优化算法</strong>扮演着至关重要的角色。训练神经网络通常需要投入大量的时间和资源，而优化算法的选择和应用直接影响模型的训练效率和效果。数值优化是一个庞大的数学学科，本篇文章将探讨与深度学习，特别是训练过程密切相关的优化算法。</p><h2id="最优化理论和深度学习优化算法的区别">最优化理论和深度学习优化算法的区别</h2><h3 id="度量和损失函数">度量和损失函数</h3><ul><li><strong>最优化理论</strong>：研究如何找到函数的<strong>最优解</strong>，即最大值或最小值，通常有明确的度量标准。</li><li><strong>深度学习</strong>：使用代理损失函数（如负对数似然或交叉熵）来进行优化，通过<strong>最小化代理损失函数</strong>来最大化原始度量。</li></ul><h3 id="数据关注点">数据关注点</h3><ul><li><strong>最优化理论</strong>：只关心现有数据的最优解。</li><li><strong>深度学习</strong>：关注模型的泛化能力，即模型<strong>在测试集</strong>上的表现，避免过拟合现象。</li></ul><h3 id="研究内容">研究内容</h3><ul><li><strong>最优化理论</strong>：注重算法本身的研究。</li><li><strong>深度学习</strong>：关注实现细节，包括神经网络的结构、参数调整等。</li></ul><h2 id="训练误差与泛化误差">训练误差与泛化误差</h2><ul><li><strong>训练误差</strong>：模型在<strong>训练集</strong>上的误差，只关注<strong>训练过程</strong>中的表现。</li><li><strong>泛化误差</strong>：模型在未见过的数据（<strong>测试集</strong>）上的误差，关注模型的<strong>泛化能力</strong>。</li></ul><p>泛化误差的衡量是深度学习优化的<strong>核心</strong>，理想的模型应该在新数据上也能表现良好。</p><h2 id="经验风险与真实风险">经验风险与真实风险</h2><ul><li><strong>经验风险（EmpiricalRisk）</strong>：<strong>训练集</strong>上的期望<strong>损失</strong>，通过<strong>最小化经验风险</strong>来优化模型。</li><li><strong>真实风险（ExpectedRisk）</strong>：使用<strong>真实数据</strong>计算损失函数的期望值，由于无法直接计算<strong>真实风险</strong>，因此通过<strong>优化经验风险</strong>来尽量减少泛化误差。</li></ul><h2 id="深度学习优化中的挑战">深度学习优化中的挑战</h2><h3 id="病态问题ill-conditioned-problem">病态问题（Ill-conditionedProblem）</h3><ul><li>问题解对条件非常敏感，即使微小的变化也会导致解的大幅变化。</li><li>解决方案：正则化技术、数据预处理等。</li></ul><h3 id="局部最小值问题local-minima">局部最小值问题（Local Minima）</h3><ul><li>优化过程中可能陷入局部最小值，而不是全局最优值。</li><li>解决方案：使用不同的优化算法，如随机梯度下降（SGD）、Adam等。</li></ul><h3 id="鞍点问题saddle-points">鞍点问题（Saddle Points）</h3><ul><li>损失函数在某些点的曲率为零，但不是全局最优点。</li><li>解决方案：减少模型复杂度、增加训练数据、使用随机初始化等。</li></ul><h3 id="悬崖问题cliffs">悬崖问题（Cliffs）</h3><ul><li>多层神经网络中的损失函数可能存在陡峭的区域，导致梯度更新大幅改变参数值。</li><li>解决方案：梯度裁剪（Gradient Clipping）以控制梯度大小。</li></ul><h3id="长期依赖问题long-term-dependency-problem">长期依赖问题（Long-termDependency Problem）</h3><ul><li>深层网络结构使得模型难以学习到先前的信息，导致梯度消失或爆炸。</li><li>解决方案：使用LSTM或GRU等特殊的循环神经网络结构。</li></ul><h2 id="总结">总结</h2><p>​优化算法是深度学习模型训练中的核心工具，两者有密切的联系但也有显著的区别。最优化理论关注的是<strong>训练误差</strong>，而深度学习关注的是<strong>泛化误差</strong>。深度学习通过<strong>最小化经验风险</strong>来尽量减少<strong>泛化误差</strong>。优化过程中面临的挑战包括病态问题、局部最小值、鞍点、悬崖和长期依赖问题，这些问题需要通过不同的优化算法和策略来解决。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;深度学习中的优化算法探讨&quot;&gt;深度学习中的优化算法探讨&lt;/h1&gt;
&lt;p&gt;​
在深度学习的过程中，&lt;strong&gt;优化算法&lt;/strong&gt;扮演着至关重要的角色。训练神经网络通常需要投入大量的时间和资源，而优化算法的选择和应用直接影响模型的训练效率和效果。数值优化</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>损失函数</title>
    <link href="https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://cuiluyi.gitee.io/2024/08/13/[object%20Object]/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</id>
    <published>2024-08-12T17:02:27.000Z</published>
    <updated>2024-08-20T05:24:54.469Z</updated>
    
    <content type="html"><![CDATA[<h1 id="损失函数">损失函数</h1><p>​ 在<ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>和机器学习领域，损失函数（LossFunction）是优化问题的核心，决定了模型参数的调整方向和幅度。尽管损失函数种类繁多，但理解其起源和背后的理论有助于我们更好地选择和应用它们。</p><h2 id="损失函数的起源"><ahref="https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">损失函数</a>的起源</h2><p>所有的优化问题都需要确立一个目标函数，通过最小化（或最大化）该目标函数来求解。在<ahref="https://so.csdn.net/so/search?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">机器学习</a>中，损失函数衡量模型预测值与真实值之间的差异，是优化模型参数的重要工具。</p><h2 id="最小二乘法mse">最小二乘法（MSE）</h2><p>损失函数的起源可以追溯到统计学中的最小二乘回归。其基本思想是最小化预测值与真实值之间的差异。假设预测值为<span class="math inline">\(\hat{y}_{i}\)</span>，真实值为 <spanclass="math inline">\(y_i\)</span>，则最小二乘误差为： <spanclass="math display">\[[\mathrm{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})2]\]</span> 通过最小化MSE，可以找到使损失函数最小的参数 <spanclass="math inline">\(\theta\)</span>。</p><h2 id="最大似然估计mle">最大似然估计（MLE）</h2><p>​ 最大似然估计（Maximum Likelihood Estimation,MLE）是另一种基础且重要的参数估计方法，从概率分布的角度来理解<strong>目标函数</strong>或<strong>损失函数</strong>。假设我们有一组独立的样本数据集<spanclass="math inline">\(\{x_1, x_2, ...,x_m\}\)</span>，来自于未知的真实数据分布<spanclass="math inline">\(P_{\text{data}}(x)\)</span>。我们假设另一个分布<spanclass="math inline">\(P_{\text{model}}(x|\theta)\)</span>来近似真实分布。</p><p>最大似然估计的目标是找到参数 <spanclass="math inline">\(\theta\)</span>，使得在给定数据的情况下，模型的似然函数最大化。即：<span class="math display">\[[\hat{\theta}=\arg\max_{\theta}\prod_{i=1}^{m}P_{\mathrm{model}}(x_{i}|\theta)]\]</span> 为了简化计算，我们通常使用对数似然： <spanclass="math display">\[[\hat{\theta}=\arg\max_{\theta}\sum_{i=1}^{m}\logP_{\mathrm{model}}(x_{i}|\theta)]\]</span>在假设数据符合<strong>高斯分布</strong>的情况下，MLE与最小化均方误差（MSE）等价。</p><h2 id="交叉熵损失">交叉熵损失</h2><p>​ 交叉熵损失（Cross-EntropyLoss）是<strong>分类</strong>问题中常用的损失函数。假设数据符合伯努利分布或多项式分布，交叉熵损失用于衡量两个概率分布之间的差异。对于二分类问题，交叉熵损失定义为：</p><p><span class="math display">\[[L=-\frac{1}{m}\sum_{i=1}^{m}[y_{i}\log\hat{y}_{i}+(1-y_{i})\log(1-\hat{y}_{i})]]\]</span>交叉熵损失从概率分布角度来看，本质上也是最大似然估计的一种形式。</p><h2 id="正则化与最大后验估计map">正则化与最大后验估计（MAP）</h2><p><strong>正则化</strong>技术是解决<strong>过拟合</strong>问题的重要手段措施。正则化可以理解为在损失函数中加入<strong>惩罚项</strong>，以<strong>限制模型的复杂度</strong>，从而提高模型的<strong>泛化能力</strong>。正则化可以视作最大后验估计（MaximumA Posteriori Estimation, MAP）的特殊情况。</p><h3 id="l2正则化ridge回归">L2正则化（Ridge回归）</h3><p>L2正则化通过在损失函数中加入<strong>参数的平方和项</strong>来惩罚过大的参数。其目标函数为：</p><p><span class="math display">\[[ \text{L2正则化} = \text{MSE} + \lambda \sum_{j=1}^{p} \theta _{j}^{2}]\]</span></p><p>其中，<span class="math inline">\(\lambda\)</span>是正则化参数，用于控制惩罚项的权重。<spanclass="math inline">\(L2\)</span>正则化可以视为假设参数符合高斯分布时的最大后验估计。</p><h3 id="l1正则化lasso回归">L1正则化（Lasso回归）</h3><p>L1正则化通过在损失函数中加入参数的绝对值和项来惩罚过大的参数。其目标函数为：</p><p><span class="math display">\[[ \text{L1正则化} = \text{MSE} + \lambda \sum_{j=1}^{p} |\theta_j| ]\]</span> L1正则化可以视为假设参数符合拉普拉斯分布时的最大后验估计。</p><h3 id="最大后验估计map">最大后验估计（MAP）</h3><p>MAP估计在MLE的基础上，考虑了参数的先验分布。其目标函数为： <spanclass="math display">\[[\hat{\theta}=\arg\max_{\theta}P(\theta|X)]\]</span> 利用贝叶斯定理可以展开为：</p><p><span class="math display">\[[\hat{\theta}=\arg\max_\theta\left[\log P(X|\theta)+\logP(\theta)\right]]\]</span>前者是似然函数，后者是先验分布。通过对数变换和相加的方式，将<strong>最大化后验概率</strong>的问题转化为<strong>最大化对数似然函数</strong>与<strong>对数先验分布之和</strong>的问题。</p><h2 id="贝叶斯估计bayesian-estimation">贝叶斯估计（BayesianEstimation）</h2><p>贝叶斯估计（BayesianEstimation）与频率学派的视角不同。贝叶斯学派认为数据是固定的，但参数是随机的，并且参数的估计应基于其全分布而不是点估计。</p><p>贝叶斯估计的核心在于求解后验分布： <span class="math display">\[[P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}]\]</span> 其中，<spanclass="math inline">\(P(X)\)</span>是证据（evidence），用于归一化。</p><p>在实际应用中，贝叶斯估计通常也会使用对数形式： <spanclass="math display">\[\log P(\theta|X) = \log P(X|\theta) + \log P(\theta) - \log P(X)\]</span>通过这种方式，我们可以更加灵活地处理不确定性，并且可以自然地引入先验信息。</p><h2 id="统一理解">统一理解</h2><p>​损失函数在深度学习中的应用广泛，虽然种类繁多，但从概率分布和参数估计的角度，我们可以将其统一起来理解。通过最大似然估计（MLE）、最大后验估计（MAP）和贝叶斯估计（BayesianEstimation），我们能够更系统地理解损失函数及其背后的统计学原理。</p><h3 id="回归问题">回归问题</h3><p>​回归问题中常用的是均方误差（MSE），其本质是最大似然估计在假设误差服从高斯分布下的特例。L2和L1正则化则分别对应参数服从高斯分布和拉普拉斯分布的最大后验估计。</p><h3 id="分类问题">分类问题</h3><p>​分类问题中常用的是交叉熵损失，其本质是最大似然估计在假设数据服从伯努利分布或多项分布下的特例。</p><h3 id="正则化">正则化</h3><p>​正则化可以视为在最大似然估计的基础上引入先验分布，从而转化为最大后验估计。L2正则化对应高斯分布的先验，L1正则化对应拉普拉斯分布的先验。</p><h2 id="总结">总结</h2><p>通过从概率分布和参数估计的角度重新梳理损失函数的定义，我们可以更高效地理解和应用各种损失函数及其变体。最大似然估计、最大后验估计和贝叶斯估计提供了统一的框架，使我们能够更系统地看待损失函数及其在机器学习和深度学习中的应用。</p><p>希望这篇文章能帮助大家在学习和应用损失函数时，从更高的角度和更深的层次理解其精髓。随着对这些概念的深入理解，我们可以更灵活地选择和设计适合具体问题的损失函数，从而提升模型的性能和泛化能力。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;损失函数&quot;&gt;损失函数&lt;/h1&gt;
&lt;p&gt;​ 在&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深</summary>
      
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.gitee.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>scanf()函数与printf()中的格式说明符</title>
    <link href="https://cuiluyi.gitee.io/2024/08/11/[object%20Object]/C++/%E3%80%90%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E3%80%91/"/>
    <id>https://cuiluyi.gitee.io/2024/08/11/[object%20Object]/C++/%E3%80%90%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E3%80%91/</id>
    <published>2024-08-11T09:29:49.000Z</published>
    <updated>2024-08-17T06:29:33.465Z</updated>
    
    <content type="html"><![CDATA[<h1id="scanf函数与printf中的格式说明符">scanf()函数与printf()中的格式说明符</h1><h2id="scanf与printf中格式说明符几乎相同的一部分">scanf（）与printf（）中格式说明符几乎相同的一部分</h2><table border="1" cellpadding="1" cellspacing="1" style="width: 600px; text-align: central;"><tbody><tr><td>%d</td><td>用来输入和输出int</td></tr><tr><td>%ld</td><td>用来输入和输出long</td></tr><tr><td>%lld</td><td>用来输入和输出long long</td></tr><tr><td>%hd</td><td>用来输入和输出short</td></tr><tr><td>%i</td><td>用来输入和输出有符号十进制整数</td></tr><tr><td>%u</td><td>用来输入和输出无符号十进制整数</td></tr><tr><td>%lu</td><td>用来输入和输出无符号十进制长整数</td></tr><tr><td>%llu</td><td>用来输入和输出无符号十进制长长整数</td></tr><tr><td>%hu</td><td>用来输入和输出无符号短十进制整数</td></tr><tr><td>%o</td><td>用来输入和输出八进制整数</td></tr><tr><td>%lo</td><td>用来输入和输出长八进制整数</td></tr><tr><td>%ho</td><td>用来输入和输出短八进制整数</td></tr><tr><td>%#o</td><td>用来输出八进制整数，数字前有0</td></tr><tr><td>%x</td><td>用来输入和输出十六制整数，字母小写</td></tr><tr><td>%#x</td><td>用来输出十六制整数，字母小写，数字前有0x</td></tr><tr><td>%lx</td><td>用来输入和输出长十六制整数，字母小写</td></tr><tr><td>%X</td><td>用来输入和输出十六制整数，字母大写 </td></tr><tr><td>%#X</td><td>用来输出十六制整数，字母大写 ，数字前有0X</td></tr><tr><td>%lX</td><td>用来输入和输出长十六制整数，字母大写</td></tr><tr><td>%c</td><td>用来输入和输出单个字符</td></tr><tr><td>%s</td><td><p>用来输入和输出一串字符串</p><p>输入时遇空格，<a            href="https://so.csdn.net/so/search?q=%E5%88%B6%E8%A1%A8%E7%AC%A6&amp;spm=1001.2101.3001.7020"            target="_blank"            class="hl hl-1"            data-report-click='{"spm":"1001.2101.3001.7020","dest":"https://so.csdn.net/so/search?q=%E5%88%B6%E8%A1%A8%E7%AC%A6&amp;spm=1001.2101.3001.7020","extra":"{\"searchword\":\"制表符\"}"}'            data-tit="制表符"            data-pretit="制表符"            >制表符</a          >或换行符结束</p><p>输出时连格式说明符一起输出</p><p>printf（"%s","%d%f",a,b）;输出 %d%f</p></td></tr><tr><td>%f</td><td>用来输入和输出float，输出double</td></tr><tr><td>%lf</td><td>用来输入和输出double（double输出用%f和%lf都可以）</td></tr><tr><td>%Lf</td><td>用来输入和输出long double</td></tr><tr><td>%e</td><td>用来输入和输出指数，字母小写</td></tr><tr><td>%le</td><td>用来输入和输出长指数，字母小写</td></tr><tr><td>%E</td><td>用来输入和输出指数，字母大写</td></tr><tr><td>%lE</td><td>用来输入和输出长指数，字母大写</td></tr><tr><td>%g</td><td>用来输入和输出指数或float（输出最短的一种），字母小写</td></tr><tr><td>%lg</td><td>用来输入和输出长指数或double（输出最短的一种），字母小写</td></tr><tr><td>%G</td><td>用来输入和输出指数或float（输出最短的一种），字母大写</td></tr><tr><td>%lG</td><td>用来输入和输出长指数或double（输出最短的一种），字母大写</td></tr></tbody></table><h2 id="scanf独有">scanf()独有</h2><table border="1" cellpadding="1" cellspacing="1" style="width: 600px"><tbody><tr><td>%<em>（所有类型），如%</em>d</td><td><p>用来输入一个数，字符或字符串而不赋值（跳过无关输入）</p><p>如scanf("%d%*c%d",&amp;a,&amp;b);</p><p>这样就可以只将1+2中的1和2赋值给a和b。</p></td></tr><tr><td><p>%m（所有类型），其中m为常数</p></td><td>限定输入范围，如scanf（“%4d”，&amp;a）时输入123456，只把1234赋值给a</td></tr><tr><td>，(逗号）</td><td>无实际用处，仅用于美观。如scanf（“%d,%d,%d”,&amp;a,&amp;b,&amp;c）;</td></tr><tr><td>-（横杠）    ：（冒号）</td><td><p>方便日期等输入，但不赋值</p><p>scanf（“%d-%d-%d”,&amp;a,&amp;b,&amp;c）;需输入2018-11-20</p><p>scanf（“%d：%d：%d”：&amp;a,&amp;b,&amp;c）;需输入2018:11:20</p></td></tr><tr><td><p>所有字符串，符号（包括空格）</p><p>数字（不与输入数相挨）</p></td><td><p>任何所写的东西都必须如横杠一般先输入（不赋值），不然系统报错</p><p>scanf（“%d 456 %d”,&amp;a,&amp;b）;</p><p>需输入  1 456 7（1和7之间有 456 （前后各一个空格））</p><p>结果为a=1  b=7</p></td></tr></tbody></table><h2 id="printf独有">printf()独有</h2><table border="1" cellpadding="1" cellspacing="1" style="width: 600px"><tbody><tr><td><p>%m.nd     %-m.nd</p><p>（m和n为常数）</p></td><td><p>m用于在d位数小于m时补空格（<a            href="https://so.csdn.net/so/search?q=%E5%8F%B3%E5%AF%B9%E9%BD%90&amp;spm=1001.2101.3001.7020"            target="_blank"            class="hl hl-1"            data-report-click='{"spm":"1001.2101.3001.7020","dest":"https://so.csdn.net/so/search?q=%E5%8F%B3%E5%AF%B9%E9%BD%90&amp;spm=1001.2101.3001.7020","extra":"{\"searchword\":\"右对齐\"}"}'            data-tit="右对齐"            data-pretit="右对齐"            >右对齐</a          >）d位数大于m时忽略</p><p>如%5d，输出123，          <u            ><span style="color: #3399ea">  </span></u          >123（123前面两个空格）</p><p></p><p>.n用于在d位数小于n时补0（右对齐）d位数大于n时忽略</p><p>如%.5d，输出123，          00123（123前面两个0）</p><p></p><p>%-m.nd则为左对齐</p></td></tr><tr><td><p>%m.nf  %m.nlf  %m.nLf</p><p>%-m.nf %-m.nlf %-m.nLf</p></td><td><p>m用于在小数位数小于m时补空格（右对齐）</p><p>小数位数大于m时忽略      (小数点算一位）</p><p>如%6f  需输出3.14            结果为<u>  </u>3.14（3.14前面两个空格）</p><p></p><p>.n用于控制小数位数      小数部分长度大于n则四舍五入</p><p>小数部分长度小于于n则补0</p><p>如%.6f  需输出3.14           结果为3.140000</p><p>如%6f  需输出3.1415926  结果为3.141593</p><p></p><p>%-m.nf %-m.nlf %-m.nLf   则为左对齐</p></td></tr><tr><td>%m.ns     %-m.ns</td><td><p>m用于在字符串位数小于m时补空格（右对齐）字符串位数大于m时忽略</p><p>如%5s，输出abc，          <u            ><span style="color: #3399ea">  </span></u          >abc（abc前面两个空格）</p><p></p><p>.n用于控制字符串位数      长度大于n则仅输出前n位</p><p>字符串长度小于于n时忽略</p><p>如%.6s 需输出abcdefg      结果为abcdef</p><p>如%6f  需输出abc              结果为abc</p><p></p><p>%-m.ns则为左对齐</p></td></tr><tr><td>%mc        %-mc</td><td><p>m限制char的输出长度       当m&gt;1时，在左方补m-1个空格</p><p></p><p>%-mc则为左对齐</p></td></tr><tr><td><p>%m.ne     %-m.ne</p><p>%m.nE     %-m.nE</p></td><td><p>m用于控制指数长度，在QT中，指数部分占五位（如 e+001 ）</p><p>位数小于m时左方补空格   位数大于m时忽略</p><p>如printf("%15.5e",a);          设a为123.456789</p><p>结果为 <u>   </u>1.23457e+002（三个空格）</p><p></p><p>.n用于控制小数长度           小数部分长度大于n则四舍五入</p><p>小数部分长度小于于n则补0</p><p>如printf("%15.5e",a);          设a为123.456789</p><p>结果为 <u>   </u>1.23457e+002（三个空格）</p><p></p><p>%-m.ne   %-m.nE则为左对齐</p></td></tr><tr><td><p>%<em>（整型）如%</em>d</p><p>%-*（整型）</p></td><td><p>在输出项中规定整型数据的宽度，少于限制补空格，大于忽略</p><p>如printf（“%*d”,a,b）;</p><p>a=5  b=123</p><p>结果为 <u>  </u>123（前面有两个空格）</p><p></p><p>%-*（整型） 左对齐</p></td></tr><tr><td>%0*（整型）</td><td><p>在输出项中规定整型数据的宽度，少于限制补0，大于忽略</p><p>如printf（“%0*d”,a,b）;</p><p>a=5  b=123</p><p>结果为 00123</p></td></tr><tr><td>注意</td><td><p>printf（）中的运算是从右至左，而输出是从做左至右</p><p>如a=1，printf（“%d %d %d”,a++,a++,a++）;</p><p>结果为3 2 1</p></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1
id=&quot;scanf函数与printf中的格式说明符&quot;&gt;scanf()函数与printf()中的格式说明符&lt;/h1&gt;
&lt;h2
id=&quot;scanf与printf中格式说明符几乎相同的一部分&quot;&gt;scanf（）与printf（）中格式说明符几乎相同的一部分&lt;/h2&gt;
&lt;t</summary>
      
    
    
    
    
    <category term="C++" scheme="https://cuiluyi.gitee.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.gitee.io/2024/08/09/[object%20Object]/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%8410%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation_Function%EF%BC%89%E6%80%BB%E7%BB%93/"/>
    <id>https://cuiluyi.gitee.io/2024/08/09/[object%20Object]/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%8410%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation_Function%EF%BC%89%E6%80%BB%E7%BB%93/</id>
    <published>2024-08-09T14:49:59.772Z</published>
    <updated>2024-08-10T02:17:09.377Z</updated>
    
    <content type="html"><![CDATA[<h1 id="激活函数">激活函数</h1><h2 id="简介">简介</h2><p>激活函数（Activation Function），就是在<ahref="https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/382460"title="人工神经网络">人工神经网络</a>的神经元上运行的<ahref="https://baike.baidu.com/item/%E5%87%BD%E6%95%B0/301912"title="函数">函数</a>，负责将神经元的输入映射到输出端，旨在帮助网络学习数据中的复杂模式。</p><p>下图展示了一个神经元是如何输入激活函数以及如何得到该神经元最终的输出：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/34d6b14bf4b79175276eac52abacdf94.png" /></p><h2 id="为什么要用激活函数">为什么要用激活函数</h2><ul><li><p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的<ahref="https://baike.baidu.com/item/%E6%84%9F%E7%9F%A5%E6%9C%BA/12723581"title="感知机">感知机</a>（Perceptron）</p></li><li><p>使用激活函数能够给神经元引入<strong>非线性因素</strong>，使得神经网络可以任意<strong>逼近</strong>任何<strong>非线性函数</strong>，使深层神经网络表达能力更加强大，这样神经网络就可以应用到众多的非线性模型中</p><ol type="1"><li><p>附加到网络中的每个神经元，并根据每个神经元的输入来确定是否应激活</p></li><li><p>有助于将每个神经元的输出标准化到1到0或-1到1的范围内</p></li></ol></li></ul><h2 id="激活函数的分类">激活函数的分类</h2><p>激活函数可以分为<strong>两大类：</strong></p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/e41598583c8b35dcbb907f9fd66d8351.png" /></p><ul><li><strong>饱和激活函数：</strong> sigmoid、 tanh...</li><li><strong>非饱和激活函数:</strong>  ReLU 、Leaky Relu 、ELU、PReLU、RReLU...</li></ul><p>首先，我们先了解一下<strong>什么是饱和</strong>？</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/9a7a78e50434edbcad6939b64efee0e1.png" /></p><p>反之，不满足以上条件的函数则称为非饱和激活函数。</p><p>        <strong>Sigmoid函数</strong>需要一个实值输入压缩至[<strong>0,1]</strong>的范围        <strong>tanh函数</strong>需要讲一个实值输入压缩至<strong>[-1, 1]</strong>的范围</p><p>相对于饱和激活函数，使用<strong>非饱和激活函数</strong>的<strong>优势</strong>在于两点：</p><p>   1.非饱和激活函数能解决深度神经网络（层数非常多）带来的<strong>梯度消失</strong>问题</p><p>    2.使用非饱和激活函数能<strong>加快收敛速度</strong>。</p><h2 id="常见的几种激活函数">常见的几种激活函数</h2><h3 id="sigmoid函数"><ahref="https://so.csdn.net/so/search?q=Sigmoid%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">Sigmoid函数</a></h3><p><strong>Sigmoid</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/6c9269a456ee0497c9bb0d1383e51cc8.png" /></p><p>导数表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/e646103baf196c690998f97ee5987716.png" /></p><p> 函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/af5f15ef44f1b276c78bec5e140e7082.png" /></p><p> Sigmoid函数在历史上曾非常常用，输出值范围为[0,1]之间的实数。但是现在它已经不太受欢迎，实际中很少使用。</p><p><strong>什么情况下适合使用Sigmoid？</strong></p><ul><li><p>Sigmoid 函数的输出范围是 0 到1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。</p></li><li><p>梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度</p></li></ul><p><strong>Sigmoid有哪些缺点？</strong></p><ul><li><p>容易造成梯度消失。我们从导函数图像中了解到sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习。</p></li><li><p>函数输出不是以 0为中心的，梯度可能就会向特定方向移动，从而降低权重更新的效率</p></li><li><p>Sigmoid函数执行指数运算，计算机运行得较慢，比较消耗计算资源。</p></li></ul><h3 id="tanh函数">Tanh函数</h3><p><strong>tanh</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/70ad61d6f8c019d3dee98ad6ef45cb1a.png" /></p><p> 函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/e58d7204fef7f7410e92b4bf82e91938.png" /></p><p> 实际上，Tanh函数是 sigmoid 的变形：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/4d6487fb47aa7a4bd310d1181ef19884.png" /></p><p> 与sigmoid不同的是，tanh是“零为中心”的。因此在实际应用中，tanh会比sigmoid更好一些。但是在饱和神经元的情况下，tanh还是没有解决梯度消失问题。</p><p><strong>什么情况下适合使用Tanh</strong>？</p><ul><li><p>tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid函数更好；</p></li><li><p>在 tanh图中，负输入将被强映射为负，而零输入被映射为接近零。</p></li></ul><p><strong>Tanh有哪些缺点？</strong></p><ul><li><p>仍然存在梯度饱和的问题</p></li><li><p>依然进行的是指数运算</p></li></ul><h3 id="relu函数"><ahref="https://so.csdn.net/so/search?q=ReLU%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">ReLU函数</a></h3><p><strong>ReLU</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/ae65877d14768ad1b6789c6ccdcd6e31.png" /></p><p>函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/a2f4f9f3c522518908e20d7865beb48c.png" /></p><p><strong>什么情况下适合使用ReLU？</strong></p><ul><li><p>ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和</p></li><li><p>由于ReLU线性、非饱和的性质，在SGD中能够快速收敛</p></li><li><p>计算复杂度低，不需要进行指数运算</p></li></ul><p><strong>ReLU有哪些缺点？</strong></p><ul><li><p>与Sigmoid一样，其输出不是以0为中心的</p></li><li><p>Dead ReLU问题。当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新</p></li></ul><p>训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活。所以，要设置一个合适的较小的学习率，来降低这种情况的发生</p><h3 id="leaky-relu函数">Leaky Relu函数</h3><p><strong>Leaky Relu</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/88b3993e5054a035be6962f0b3e2ab0b.png" /></p><p> 函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/f08a4f516132c6965a99f4e4ede084e5.png" /></p><p><strong>什么情况下适合使用Leaky ReLU？</strong></p><ul><li><p>解决了ReLU输入值为负时神经元出现的死亡的问题</p></li><li><p>Leaky ReLU线性、非饱和的性质，在SGD中能够快速收敛</p></li><li><p>计算复杂度低，不需要进行指数运算</p></li></ul><p><strong>Leaky ReLU有哪些缺点？</strong></p><ul><li><p>函数中的α，需要通过先验知识人工赋值（一般设为0.01）</p></li><li><p>有些近似线性，导致在复杂分类中效果不好。</p></li></ul><p><strong>注意：</strong>从理论上讲，Leaky ReLU 具有 ReLU的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明Leaky ReLU 总是比 ReLU 更好</p><h3 id="prelu函数">PRelu函数</h3><p><strong>PRelu</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/e139c27fe15b02144158329284c4d81f.png" /></p><p> 函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/fd071b3c80f462bc94a1fff457211c06.png" /></p><p> PRelu激活函数也是用来解决ReLU带来的神经元坏死的问题。与LeakyReLU激活函数不同的是，PRelu激活函数负半轴的斜率参数<strong>α</strong>是通过学习得到的，而不是手动设置的恒定值</p><h3 id="elu函数">ELU函数</h3><p><strong>ELU</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/b70bac8ea93f96755952563c81c430d6.png" /></p><p> 函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/93bfa8c9fd8447aa8503a2c75f6308ce.png" /></p><p> 与LeakyReLU和PRelu激活函数不同的是，ELU激活函数的负半轴是一个指数函数而不是一条直线</p><p><strong>什么情况下适合使用ELU？</strong></p><ul><li><p>ELU试图将激活函数的输出均值接近于零，使正常梯度更接近于单位自然梯度，从而加快学习速度</p></li><li><p>ELU在较小的输入下会饱和至负值，从而减少前向传播的变异和信息</p></li></ul><p><strong>ELU有哪些缺点？</strong></p><ul><li>计算的时需要计算指数，计算效率低</li></ul><h3 id="selu函数">SELU函数</h3><p><strong>SELU</strong>激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/32a0b0737b9af34efab5dc2b9881cfff.png" /></p><p> 其中λ = 1.0507 , α = 1.6733 </p><p>函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/ac5337b33a267bc0e10e6f7125f3cecb.png" /></p><p> SELU 允许构建一个映射 g，其性质能够实现 SNN（自归一化神经网络）。SNN不能通过ReLU、sigmoid 、tanh 和 Leaky ReLU实现。这个激活函数需要有：（1）负值和正值，以便控制均值；（2）饱和区域（导数趋近于零），以便抑制更低层中较大的方差；（3）大于1的斜率，以便在更低层中的方差过小时增大方差；（4）连续曲线。后者能确保一个固定点，其中方差抑制可通过方差增大来获得均衡。通过乘上指数线性单元（ELU）来满足激活函数的这些性质，而且λ&gt;1 能够确保正值净输入的斜率大于 1</p><p>SELU激活函数是在自归一化网络中定义的，通过调整均值和方差来实现内部的归一化，这种内部归一化比外部归一化更快，这使得网络能够更快得收敛</p><h3 id="swish函数">Swish函数</h3><p>Swish激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/ebfc932dd7e11123ece4d00624a800ac.png" /></p><p> 函数图像如下：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/cd5fbc5ff57d321fae3a88975d3ad19d.png" /></p><p> Swish激活函数<strong>无界性</strong>有助于防止慢速训练期间，梯度逐渐接近0并导致饱和；同时，<strong>有界性</strong>也是有优势的，因为有界激活函数可以具有很强的正则化(防止过拟合，进而增强泛化能力)，并且较大的负输入问题也能解决</p><p>Swish激活函数在<code>x=0</code>附近更为平滑，而非单调的特性增强了输入数据和要学习的权重的表达能力。</p><h3 id="mish函数">Mish函数</h3><p>Mish激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/39c694b4d7bfbea87c7f5bae27eda51f.png" /></p><p> 函数图像如下:</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/b5520519b3392bc1618009ec673c396a.png" /></p><p>Mish激活函数的函数图像与Swish激活函数类似，但要更为平滑一些，缺点是计算复杂度要更高一些</p><h3 id="softmax函数">Softmax函数</h3><p>Softmax激活函数的数学表达式为：</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/91781adb7d2a642f3fe2fd132f4dedb0.png" /></p><p> 函数图像如下:</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/05b86dc2a65dfa843851e7d8dbc1b7c9.png" /></p><p> Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大</p><p>下图给出了Softmax作激活函数对输出值映射处理过程，进而形象理解Softmax函数</p><p><imgsrc="https://i-blog.csdnimg.cn/blog_migrate/7a38f41623bcda0fa735cad89ee62a8b.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;激活函数&quot;&gt;激活函数&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;激活函数（Activation Function），就是在&lt;a
href=&quot;https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>项目经历</title>
    <link href="https://cuiluyi.gitee.io/2024/08/09/[object%20Object]/%E4%BF%9D%E7%A0%94/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/"/>
    <id>https://cuiluyi.gitee.io/2024/08/09/[object%20Object]/%E4%BF%9D%E7%A0%94/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/</id>
    <published>2024-08-09T01:46:27.000Z</published>
    <updated>2024-08-09T14:22:07.401Z</updated>
    
    <content type="html"><![CDATA[<p>基于 <span class="math inline">\(sketch\)</span>的高效通信联邦学习客户端选择算法</p><ul><li><strong>内容：</strong>现有的FL系统中，参与每轮更新的客户端通常数量固定且由服务端随机选择，但这种<strong>随机选择客户端的方式会加剧数据异质性（Non-IID）的不利影响</strong>，降低模型的收敛速度和准确率，导致通信代价大幅增加。因此我们提出了一种新的客户端选择算法：在FetchSGD的基础上，算法根据客户端的数据分布、本地损失、与服务器模型参数的差异程度计算其重要程度，服务端基于客户端重要程度进行选择，在保证随机性的同时算法倾向于选择某些更加重要的客户端；此外我们还基于<spanclass="math inline">\(sketch\)</span>实现了在上行链路与下行链路降低通信开销，同时提出skipcommunication算法并运用周期平均思想减少通信的轮数，实现全面降低联邦学习的通信开销</li><li><strong>贡献</strong>：独立完成，论文在写</li></ul><p>对大模型分布式训练技术（如TP、SP、PP）等有一定研究，曾在北京大学数据科学与工程研究所有一段线上的实习经历；此外本人曾对基于sketch解决联邦学习通信效率有一定的研究，以下为个人想法：</p><ol type="1"><li><p>设计并实现了基于FetchSGD的客户端选择算法，通过综合考量数据分布、本地损失及模型参数差异，有效缓解了数据异质性问题，预期提升模型收敛速度15%，准确率提高3个百分点。</p></li><li><p>引入sketch技术优化通信链路，在上行与下行传输中显著降低通信数据量，预计减少通信成本约40%，增强联邦学习效率。</p></li><li><p>创新提出skipcommunication算法，结合周期平均策略，灵活调整通信轮次，成功减少不必要通信轮数20%，进一步降低总体通信开销。</p></li><li><p>独立承担从算法设计到实验验证的全过程，目前正处于关键验证阶段，致力于全面评估并优化新算法在联邦学习环境下的性能表现。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;基于 &lt;span class=&quot;math inline&quot;&gt;&#92;(sketch&#92;)&lt;/span&gt;
的高效通信联邦学习客户端选择算法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;内容：&lt;/strong&gt;现有的FL系统中，参与每轮更新的客户端通常数量固定且由服务端随机选择，但这</summary>
      
    
    
    
    
    <category term="保研" scheme="https://cuiluyi.gitee.io/tags/%E4%BF%9D%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.gitee.io/2024/08/07/[object%20Object]/%E4%BF%9D%E7%A0%94/sjtu%E7%94%B5%E9%99%A2%E5%A5%BD%E8%80%81%E5%B8%88/"/>
    <id>https://cuiluyi.gitee.io/2024/08/07/[object%20Object]/%E4%BF%9D%E7%A0%94/sjtu%E7%94%B5%E9%99%A2%E5%A5%BD%E8%80%81%E5%B8%88/</id>
    <published>2024-08-07T01:34:40.165Z</published>
    <updated>2024-08-14T13:57:11.681Z</updated>
    
    <content type="html"><![CDATA[<p><ahref="https://www.yankong.org/review?professor=bf395d5dcafcc2c413fab6ac775a00f7bc7446cb">朱燕民- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=cb5b9d9f9a6f31c9e8d81c03ff7262fe30912935">朱浩瑾- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=254fdcef16ffbe3deed4069766b16bda3337af4b">赵海- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=1c9b2f2de7a8373783781d4abfbab74c1438e47e">张娅- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=33f03f9161d959053af920fd405ccb57ee788003">汪小帆- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=866ff9dbde98a00147f61fd631a7ae525780385b">申丽萍- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=35edcf92b9036d8d15e0ec855b92ea16be84aebd">卢宏涛- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=e4b1e894162c8f97ca1d9277ef3a4e94afcec0ab">卢策吾- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=463fa334705ab0305f26dae10dea8dfb008fbaed">刘胜利- 导师评价 (yankong.org)</a></p><p><ahref="https://www.yankong.org/review?professor=3d79ed31d8127e99851231cb90d3175be71c7e30">程帆- 导师评价 (yankong.org)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a
href=&quot;https://www.yankong.org/review?professor=bf395d5dcafcc2c413fab6ac775a00f7bc7446cb&quot;&gt;朱燕民
- 导师评价 (yankong.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a
hr</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.gitee.io/2024/06/11/[object%20Object]/python/pytorch/"/>
    <id>https://cuiluyi.gitee.io/2024/06/11/[object%20Object]/python/pytorch/</id>
    <published>2024-06-11T09:26:47.884Z</published>
    <updated>2024-06-11T09:26:47.884Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>HTTP例题</title>
    <link href="https://cuiluyi.gitee.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E4%BE%8B%E9%A2%98/"/>
    <id>https://cuiluyi.gitee.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E4%BE%8B%E9%A2%98/</id>
    <published>2024-05-19T01:46:27.000Z</published>
    <updated>2024-07-07T05:35:28.088Z</updated>
    
    <content type="html"><![CDATA[<h1 id="http">HTTP</h1><h2 id="http协议">HTTP协议</h2><ul><li><span class="math inline">\(HTTP/1.1\)</span>默认使用<strong><font color='red'>持续</font></strong>、<strong><font color='blue'>非流水线</font></strong>方式</li></ul><h2 id="持续http连接">持续HTTP连接</h2><p>Client向 Server 请求共包含 4 个对象（1 个 HTML 基文件和 3 个 JPEG图片)的页面</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202406111656225.jpg"alt="8713353fc94f5ca8948d4f31e81012a" /><figcaptionaria-hidden="true">8713353fc94f5ca8948d4f31e81012a</figcaption></figure><h2 id="非持续连接">非持续连接</h2><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202406111659324.png"alt="image-20240611165958239" /><figcaption aria-hidden="true">image-20240611165958239</figcaption></figure><blockquote><p>==<strong>先请求HTML基文件，接收方收到HTML文件之后，才能再请求HTML的内嵌对象</strong>==</p></blockquote><h2 id="例题">例题</h2><h3 id="例1">例1</h3><ol type="1"><li>假设在客户端浏览器中通过点击某个超链接获得另一个Web页面。如果客户端只知道Web页面所在服务器的名字，但不知道该服务器的IP地址。因此，客户端必须使用DNS查询服务器的IP地址。假设，客户端使用DNS获得Web服务器的IP地址共耗时t<sub>dns</sub>。进一步假定，该Web页面共有10个非常小的对象。令RTT<sub>0</sub>表示客户端和Web服务器间数据传输的RTT值，<strong>HTML文档及内嵌对象的<font color='red'>传输时间(发送时间)均忽略不计</font></strong>。在下列情况下，从点击该超链接开始，到接收到完整的页面分别需要多少时间？(注意，比须解释所得结果的理由)</li></ol><p>(1)<font color='blue'><strong>无并行</strong></font>连接的<strong><font color='blue'>非持续</font></strong>HTTP</p><blockquote><p><strong>答:(1)DNS解析所需的tdns；使用串行非持续连接，获得一个对象需要2RTT0。因此，共需20RTT0+tdns</strong>。</p></blockquote><p>(2)<strong><font color='blue'>有流水线</font></strong>的持续HTTP</p><blockquote><p><strong>答:(2) 有流水线的持续HTTP，DNS解析需要tdns。建立TCP连接耗时RTT0；然后，获得页面的html文档耗时RTT0；获得页面其他对象耗时RTT0。综上，共耗时：3RTT0+tdns。</strong></p></blockquote><h3 id="例2">例2</h3><p>​浏览器与WWW服务器之间的网络路径被抽象为一条传输速率为100Mbps的链路。假设浏览器要从服务器下载100Kbits(1K=1000)长的页面，并包含10个嵌入的图像（文件名分别img01.jpg,img02.jpg……img10.jpg），每个图像文件大小都是100Kbits，页面和10个图像存储在同一服务器中。从浏览器到服务器的往返时间（RTT）为300毫秒。由于GET方法的request消息比较小，<strong><font color='red'>忽略客户端发送GETrequest消息需要的时间</font></strong>，但需要考虑在链路上<strong><font color='red'>传输HTML基文件和嵌入对象所需的时间</font></strong>，以及建立TCP连接所需要的时间（假设TCP连接建立时间为1RTT）。</p><ol type="1"><li>使用<strong><font color='blue'>非持续HTTP</font></strong>（假设在浏览器和服务器之间<strong><font color='blue'>没有并行连接</font></strong>），响应时间多长？即从用户请求URL到页面和嵌入的图像都到达浏览器需要多长时间？注意要描述导致延迟的不同部分。</li></ol><blockquote><p><strong>一个RTT时间请求，一个RTT响应并返回请求的文件+文件传数据时间。由于是没有并行链接的非持续HTTP链接，每个对象都需要逐个请求所以是：(2RTT+0.001)*11=6.611s</strong>*</p></blockquote><ol start="2" type="1"><li>仍然使用<strong>非持续连接</strong>，假设浏览器想打开多少到服务器的并行连接都可以。这种情况下，响应时间是多少？</li></ol><blockquote><p><strong><font color='blue'>先请求HTML基文件，2RTT+文件传输时间。</font>现在知道了对象的个数，创建一个TCP链接2RTT，但是由于文件储存在同一服务器中，所以服务器需要逐个发送文件，因此时间是10个文件传输时间，所以总时间是：(2RTT+0.001)+(2RTT+10*0.001)=1s</strong></p></blockquote><ol start="3" type="1"><li>假设使用<strong><font color='blue'>持续</font></strong>HTTP连接，但<strong>没有使用流水线机制</strong>。这种情况下响应时间是多少？</li></ol><blockquote><p><strong>使用无流水的持续HTTP链接，服务器的连续发送基文档和内嵌文件对象，建立连接1个RTT，每个对象是1RTT+文件传输时间所以总时间是：1RTT+11*(1RTT+对象传输时间)=0.3+11(0.3+0.001)=3.611s。</strong></p></blockquote><ol start="4" type="1"><li>假设使用<strong><font color='blue'>流水线的持续</font></strong>HTTP，响应时间是多少？</li></ol><blockquote><p><strong>使用流水线的持续HTTP，首先建立链接1个RTT，然后获取HTML基文档1RTT+文件传输时间，内嵌对象的请求背靠背的从客户端发送1RTT，服务器则一次发送文件对象10个文件传输时间，所以总时间是：3RTT+11*0.001=0.911s</strong></p></blockquote><h3 id="例3">例3</h3><p>​假设在浏览器键入地址(URL)后从某个WWW服务器获取一个Web页面，<strong><font color='blue'>该服务器的IP地址已缓存在本地</font></strong>。用RTT表示从客户机到Web服务器的往返时间（RTT）。假设该Web页面由一个基本的HTML文件和五个小图像组成，<strong><font color='red'>每个对象的发送时间与RTT相比很小，可以忽略不计。</font></strong>在下列情况下，从用户输入地址后按下回车键到接收到完整的Web页面需要多长时间？</p><ol type="1"><li><p>没有并行连接的非持续HTTP——<strong>12RTT</strong></p></li><li><p><strong><font color='blue'>最多有五个并行连接的非持续</font></strong>HTTP——<strong>4RTT</strong></p></li><li><p>使用流水线的持续HTTP——<strong>3RTT</strong></p></li></ol><h3 id="例4">例4</h3><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191658229.png"alt="image-20230129172238917" /><figcaption aria-hidden="true">image-20230129172238917</figcaption></figure><h3 id="例5">例5</h3><p>17.【2022 统考真题】假设主机 H 通过 HTTP/1.1 请求浏览某 Web 服务器S上的 Web页news408.html，news408.html引用了同目录下的1幅图像，news408.html文件大小为1MSS(最大段长)，图像文件大小为3MSS，H访问S的往返时间 RTT=10 ms，忽略HTTP响应报文的首部开销和 TCP段传输时延。若H已完成域名解析,则从H请求与S建立TCP连接时刻起，到接收到全部内容止，所需的时间至少是（）</p><p>A.30ms B. 40ms C.50ms D.60ms</p><ul><li><p><span class="math inline">\(HTTP/1.1\)</span>默认使用<strong><font color='red'>持续</font></strong>、<strong><font color='blue'>非流水线</font></strong>方式</p></li><li><p>要求最少时间，理想的情况是TCP在第3次握手的报文段中捎带了 HTTP请求，以及传输过程中的慢开始阶段不考虑拥塞</p></li><li><p>假设接收方有足够大的缓存空间，即发送窗口等同于拥塞窗口，共需要经过：</p><ul><li>第1个RTT，进行TCP连接建立的前两次握手</li><li>第2个RTT，主机C发送第3次握手报文并捎带了对html文件的HTTP请求，<strong>TCP连接刚建立时服务器S的发送窗口=1MSS</strong>，服务器S发送大小为1MSS的 html文件</li><li>第3个RTT，主机C发送对 html 文件的确认并捎带了对图形文件的HTTP请求，<strong>服务器S收到确认后发送窗口变为2MSS</strong>，然后服务器S发送大小为2MSS的图像文件</li><li>第4个RTT，主机C向服务器S发送对收到的部分图像文件的确认，<strong>服务器S收到确认后发送窗口变为4MSS</strong>，然后服务器S发送剩下的1MSS 图像文件，完成传输，</li></ul><p>共需要4个RTT，即 40ms。整个传输过程如下图所示：</p></li></ul><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202407071335383.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;http&quot;&gt;HTTP&lt;/h1&gt;
&lt;h2 id=&quot;http协议&quot;&gt;HTTP协议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(HTTP/1.1&#92;)&lt;/span&gt;
默认使用&lt;strong&gt;&lt;font color=&#39;red&#39;</summary>
      
    
    
    
    
    <category term="计算机网络" scheme="https://cuiluyi.gitee.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>CRC</title>
    <link href="https://cuiluyi.gitee.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/CRC/"/>
    <id>https://cuiluyi.gitee.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/CRC/</id>
    <published>2024-05-19T01:46:27.000Z</published>
    <updated>2024-05-19T09:56:35.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="crc">CRC</h1><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656975.jpg"alt="258a1611faa096e4f1367a79335fb90" /><figcaptionaria-hidden="true">258a1611faa096e4f1367a79335fb90</figcaption></figure><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656977.jpg" alt="3b36f8ff943c741c889d81022a745df" style="zoom:67%;" /></p><blockquote><ol type="1"><li>生成多项式==&gt;除数</li><li>除数为n位<ul><li>余数为n-1位<strong><font color='red'>(不够的在前面补0)</font></strong></li><li>被除数后面加上<strong><font color='red'>n-1位‘0’</font></strong></li></ul></li></ol></blockquote><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656978.jpg"alt="05c133fba1b7e11497708c5df82366d" /><figcaptionaria-hidden="true">05c133fba1b7e11497708c5df82366d</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;crc&quot;&gt;CRC&lt;/h1&gt;
&lt;figure&gt;
&lt;img
src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656975.jpg&quot;
alt=&quot;258a1611faa096e4f136</summary>
      
    
    
    
    
    <category term="计算机网络" scheme="https://cuiluyi.gitee.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>端到端 &amp;&amp; 点到点</title>
    <link href="https://cuiluyi.gitee.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%AB%AF%E5%88%B0%E7%AB%AF%20&amp;&amp;%20%E7%82%B9%E5%88%B0%E7%82%B9/"/>
    <id>https://cuiluyi.gitee.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%AB%AF%E5%88%B0%E7%AB%AF%20&amp;&amp;%20%E7%82%B9%E5%88%B0%E7%82%B9/</id>
    <published>2024-05-19T01:46:27.000Z</published>
    <updated>2024-05-19T09:28:18.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="端到端-点到点">端到端 &amp;&amp; 点到点</h1><ul><li>数据传输的可靠性是通过<strong>数据链路层和网络层的点对点</strong>和<strong>传输层的端对端</strong>保证的。</li><li>端到端与点到点是针对网络中传输的<strong>两端设备间的关系</strong>而言的。</li><li>在一个网络系统的不同分层中，可能用到端到端传输，也可能用到点到点传输。<ul><li>如Internet网，IP及以下各层采用点到点传输</li><li>4层以上采用端到端传输。</li></ul></li></ul><h2 id="端到端通信">端到端通信</h2><p>端到端通信是针对<strong><font color='red'>传输层</font></strong>来说的，它是一个网络连接，指的是在数据传输之前，在发送端与接收端之间（忽略中间有多少设备）为数据的传输建立一条链路，链路建立以后，发送端就可以发送数据，知道数据发送完毕，接收端确认接收成功。也就是说在数据传输之前，先为数据的传输开辟一条通道，然后在进行传输。从发送端发出数据到接收端接收完毕，结束。</p><h3 id="端到端优点">端到端优点：</h3><p>链路建立之后，发送端知道接收端一定能收到，而且经过中间交换设备时不需要进行存储转发，因此传输延迟小。</p><h3 id="端到端缺点">端到端缺点：</h3><ul><li><p>直到接收端收到数据为止，发送端的设备一直要参与传输。如果整个传输的延迟很长，那么对发送端的设备造成很大的浪费。</p></li><li><p>如果接收设备关机或故障，那么端到端传输不可能实现。</p></li></ul><h2 id="点到点通信">点到点通信</h2><p>点到点通信是针对数据链路层或网络层来说的，点对点是基于MAC地址和或者IP地址，是指一个设备发数据给与该这边直接连接的其他设备，这台设备又在合适的时候将数据传递给与它相连的下一个设备，通过一台一台直接相连的设备把数据传递到接收端。</p><h3 id="点到点优点">点到点优点：</h3><ul><li><p>发送端设备送出数据后，它的任务已经完成，不需要参与整个传输过程，这样不会浪费发送端设备的资源。</p></li><li><p>即使接收端设备关机或故障，点到点传输也可以采用存储转发技术进行缓冲。</p></li></ul><h3 id="点到点缺点">点到点缺点：</h3><ul><li>发送端发出数据后，不知道接收端能否收到或何时能收到数据。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;端到端-点到点&quot;&gt;端到端 &amp;amp;&amp;amp; 点到点&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;数据传输的可靠性是通过&lt;strong&gt;数据链路层和网络层的点对点&lt;/strong&gt;和&lt;strong&gt;传输层的端对端&lt;/strong&gt;保证的。&lt;/li&gt;
&lt;li&gt;端到端与点到点</summary>
      
    
    
    
    
    <category term="计算机网络" scheme="https://cuiluyi.gitee.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
