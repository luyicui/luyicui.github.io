<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>弘毅 の blog</title>
  
  
  <link href="https://luyicui.github.io/atom.xml" rel="self"/>
  
  <link href="https://luyicui.github.io/"/>
  <updated>2025-02-11T15:34:49.097Z</updated>
  <id>https://luyicui.github.io/</id>
  
  <author>
    <name>弘毅</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2025/02/11/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/unittest/introduction/"/>
    <id>https://luyicui.github.io/2025/02/11/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/unittest/introduction/</id>
    <published>2025-02-11T15:34:36.228Z</published>
    <updated>2025-02-11T15:34:49.097Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python-unit-testing">Python Unit Testing</h1><p>In this section, you’ll learn about unit testing in Python by usingthe unittest modules to make your code more robust.在本节中，你将学习如何在 Python 中使用 unittest模块进行单元测试，以使你的代码更健壮。</p><h2 id="what-youll-learn">What you’ll learn:</h2><ul><li>Write effective test cases using the <code>unittest</code>module</li><li>Run unit tests fast</li><li>Skip tests unconditionally and conditionally</li><li>Using test doubles including mocks, stubs, and fakes使用测试替身，包括模拟对象、存根和伪对象</li><li>Parameterize tests 参数化测试</li><li>Generate test coverage reports</li></ul><h2 id="section-1.-introduction-to-unit-testing-in-python">Section 1.Introduction to unit testing in Python</h2><p>This section introduces you to the unit testing and the unittestmodule. After completing this section, you’ll know how to define andexecute unit tests effectively.</p><ul><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-unittest/">Whatis unit testing</a> – introduce you to the unittest testing and how touse the unittest module to perform unit tests.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-test-fixtures/">Testfixtures</a> – learn how to use test fixtures including setUp() andtearDown() to carry out the steps before and after test methods.测试夹具 – 学习如何使用测试夹具，包括 setUp() 和tearDown()，来执行测试方法之前和之后的操作。</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-unittest-skip-test/">Skippingtests</a> – guide you on how to skip a test method or test class.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-run-unittest/">Runningunittest</a> – show you various commands to run unit tests.</li></ul><h2 id="section-2.-assert-methods">Section 2. assert methods</h2><p>This section covers the assert methods so that you know how to eachof them more effectively.</p><ul><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-unittest-assert/">assertmethods</a> – introduce to you a brief overview of the assert methods ofthe <code>TestCase</code> class.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-assertequal/">assertEqual()</a>– test if two values are equal.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-assertalmostequal/">assertAlmostEqual()</a>– test if two values are approximately equal.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-assertis/">assertIs()</a>– test if two objects are the same.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-assertisinstance/">assertIsInstance()</a>– test if an object is an instance of a class or a tuple ofclasses.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-assertisnone/">assertIsNone()</a>– test if an expression is None.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-asserttrue/">assertTrue()</a>– test if an expression is True.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-assertin/">assertIn()</a>– test if a member is in a container.</li></ul><h2 id="section-3.-test-doubles">Section 3. Test doubles</h2><p>This section introduces to you the test doubles to decouple thesystem under test code from the rest of the system so that code can betested in isolation.本节介绍测试替身，用于将被测系统代码与系统其余部分解耦，以便能够独立测试代码。</p><ul><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-unittest-mock/">Mock</a>– learn how to use the Mock class to mimic behaviors of another functionor class. 模拟 - 学习如何使用 Mock 类来模拟其他函数或类的行为。</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-patch/">patch()</a>– show you how to use the patch() to temporarily replace an object withanother object for testing.</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-stubs/">Stubs</a>– show you how to use the MagicMock class &amp; patch() to create stubs.存根——展示如何使用 MagicMock 类和 patch() 创建存根。</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-mock-requests/">Mockingrequests module</a> – learn how to mock the requests module to test anAPI call using the unittest module. 模拟 requests 模块 – 学习如何模拟requests 模块以使用 unittest 模块测试 API 调用。</li></ul><h2 id="section-4.-test-coverage-parameterized-tests">Section 4. Testcoverage &amp; Parameterized tests</h2><p>This section introduces you to the test coverage and how to defineparameterized tests using the <code>subTest()</code> context manager.本节介绍测试覆盖率以及如何使用 <code>subTest()</code>上下文管理器定义参数化测试。</p><ul><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-unittest-coverage/">Generatingtest coverage reports</a> – learn about test coverage and how togenerate the test coverage report using the coverage module.生成测试覆盖率报告——了解测试覆盖率以及如何使用 coverage模块生成测试覆盖率报告。</li><li><ahref="https://www.pythontutorial.net/python-unit-testing/python-unittest-subtest/">Definingparameterized tests using subTest()</a>– show you how to defineparameterized tests using the unittest’s subTest() context manager. 使用subTest()定义参数化测试——演示如何使用 unittest 的subTest()上下文管理器定义参数化测试。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;python-unit-testing&quot;&gt;Python Unit Testing&lt;/h1&gt;
&lt;p&gt;In this section, you’ll learn about unit testing in Python by using
the unittest </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2025/01/28/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/#!usrbinpython%E5%92%8C#%20--%20codingutf-8%20--/"/>
    <id>https://luyicui.github.io/2025/01/28/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/#!usrbinpython%E5%92%8C#%20--%20codingutf-8%20--/</id>
    <published>2025-01-27T16:20:08.908Z</published>
    <updated>2025-01-27T16:25:56.023Z</updated>
    
    <content type="html"><![CDATA[<h1id="usrbinpython和----codingutf-8---"><code>#!/usr/bin/python</code>和<code># -*- coding:utf-8 -*-</code></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]]</span><br></pre></td></tr></table></figure><h2 id="usrbinpython"><code>#!/usr/bin/python</code></h2><p>关于脚本第一行的 <code>#!/usr/bin/python</code>的解释，相信很多不熟悉 Linux系统的同学需要普及这个知识，脚本语言的第一行，只对 Linux/Unix用户适用，用来指定本脚本用什么解释器来执行。</p><p>有这句的，加上执行权限后，可以直接用 <code>./</code>执行，不然会出错，因为找不到 python 解释器。</p><ul><li><p><code>#!/usr/bin/python</code>是告诉操作系统执行这个脚本的时候，调用 /usr/bin 下的 python解释器，相当于写死了 python 路径。</p></li><li><p><code>#!/usr/bin/env python</code>这种用法是为了防止操作系统用户没有将 python 装在默认的 /usr/bin路径里。当系统看到这一行的时候，首先会到 env 环境设置里查找 python的安装路径，再调用对应路径下的解释器程序完成操作，可以增强代码的可移植性，推荐这种写法。</p></li></ul><p><strong>分成两种情况：</strong></p><p>（1）如果调用 python 脚本时，使用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python script.py </span><br></pre></td></tr></table></figure><p><code>#!/usr/bin/python</code> 被忽略，等同于注释</p><p>（2）如果调用python脚本时，使用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x script.py</span><br><span class="line">./script.py </span><br></pre></td></tr></table></figure><p><code>#!/usr/bin/python</code> 指定解释器的路径</p><p>PS：shell 脚本中在第一行也有类似的声明。</p><h2 id="codingutf-8---"><code># -*- coding:utf-8 -*-</code></h2><p><strong>含义</strong>：指定脚本的字符编码为 UTF-8。</p><p><strong>作用</strong>：</p><ul><li>确保脚本中的非 ASCII 字符（如中文）能够正确编码和解析。</li><li>通常用于处理包含多语言字符的脚本。</li></ul><p><strong>重要性</strong>：</p><ul><li><p><strong>在 Python 3 中，源文件默认使用 UTF-8编码，因此这行代码并不是必须的。</strong></p></li><li><p>在 Python 2 中，如果不指定编码且脚本中包含非 ASCII字符，会报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SyntaxError: Non-ASCII character &#x27;\xe4&#x27; in file script.py</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1
id=&quot;usrbinpython和----codingutf-8---&quot;&gt;&lt;code&gt;#!/usr/bin/python&lt;/code&gt;和&lt;code&gt;# -*- coding:utf-8 -*-&lt;/code&gt;&lt;/h1&gt;
&lt;figure class=&quot;highlight </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2025/01/27/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/%E3%80%90pytorch%E3%80%91/Base/torch.Tensor/data_ptr()/"/>
    <id>https://luyicui.github.io/2025/01/27/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/%E3%80%90pytorch%E3%80%91/Base/torch.Tensor/data_ptr()/</id>
    <published>2025-01-27T08:06:53.154Z</published>
    <updated>2025-01-27T08:08:12.363Z</updated>
    
    <content type="html"><![CDATA[<h1 id="data_ptr">data_ptr()</h1><p>Tensor.data_ptr() → <ahref="https://docs.python.org/3/library/functions.html#int">int</a></p><p>Returns the address of the first element of <code>self</code>tensor.</p><p>返回 <code>self</code> 张量的第一个元素的地址。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个张量</span></span><br><span class="line">tensor = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取张量第一个元素的内存地址</span></span><br><span class="line">address = tensor.data_ptr()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印张量和地址</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Memory address of the first element:&quot;</span>, address)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(qwen-agent) D:\Document\GitHub\temp&gt;python -u &quot;d:\Document\GitHub\temp\demo1.py&quot;</span><br><span class="line">Tensor: tensor([1., 2., 3.])</span><br><span class="line">Memory address of the first element: 4425208828160</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;data_ptr&quot;&gt;data_ptr()&lt;/h1&gt;
&lt;p&gt;Tensor.data_ptr() → &lt;a
href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Re</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【二叉树】</title>
    <link href="https://luyicui.github.io/2024/08/23/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/C++/%E7%AE%97%E6%B3%95/%E3%80%90%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91/"/>
    <id>https://luyicui.github.io/2024/08/23/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/C++/%E7%AE%97%E6%B3%95/%E3%80%90%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91/</id>
    <published>2024-08-22T17:02:27.000Z</published>
    <updated>2024-11-13T14:13:34.984Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二叉树">二叉树</h1><h2 id="存储">存储</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r;<span class="comment">// l[i] 和 r[i] 分别存储节点 i 的左、右孩子编号</span></span><br></pre></td></tr></table></figure><blockquote><p>不定义为 <code>int l[N], r[N]</code> 的原因是：二叉树的结点个数最大为N，但是结点权值可以大于N，此时就会导致段错误，而定义成哈希表就避免了很多麻烦</p></blockquote><h2 id="非递归遍历">非递归遍历</h2><h3 id="结论">结论</h3><p>【<strong>结论</strong>】用 <strong>栈</strong> 模拟实现<strong>中序遍历</strong>，<font color='red'> <strong>Push</strong></font> 操作的数据过程是 <font color='blue'> <strong>先序</strong></font> 遍历，<font color='red'> <strong>Pop</strong> </font>操作的数据过程是 <font color='blue'> <strong>中序</strong> </font>遍历</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408232259283.png"alt="树的遍历.png" /><figcaption aria-hidden="true">树的遍历.png</figcaption></figure><p>​ 如图所示，⊗ 是先序遍历，☆ 是中序遍历，△ 是后序遍历。我们发现：树的<strong>前序、中序、后序</strong> 实际上都是将整棵树以<strong>上图所示的路线</strong> 跑了 <spanclass="math inline">\(1\)</span> 遍，每个结点都碰到了 <spanclass="math inline">\(3\)</span> 次，三者唯一不同之处在于<strong>访问节点的时机不同</strong></p><ul><li><strong>先序</strong> 遍历在第 <spanclass="math inline">\(1\)</span> 次碰到结点时访问</li><li><strong>中序</strong> 遍历在第 <spanclass="math inline">\(2\)</span> 次碰到结点时访问</li><li><strong>后序</strong> 遍历在第 <spanclass="math inline">\(3\)</span> 次碰到结点时访问</li></ul><h3 id="例题">例题</h3><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8792912/">AcWing1576. 再次树遍历 - AcWing</a></li></ul><span id="more"></span><h2 id="层序遍历">层序遍历</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> q[N];</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">bfs</span><span class="params">(<span class="type">int</span> root)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> hh = <span class="number">0</span>, tt = <span class="number">-1</span>;</span><br><span class="line">    q[++ tt] = root;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(hh &lt;= tt)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">auto</span> t = q[hh ++];</span><br><span class="line">        <span class="keyword">if</span>(l.count(t))<span class="comment">// 存在左孩子</span></span><br><span class="line">            q[++ tt] = l[t];</span><br><span class="line">        <span class="keyword">if</span>(r.count(t))<span class="comment">// 存在右孩子</span></span><br><span class="line">            q[++ tt] = r[t];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bfs(root);</span><br><span class="line"><span class="comment">// 输出层序序列</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, q[i]);</span><br></pre></td></tr></table></figure><h2 id="后序中序建树">后序中序建树</h2><h3 id="模板">模板</h3><ul><li><strong>时间复杂度</strong>：<spanclass="math inline">\(O(n)\)</span></li></ul><blockquote><p>注意：前提是二叉树中<strong>节点编号或权值互不相同</strong>，我们才能用<strong>哈希表</strong>记录中序序列各节点对应的下标，从而将时间复杂度优化为 <spanclass="math inline">\(O(n)\)</span>。如果<strong>二叉树节点编号或权值可能重复</strong>，则只能遍历搜索位置，此时时间复杂度为<span class="math inline">\(O(n^2)\)</span></p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> in[N], post[N];<span class="comment">// n 个节点的中序序列、后序序列</span></span><br><span class="line"><span class="type">int</span> pre[N], cnt;<span class="comment">// 建图的同时记录 n 个节点的前序序列</span></span><br><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r, pos;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">build</span><span class="params">(<span class="type">int</span> il, <span class="type">int</span> ir, <span class="type">int</span> pl, <span class="type">int</span> pr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> root = post[pr];</span><br><span class="line">    <span class="type">int</span> k = pos[root];<span class="comment">// 优化时间复杂度 O(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 记录前序序列 */</span></span><br><span class="line">    <span class="comment">// pre[cnt ++] = root;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(il &lt; k)</span><br><span class="line">        l[root] = build(il, k - <span class="number">1</span>, pl, pl + (k - <span class="number">1</span> - il));</span><br><span class="line">    <span class="keyword">if</span>(k &lt; ir)</span><br><span class="line">        r[root] = build(k + <span class="number">1</span>, ir, pl + (k - <span class="number">1</span> - il) + <span class="number">1</span>, pr - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;post[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;in[i]);</span><br><span class="line">        pos[in[i]] = i;        <span class="comment">// 哈希表记录每个数在中序遍历的下标</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// root 为二叉树的根节点</span></span><br><span class="line">    <span class="type">int</span> root = build(<span class="number">0</span>, n - <span class="number">1</span>, <span class="number">0</span>, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题-1">例题</h3><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8790598/">AcWing1497. 树的遍历 - AcWing</a></li><li><ahref="https://www.acwing.com/activity/content/code/content/8790623/">AcWing1620. Z 字形遍历二叉树 - AcWing</a></li></ul><h2 id="中序建树">中序建树</h2><h3 id="模板-1">模板</h3><ul><li><strong>时间复杂度</strong>：<spanclass="math inline">\(O(n)\)</span></li></ul><blockquote><p>注意：前提是二叉树中每个节点的权值互不相同，我们才能用哈希表记录中序序列各节点对应的下标</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> in[N], pre[N];<span class="comment">// n 个节点的中序序列、前序序列</span></span><br><span class="line"><span class="type">int</span> post[N], cnt;<span class="comment">// 建图的同时记录 n 个节点的后序序列</span></span><br><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r, pos;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">build</span><span class="params">(<span class="type">int</span> il, <span class="type">int</span> ir, <span class="type">int</span> pl, <span class="type">int</span> pr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> root = pre[pl];</span><br><span class="line">    <span class="type">int</span> k = pos[root];<span class="comment">// 优化时间复杂度 O(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(il &lt; k)</span><br><span class="line">        l[root] = build(il, k - <span class="number">1</span>, pl + <span class="number">1</span>, pl + <span class="number">1</span> + k - <span class="number">1</span> - il);</span><br><span class="line">    <span class="keyword">if</span>(k &lt; ir)</span><br><span class="line">        r[root] = build(k + <span class="number">1</span>, ir, pl + <span class="number">1</span> + k - <span class="number">1</span> - il + <span class="number">1</span>, pr);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 记录后序序列 */</span></span><br><span class="line">    <span class="comment">// post[cnt ++] = root;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;pre[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;in[i]);</span><br><span class="line">        pos[in[i]] = i;        <span class="comment">// 哈希表记录每个数在中序遍历的下标</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// root 为二叉树的根节点</span></span><br><span class="line">    <span class="type">int</span> root = build(<span class="number">0</span>, n - <span class="number">1</span>, <span class="number">0</span>, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题-2">例题</h3><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8790607/">AcWing1631. 后序遍历 - AcWing</a></li><li><a href="https://www.acwing.com/solution/content/251933/">AcWing2019 清华软院 T2. 二叉树算权 - AcWing</a></li></ul><h2 id="前序和后序">前序和后序</h2><p>已知二叉树的前序序列和后序序列，<strong>无法唯一确定</strong>这个二叉树，但是我们可以确定每个子树的形状和个数，仅仅是子树的位置不能确定</p><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8793054/">AcWing1609. 前序和后序遍历 - AcWing</a></li><li><a href="https://www.acwing.com/solution/content/251849/">AcWing3486. 前序和后序 - AcWing</a></li></ul><h1 id="完全二叉树">完全二叉树</h1><h2 id="存储-1">存储</h2><p>完全二叉树采用 <strong>数组</strong> 存储</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br></pre></td></tr></table></figure><h2 id="性质">性质</h2><p>完全二叉树的性质如下：</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408230026116.jpeg"alt="7476a7cbe9d1fbd33df03cf13ab37b4" /><figcaptionaria-hidden="true">7476a7cbe9d1fbd33df03cf13ab37b4</figcaption></figure><ol type="1"><li><strong><font color='red'> 从 1 号单元开始存储树节点</font></strong>（0 号单元存节点的个数）</li><li>节点 <span class="math inline">\(i\)</span> 左子树是 <spanclass="math inline">\(2i\)</span>，右子树是 <spanclass="math inline">\(2i+1\)</span>，根节点是 <spanclass="math inline">\(\lfloor i/2 \rfloor\)</span>，左兄弟是 <spanclass="math inline">\(i-1\)</span>，右兄弟 <spanclass="math inline">\(i+1\)</span></li><li>第 <span class="math inline">\(d\)</span> 层最多有 <spanclass="math inline">\(2^{d-1}\)</span>，<strong>起始节点</strong> 编号为<span class="math inline">\(2^{d-1}\)</span>（<spanclass="math inline">\(d\)</span> 从 <spanclass="math inline">\(1\)</span> 开始）</li><li>如果完全二叉树一共有 <span class="math inline">\(n\)</span>个节点，则 <strong>非</strong> 叶子节点为 <spanclass="math inline">\(T[0…n/2]\)</span>，叶节点为 <spanclass="math inline">\(T[n/2+1,…,n]\)</span></li></ol><h2 id="例题-3">例题</h2><ul><li><a href="https://www.acwing.com/solution/content/251776/">AcWing1240. 完全二叉树的权值 - AcWing</a></li></ul><h1 id="二叉搜索树bst">二叉搜索树(BST)</h1><p>==<strong><font color='blue'> 二叉搜索树 </font></strong> ==<strong><font color='blue'> 二叉查找树 </font></strong> （Binary SearchTree） == <strong><font color='blue'> 二叉排序树 </font></strong>（Binary Sort Tree）==</p><ul><li>若它的左子树不空，则 <strong>左子树</strong> 上<strong>所有</strong> 结点的值均 <font color='red'><strong>小于</strong> </font> 根结点的值;</li><li>若它的右子树不空，则 <strong>右子树</strong> 上<strong>所有</strong> 结点的值均 <font color='red'><strong>大于等于</strong> </font> 根结点的值;</li><li>它的左、右子树也都分别是 <strong>二又搜索树</strong></li></ul><blockquote><p>注意：上述定义在不同题目中，等号的位置可能不一样（即也有可能左子树均小于等于根节点，右子树均大于根节点）</p></blockquote><h2 id="性质-1">性质</h2><ul><li><strong>二叉排序树</strong> 的 <strong><font color='red'> 中序遍历</font></strong> 是 <strong><font color='gree'> 递增 </font></strong>序列</li></ul><blockquote><p>在构造二叉排序树时，若关键字序列有序，则二叉排序树的高度最大</p></blockquote><h2 id="例题-4">例题</h2><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8790764/">AcWing1527. 判断二叉搜索树 - AcWing</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;二叉树&quot;&gt;二叉树&lt;/h1&gt;
&lt;h2 id=&quot;存储&quot;&gt;存储&lt;/h2&gt;
&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;unordered_map&lt;/span&gt;&amp;lt;&lt;span class=&quot;type&quot;&gt;int&lt;/span&gt;, &lt;span class=&quot;type&quot;&gt;int&lt;/span&gt;&amp;gt; l, r;	&lt;span class=&quot;comment&quot;&gt;// l[i] 和 r[i] 分别存储节点 i 的左、右孩子编号&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;不定义为 &lt;code&gt;int l[N], r[N]&lt;/code&gt; 的原因是：二叉树的结点个数最大为
N，但是结点权值可以大于
N，此时就会导致段错误，而定义成哈希表就避免了很多麻烦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;非递归遍历&quot;&gt;非递归遍历&lt;/h2&gt;
&lt;h3 id=&quot;结论&quot;&gt;结论&lt;/h3&gt;
&lt;p&gt;【&lt;strong&gt;结论&lt;/strong&gt;】用 &lt;strong&gt;栈&lt;/strong&gt; 模拟实现
&lt;strong&gt;中序遍历&lt;/strong&gt;，&lt;font color=&#39;red&#39;&gt; &lt;strong&gt;Push&lt;/strong&gt;
&lt;/font&gt; 操作的数据过程是 &lt;font color=&#39;blue&#39;&gt; &lt;strong&gt;先序&lt;/strong&gt;
&lt;/font&gt; 遍历，&lt;font color=&#39;red&#39;&gt; &lt;strong&gt;Pop&lt;/strong&gt; &lt;/font&gt;
操作的数据过程是 &lt;font color=&#39;blue&#39;&gt; &lt;strong&gt;中序&lt;/strong&gt; &lt;/font&gt;
遍历&lt;/p&gt;
&lt;figure&gt;
&lt;img
src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408232259283.png&quot;
alt=&quot;树的遍历.png&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;树的遍历.png&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;​ 如图所示，⊗ 是先序遍历，☆ 是中序遍历，△ 是后序遍历。我们发现：树的
&lt;strong&gt;前序、中序、后序&lt;/strong&gt; 实际上都是将整棵树以
&lt;strong&gt;上图所示的路线&lt;/strong&gt; 跑了 &lt;span
class=&quot;math inline&quot;&gt;&#92;(1&#92;)&lt;/span&gt; 遍，每个结点都碰到了 &lt;span
class=&quot;math inline&quot;&gt;&#92;(3&#92;)&lt;/span&gt; 次，三者唯一不同之处在于
&lt;strong&gt;访问节点的时机不同&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;先序&lt;/strong&gt; 遍历在第 &lt;span
class=&quot;math inline&quot;&gt;&#92;(1&#92;)&lt;/span&gt; 次碰到结点时访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中序&lt;/strong&gt; 遍历在第 &lt;span
class=&quot;math inline&quot;&gt;&#92;(2&#92;)&lt;/span&gt; 次碰到结点时访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后序&lt;/strong&gt; 遍历在第 &lt;span
class=&quot;math inline&quot;&gt;&#92;(3&#92;)&lt;/span&gt; 次碰到结点时访问&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;例题&quot;&gt;例题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a
href=&quot;https://www.acwing.com/activity/content/code/content/8792912/&quot;&gt;AcWing
1576. 再次树遍历 - AcWing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="算法" scheme="https://luyicui.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam详解</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</id>
    <published>2024-08-13T02:02:27.000Z</published>
    <updated>2025-01-18T16:14:57.307Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adam-算法详解">Adam 算法详解</h1><p>Adam 算法在 RMSProp 算法基础上对 <ahref="https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;spm=1001.2101.3001.7020">小批量</a>随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。</p><blockquote><p>所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。adam算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶moment 和二阶moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了 Adagrad算法的处理稀疏型数据，又结合了 RMSProp 算法的可以处理非稳态的数据。</p></blockquote><p>小tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个<ahref="https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;vd_source=12c80a98ec9426002a2f54318421082c">优化算法系列合集</a>，个人觉得比较容易听懂</p><h2 id="算法">算法</h2><p>Adam 算法使用了动量变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> ​和 RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量 <spanclass="math inline">\(\boldsymbol{s}_t\)</span> ​，并在时间步 <spanclass="math inline">\(0\)</span> 将它们中每个元素初始化为 <spanclass="math inline">\(0\)</span>。给定超参数 <spanclass="math inline">\(0 \leq \beta_1 &lt; 1\)</span> （算法作者建议设为<span class="math inline">\(0.9\)</span>），时间步 <spanclass="math inline">\(t\)</span> 的动量变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> ​即小批量随机梯度 <spanclass="math inline">\(\boldsymbol{g}_t\)</span> ​的指数加权移动平均：</p><p><span class="math display">\[\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1)\boldsymbol{g}_t\]</span> 和 RMSProp 算法中一样，给定超参数 <spanclass="math inline">\(0 \leq \beta_2 &lt; 1\)</span> （算法作者建议设为0.999）</p><p>将小批量随机梯度按元素平方后的项 <spanclass="math inline">\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)</span>​做指数加权移动平均得到 <spanclass="math inline">\(\boldsymbol{s}_t\)</span>​： <spanclass="math display">\[\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2)\boldsymbol{g}_t \odot \boldsymbol{g}_t\]</span> 由于我们将 <spanclass="math inline">\(\boldsymbol{v}_0\)</span> 和 <spanclass="math inline">\(\boldsymbol{s}_0\)</span> 中的元素都初始化为 <spanclass="math inline">\(0\)</span></p><p>在时间步 <span class="math inline">\(t\)</span> 我们得到 <spanclass="math inline">\(\boldsymbol{v}_t = (1-\beta_1) \sum_{i=1}^t\beta_1^{t-i}\boldsymbol{g}_i\)</span>。将过去各时间步小批量随机梯度的权值相加，得到<span class="math inline">\((1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 -\beta_1^t\)</span>。需要注意的是，当 <spanclass="math inline">\(t\)</span>较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 <spanclass="math inline">\(\beta_1 = 0.9\)</span> 时，<spanclass="math inline">\(\boldsymbol{v}_1 =0.1\boldsymbol{g}_1\)</span>。为了消除这样的影响，对于任意时间步 <spanclass="math inline">\(t\)</span> ，我们可以将 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> 再除以 <spanclass="math inline">\(1 -\beta_1^t\)</span>，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作 <strong>偏差修正</strong>。在 Adam 算法中，我们对变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> 和 <spanclass="math inline">\(\boldsymbol{s}_t\)</span> 均作偏差修正： <spanclass="math display">\[\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}\]</span></p><p><span class="math display">\[\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}\]</span></p><p>接下来，Adam 算法使用以上偏差修正后的变量 $_t $ 和 <spanclass="math inline">\(\hat{\boldsymbol{s}}_t\)</span>，将模型参数中每个元素的学习率通过按元素运算重新调整：</p><p><span class="math display">\[\boldsymbol{g}_t&#39; \leftarrow \frac{\eta\hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}\]</span> 其中 <span class="math inline">\(\eta\)</span> 是学习率，<spanclass="math inline">\(\epsilon\)</span>是为了维持数值稳定性而添加的常数，如 <spanclass="math inline">\(10^{-8}\)</span> 。和 AdaGrad 算法、RMSProp算法以及 AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用<span class="math inline">\(\boldsymbol{g}_t&#39;\)</span>​迭代自变量：</p><p><span class="math display">\[\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t&#39;\]</span></p><span id="more"></span><h2 id="从零开始实现">从零开始实现</h2><p>我们按照 Adam 算法中的公式实现该算法。其中时间步 t t t 通过<code>hyperparams</code> 参数传入 <code>adam</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>():</span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>使用学习率为 0.01 的 Adam 算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.245370, 0.065155 sec per epoch</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202411132136580.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="简洁实现">简洁实现</h2><p>通过名称为“Adam”的优化器实例，我们便可使用 PyTorch 提供的 Adam算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.242066, 0.056867 sec per epoch</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202411132136204.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="小结">小结</h2><ul><li>Adam 算法在 RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。</li><li>Adam 算法使用了偏差修正。</li></ul><h2 id="参考文献">参考文献</h2><p>[1] Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochasticoptimization. arXiv preprint arXiv: 1412.6980.</p><hr /><blockquote><p>注：除代码外本节与原书此节基本相同，<ahref="https://zh.d2l.ai/chapter_optimization/adam.html">原书传送门</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;adam-算法详解&quot;&gt;Adam 算法详解&lt;/h1&gt;
&lt;p&gt;Adam 算法在 RMSProp 算法基础上对 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;amp;spm=1001.2101.3001.7020&quot;&gt;小批量&lt;/a&gt;
随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。adam
算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶
moment 和二阶
moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了 Adagrad
算法的处理稀疏型数据，又结合了 RMSProp 算法的可以处理非稳态的数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;小
tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个
&lt;a
href=&quot;https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;amp;vd_source=12c80a98ec9426002a2f54318421082c&quot;&gt;优化算法系列合集&lt;/a&gt;，个人觉得比较容易听懂&lt;/p&gt;
&lt;h2 id=&quot;算法&quot;&gt;算法&lt;/h2&gt;
&lt;p&gt;Adam 算法使用了动量变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; ​和 RMSProp
算法中小批量随机梯度按元素平方的指数加权移动平均变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_t&#92;)&lt;/span&gt; ​，并在时间步 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0&#92;)&lt;/span&gt; 将它们中每个元素初始化为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0&#92;)&lt;/span&gt;。给定超参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0 &#92;leq &#92;beta_1 &amp;lt; 1&#92;)&lt;/span&gt; （算法作者建议设为
&lt;span class=&quot;math inline&quot;&gt;&#92;(0.9&#92;)&lt;/span&gt;），时间步 &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; 的动量变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; ​即小批量随机梯度 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{g}_t&#92;)&lt;/span&gt; ​的指数加权移动平均：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{v}_t &#92;leftarrow &#92;beta_1 &#92;boldsymbol{v}_{t-1} + (1 - &#92;beta_1)
&#92;boldsymbol{g}_t
&#92;]&lt;/span&gt; 和 RMSProp 算法中一样，给定超参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0 &#92;leq &#92;beta_2 &amp;lt; 1&#92;)&lt;/span&gt; （算法作者建议设为
0.999）&lt;/p&gt;
&lt;p&gt;将小批量随机梯度按元素平方后的项 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{g}_t &#92;odot &#92;boldsymbol{g}_t&#92;)&lt;/span&gt;
​做指数加权移动平均得到 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_t&#92;)&lt;/span&gt;​： &lt;span
class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{s}_t &#92;leftarrow &#92;beta_2 &#92;boldsymbol{s}_{t-1} + (1 - &#92;beta_2)
&#92;boldsymbol{g}_t &#92;odot &#92;boldsymbol{g}_t
&#92;]&lt;/span&gt; 由于我们将 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_0&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_0&#92;)&lt;/span&gt; 中的元素都初始化为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0&#92;)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在时间步 &lt;span class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; 我们得到 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t = (1-&#92;beta_1) &#92;sum_{i=1}^t
&#92;beta_1^{t-i}
&#92;boldsymbol{g}_i&#92;)&lt;/span&gt;。将过去各时间步小批量随机梯度的权值相加，得到
&lt;span class=&quot;math inline&quot;&gt;&#92;((1-&#92;beta_1) &#92;sum_{i=1}^t &#92;beta_1^{t-i} = 1 -
&#92;beta_1^t&#92;)&lt;/span&gt;。需要注意的是，当 &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt;
较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_1 = 0.9&#92;)&lt;/span&gt; 时，&lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_1 =
0.1&#92;boldsymbol{g}_1&#92;)&lt;/span&gt;。为了消除这样的影响，对于任意时间步 &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; ，我们可以将 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; 再除以 &lt;span
class=&quot;math inline&quot;&gt;&#92;(1 -
&#92;beta_1^t&#92;)&lt;/span&gt;，从而使过去各时间步小批量随机梯度权值之和为
1。这也叫作 &lt;strong&gt;偏差修正&lt;/strong&gt;。在 Adam 算法中，我们对变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_t&#92;)&lt;/span&gt; 均作偏差修正： &lt;span
class=&quot;math display&quot;&gt;&#92;[
&#92;hat{&#92;boldsymbol{v}}_t &#92;leftarrow &#92;frac{&#92;boldsymbol{v}_t}{1 - &#92;beta_1^t}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;hat{&#92;boldsymbol{s}}_t &#92;leftarrow &#92;frac{&#92;boldsymbol{s}_t}{1 - &#92;beta_2^t}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;接下来，Adam 算法使用以上偏差修正后的变量 $_t $ 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;hat{&#92;boldsymbol{s}}_t&#92;)&lt;/span&gt;
，将模型参数中每个元素的学习率通过按元素运算重新调整：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{g}_t&amp;#39; &#92;leftarrow &#92;frac{&#92;eta
&#92;hat{&#92;boldsymbol{v}}_t}{&#92;sqrt{&#92;hat{&#92;boldsymbol{s}}_t} + &#92;epsilon}
&#92;]&lt;/span&gt; 其中 &lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;eta&#92;)&lt;/span&gt; 是学习率，&lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;epsilon&#92;)&lt;/span&gt;
是为了维持数值稳定性而添加的常数，如 &lt;span
class=&quot;math inline&quot;&gt;&#92;(10^{-8}&#92;)&lt;/span&gt; 。和 AdaGrad 算法、RMSProp
算法以及 AdaDelta
算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用
&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{g}_t&amp;#39;&#92;)&lt;/span&gt;
​迭代自变量：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{x}_t &#92;leftarrow &#92;boldsymbol{x}_{t-1} - &#92;boldsymbol{g}_t&amp;#39;
&#92;]&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/</id>
    <published>2024-08-13T01:02:27.000Z</published>
    <updated>2025-01-18T16:14:53.991Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adam-算法">Adam 算法</h1><p>​ 接下来，我们将介绍目前常用的梯度下降法中的王者——Adam算法。Adam（Adaptive MomentEstimation）是目前深度学习中最常用的优化算法之一。Adam 算法的核心思想是<strong>利用梯度一阶动量和二阶动量来动态自适应调整学习率</strong>，既保持了<strong>Momentum 收敛速度快</strong> 的优点，又结合了 <strong>RMSProp自适应学习率</strong> 的优点</p><h2 id="基本思想">基本思想</h2><p>Adam 算法通过计算梯度的 <strong>一阶动量</strong>（即<strong>梯度的指数加权移动平均）</strong> 和梯度的<strong>二阶动量</strong>（即<strong>梯度平方的指数加权移动平均</strong>）来<strong>动态调整</strong> 每个参数的<strong>学习率</strong>。具体公式如下：</p><ol type="1"><li>梯度的一阶动量：</li></ol><p><span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\]</span></p><ol start="2" type="1"><li>梯度的二阶动量：</li></ol><p><span class="math display">\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p><ol start="3" type="1"><li>偏差修正：</li></ol><p><span class="math display">\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}  \]</span></p><p><span class="math display">\[\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\]</span></p><ol start="4" type="1"><li>更新参数：</li></ol><p><span class="math display">\[\theta_{t+1} = \theta_t - \frac{\alpha \hat{m}_t}{\sqrt{\hat{v}_t} +\epsilon}\]</span></p><p>其中：<br />- <span class="math inline">\(\beta_1\)</span> 和 <spanclass="math inline">\(\beta_2\)</span> 分别是 <strong>动量</strong> 和<strong>均方根动量</strong> 的衰减率，常用值为 <spanclass="math inline">\(\beta_1 = 0.9\)</span> 和 <spanclass="math inline">\(\beta_2 = 0.999\)</span> - <spanclass="math inline">\(\epsilon\)</span>是一个很小的常数，用于防止分母为零，常用值为 <spanclass="math inline">\(10^{-8}\)</span></p><span id="more"></span><h2 id="优缺点">优缺点</h2><p><strong>优点</strong>：</p><ul><li><strong>自适应调整学习率</strong>：根据一阶动量和二阶动量动态调整每个参数的学习率，使得训练过程更加稳定。</li><li><strong>收敛速度快</strong>：结合动量法的 <strong>加速特性</strong>和 RMSProp 的 <strong>平稳特性</strong>，能够快速收敛到最优解。</li><li>能处理 <strong>稀疏梯度</strong>，适用于大规模数据和参数。</li></ul><p><strong>缺点</strong>：</p><ul><li>对于某些特定问题，Adam 可能会出现不稳定的收敛行为。<br /></li><li>参数较多：Adam 算法需要调整的 <strong>超参数较多</strong>（例如<span class="math inline">\(\beta_1\)</span> , <spanclass="math inline">\(\beta_2\)</span> , <spanclass="math inline">\(\epsilon\)</span>），调参复杂度高。</li></ul><h2 id="代码实现">代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义Adam优化器</span></span><br><span class="line">optimizer = torch.optim.Adam([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adam&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="adam-与其他算法的比较">Adam 与其他算法的比较</h2><p>Adam 算法集成了 SGD、动量法、Adagrad、Adadelta等多种优化算法的优点，具有快速收敛和稳定的特点。以下是它与其他算法的对比：</p><ol type="1"><li>SGD：基本的随机梯度下降法，收敛速度较慢，易陷入局部最优。</li><li>动量法：在 SGD基础上加入一阶动量，加速收敛，但仍然可能陷入局部最优。</li><li>Adagrad：自适应学习率，但对历史梯度的累积会导致学习率不断减小，后期训练缓慢。</li><li>RMSProp：改进了Adagrad，通过引入衰减系数解决学习率不断减小的问题。</li><li>Adam：结合动量法和 RMSProp的优点，具有快速收敛和稳定的特点，是目前最常用的优化算法。</li></ol><h2 id="小结">小结</h2><p>Adam 算法作为一种自适应的梯度下降优化算法，结合了动量法和 RMSProp的优点，能够有效地加速模型的收敛，同时保持稳定性。它通过计算一阶和二阶动量来动态调整学习率，使得模型在训练过程中能够快速收敛，并适应不同的优化问题。尽管Adam需要调整的超参数较多，但其优越的性能使得它成为深度学习中最广泛使用的优化算法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;adam-算法&quot;&gt;Adam 算法&lt;/h1&gt;
&lt;p&gt;​ 接下来，我们将介绍目前常用的梯度下降法中的王者——Adam
算法。Adam（Adaptive Moment
Estimation）是目前深度学习中最常用的优化算法之一。Adam 算法的核心思想是
&lt;strong&gt;利用梯度一阶动量和二阶动量来动态自适应调整学习率&lt;/strong&gt;，既保持了
&lt;strong&gt;Momentum 收敛速度快&lt;/strong&gt; 的优点，又结合了 &lt;strong&gt;RMSProp
自适应学习率&lt;/strong&gt; 的优点&lt;/p&gt;
&lt;h2 id=&quot;基本思想&quot;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;Adam 算法通过计算梯度的 &lt;strong&gt;一阶动量&lt;/strong&gt;（即
&lt;strong&gt;梯度的指数加权移动平均）&lt;/strong&gt; 和梯度的
&lt;strong&gt;二阶动量&lt;/strong&gt;（即
&lt;strong&gt;梯度平方的指数加权移动平均&lt;/strong&gt;）来
&lt;strong&gt;动态调整&lt;/strong&gt; 每个参数的
&lt;strong&gt;学习率&lt;/strong&gt;。具体公式如下：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;梯度的一阶动量：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
m_t = &#92;beta_1 m_{t-1} + (1 - &#92;beta_1) g_t
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;梯度的二阶动量：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = &#92;beta_2 v_{t-1} + (1 - &#92;beta_2) g_t^2
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;偏差修正：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;hat{m}_t = &#92;frac{m_t}{1 - &#92;beta_1^t}  
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;hat{v}_t = &#92;frac{v_t}{1 - &#92;beta_2^t}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;更新参数：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;theta_{t+1} = &#92;theta_t - &#92;frac{&#92;alpha &#92;hat{m}_t}{&#92;sqrt{&#92;hat{v}_t} +
&#92;epsilon}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中：&lt;br /&gt;
- &lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;beta_1&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_2&#92;)&lt;/span&gt; 分别是 &lt;strong&gt;动量&lt;/strong&gt; 和
&lt;strong&gt;均方根动量&lt;/strong&gt; 的衰减率，常用值为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_1 = 0.9&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_2 = 0.999&#92;)&lt;/span&gt; - &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;epsilon&#92;)&lt;/span&gt;
是一个很小的常数，用于防止分母为零，常用值为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(10^{-8}&#92;)&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>AdaGrad</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.AdaGrad/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.AdaGrad/</id>
    <published>2024-08-13T00:02:27.000Z</published>
    <updated>2025-01-18T16:34:08.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adagrad-算法">AdaGrad 算法</h1><p>在前面我们讲解了动量法（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的AdaGrad 算法。二阶动量的出现，才意味着真正的<strong>自适应学习率</strong> 优化算法时代的到来。</p><h2 id="adagrad-算法的基本思想">AdaGrad 算法的基本思想</h2><p>我们先回顾一下传统的 <ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>（SGD）及其各种变种。它们都是以<strong>同样的学习率</strong> 来更新 <strong>每一个参数</strong>的。但深度神经网络往往包含大量参数，这些参数并不总是<strong>均匀更新</strong> 的。有些参数更新得频繁，有些则很少更新。</p><ul><li>对于 <strong>经常更新</strong>的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的<strong>学习率小一些</strong></li><li>对于 <strong>偶尔更新</strong>的参数，我们了解的信息较少，希望从每一个样本中多学一些，即<strong>学习率大一些</strong></li></ul><p>要动态度量历史更新的频率，我们引入<strong>二阶动量</strong>。二阶动量通过将每一位各自的历史梯度的<strong>平方</strong> 叠加起来来计算。具体公式如下：</p><p><span class="math display">\[v_t = v_{t-1} + g_t^2\]</span></p><p>其中，<span class="math inline">\(g_t\)</span> 是当前的梯度。</p><span id="more"></span><h2 id="算法流程">算法流程</h2><ol type="1"><li><strong>计算当前梯度 <span class="math inline">\(g_t\)</span></strong>：</li></ol><p><span class="math display">\[g_t = \nabla f(w_t)\]</span></p><ol start="2" type="1"><li><strong>更新二阶动量 <span class="math inline">\(v_t\)</span></strong>：</li></ol><p><span class="math display">\[v_t =  v_{t-1} + g_t^2\]</span></p><ol start="3" type="1"><li><strong>计算当前时刻的下降梯度</strong>：</li></ol><p><span class="math display">\[w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t} + \epsilon} g_t\]</span></p><p>其中，<span class="math inline">\(\alpha\)</span> 是学习率，<spanclass="math inline">\(\epsilon\)</span> 是一个小的平滑项，防止分母为0。</p><h2 id="稀疏特征处理">稀疏特征处理</h2><p>AdaGrad 算法主要针对 <strong>稀疏特征</strong>进行了优化。<strong>稀疏特征</strong>在很多样本中只出现少数几次，在训练模型时，这些稀疏特征的更新很少，但每次更新可能带来较大影响。AdaGrad通过调整每个特征的学习率，针对这种情况进行了优化。</p><h3 id="优缺点">优缺点</h3><p><strong>优点</strong>：</p><ol type="1"><li><strong>有效处理稀疏特征</strong>：自动调整每个参数的学习率，使得稀疏特征的更新更少。<br /></li><li><strong>加速收敛</strong>：在自动调整学习率的同时，使得模型在训练过程中更快收敛。</li></ol><p><strong>缺点</strong>：<br />1.<strong>学习率逐渐减小</strong>：每次迭代中学习率都会减小，导致训练后期学习率变得非常小，从而使收敛速度变慢。<br />2.<strong>固定调整方式</strong>：对于不同参数，学习率调整方式是固定的，无法根据不同任务自动调整。</p><h2 id="代码实现">代码实现</h2><p>下面是一个简单的 PyTorch 实现 AdaGrad 算法的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成一些数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 AdaGrad 优化器</span></span><br><span class="line">optimizer = torch.optim.Adagrad([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112049596.png"alt="image-20240811204910528" /><figcaption aria-hidden="true">image-20240811204910528</figcaption></figure><h2 id="总结">总结</h2><p>本节我们介绍了一种新的梯度下降算法变体——AdaGrad。与动量法相比，它最大的改进在于<strong>使用二阶动量来动态调整学习率</strong>，能够记住历史上的梯度信息，以动态调整学习率。其主要优点是能够处理稀疏特征问题，但也有学习率逐渐减小和调整方式固定的缺点。</p><p>到目前为止，我们一共讲了五种梯度下降算法。AdaGrad 是 2011年提出的，而动量法在 1993 年提出，SGD 在 1951年提出。通过时间轴的对比，我们可以看出人们在不断研究和改进梯度下降算法，从最早的梯度下降法到SGD，再到动量法、小批量梯度下降，最后到 2011 年的 AdaGrad。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;adagrad-算法&quot;&gt;AdaGrad 算法&lt;/h1&gt;
&lt;p&gt;在前面我们讲解了
动量法（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的
AdaGrad 算法。二阶动量的出现，才意味着真正的
&lt;strong&gt;自适应学习率&lt;/strong&gt; 优化算法时代的到来。&lt;/p&gt;
&lt;h2 id=&quot;adagrad-算法的基本思想&quot;&gt;AdaGrad 算法的基本思想&lt;/h2&gt;
&lt;p&gt;我们先回顾一下传统的 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;随机梯度下降法&lt;/a&gt;（SGD）及其各种变种。它们都是以
&lt;strong&gt;同样的学习率&lt;/strong&gt; 来更新 &lt;strong&gt;每一个参数&lt;/strong&gt;
的。但深度神经网络往往包含大量参数，这些参数并不总是
&lt;strong&gt;均匀更新&lt;/strong&gt; 的。有些参数更新得频繁，有些则很少更新。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;strong&gt;经常更新&lt;/strong&gt;
的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的
&lt;strong&gt;学习率小一些&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对于 &lt;strong&gt;偶尔更新&lt;/strong&gt;
的参数，我们了解的信息较少，希望从每一个样本中多学一些，即
&lt;strong&gt;学习率大一些&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要动态度量历史更新的频率，我们引入
&lt;strong&gt;二阶动量&lt;/strong&gt;。二阶动量通过将每一位各自的历史梯度的
&lt;strong&gt;平方&lt;/strong&gt; 叠加起来来计算。具体公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = v_{t-1} + g_t^2
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;span class=&quot;math inline&quot;&gt;&#92;(g_t&#92;)&lt;/span&gt; 是当前的梯度。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>RMSProp 和 Adadelta</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.RMSProp%20%E5%92%8C%20Adadelta/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.RMSProp%20%E5%92%8C%20Adadelta/</id>
    <published>2024-08-12T23:02:27.000Z</published>
    <updated>2025-01-18T16:14:50.734Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rmsprop-和-adadelta-算法">RMSProp 和 Adadelta 算法</h1><p>​ 在 <ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>中，RMSProp 和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad的基础上做了改进，以适应深度学习中的大规模参数优化需求。</p><h2 id="rmsprop-算法">RMSProp 算法</h2><h3 id="基本思想">基本思想</h3><p>RMSProp 对 AdaGrad 进行改进，通过引入 <strong>衰减率</strong>来调整二阶动量的累积。这样可以 <strong>避免</strong> AdaGrad 中<strong>学习率减小过快</strong> 的问题。</p><p>AdaGrad 的二阶动量计算公式如下：</p><p><span class="math display">\[v_t = v_{t-1} + g_t^2\]</span> 而 RMSProp 采用了带有衰减率的计算方式：</p><p><span class="math display">\[v_t = \beta v_{t-1} + (1 - \beta) g_t^2\]</span> 其中，<span class="math inline">\(\beta\)</span>是衰减率系数。</p><span id="more"></span><h3 id="优缺点">优缺点</h3><p><strong>优点：</strong></p><ul><li><strong>自动调整学习率</strong>，避免学习率过大或过小的问题</li><li><strong>加速收敛速度</strong></li><li><strong>简单适用</strong>，适用于各种优化问题</li></ul><p><strong>缺点：</strong></p><ul><li>在处理稀疏特征时不够优秀</li><li>需要调整的超参数较多（衰减率 <spanclass="math inline">\(\beta\)</span> i 和学习率 <spanclass="math inline">\(\alpha\)</span> ）</li><li>收敛速度可能不如某些更先进的 <ahref="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a></li></ul><h3 id="代码实现">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 RMSProp 优化器</span></span><br><span class="line">optimizer = torch.optim.RMSprop([w, b], lr=learning_rate, alpha=beta)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with RMSProp&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="adadelta-算法">Adadelta 算法</h2><h3 id="基本思想-1">基本思想</h3><p>Adadelta 是对 RMSProp 的进一步改进，旨在<strong>自动调整学习率</strong>，避免手动调参。它通过计算梯度和权重更新量的累积值来调整学习率，使得训练过程更加稳定。</p><p>Adadelta 的公式如下：</p><ol type="1"><li>梯度的累积：</li></ol><p><span class="math display">\[E [g^2] _t = \rho E [g^2]_{t-1} + (1 - \rho) g_t^2\]</span></p><ol start="2" type="1"><li>权重更新量的累积：</li></ol><p><span class="math display">\[E [\Delta x^2] _t = \rho E [\Delta x^2]_{t-1} + (1 - \rho) (\Deltax_t)^2\]</span></p><ol start="3" type="1"><li>更新参数：</li></ol><p><span class="math display">\[\Delta x_t = -\frac{\sqrt{E [\Delta x^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}} g_t  \]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_t + \Delta x_t\]</span></p><h3 id="优缺点-1">优缺点</h3><p><strong>优点：</strong><br />- <strong>自动调整学习率</strong>，避免学习率过大或过小的问题 -避免出现学习率饱和现象，使得训练更加稳定</p><p><strong>缺点：</strong></p><ul><li>可能收敛较慢</li><li>需要维护梯度和权重更新量的累积值，增加了空间复杂度</li></ul><h3 id="代码实现-1">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">1.0</span>  <span class="comment"># Adadelta 不需要传统的学习率</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">rho = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">1e-6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 Adadelta 优化器</span></span><br><span class="line">optimizer = torch.optim.Adadelta([w, b], rho=rho, eps=epsilon)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adadelta&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;rmsprop-和-adadelta-算法&quot;&gt;RMSProp 和 Adadelta 算法&lt;/h1&gt;
&lt;p&gt;​ 在 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt;
中，RMSProp 和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad
的基础上做了改进，以适应深度学习中的大规模参数优化需求。&lt;/p&gt;
&lt;h2 id=&quot;rmsprop-算法&quot;&gt;RMSProp 算法&lt;/h2&gt;
&lt;h3 id=&quot;基本思想&quot;&gt;基本思想&lt;/h3&gt;
&lt;p&gt;RMSProp 对 AdaGrad 进行改进，通过引入 &lt;strong&gt;衰减率&lt;/strong&gt;
来调整二阶动量的累积。这样可以 &lt;strong&gt;避免&lt;/strong&gt; AdaGrad 中
&lt;strong&gt;学习率减小过快&lt;/strong&gt; 的问题。&lt;/p&gt;
&lt;p&gt;AdaGrad 的二阶动量计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = v_{t-1} + g_t^2
&#92;]&lt;/span&gt; 而 RMSProp 采用了带有衰减率的计算方式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = &#92;beta v_{t-1} + (1 - &#92;beta) g_t^2
&#92;]&lt;/span&gt; 其中，&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;beta&#92;)&lt;/span&gt;
是衰减率系数。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Momentum</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/</id>
    <published>2024-08-12T22:02:27.000Z</published>
    <updated>2025-01-18T16:14:45.432Z</updated>
    
    <content type="html"><![CDATA[<h1 id="动量法momentum">动量法（Momentum）</h1><h2 id="背景知识">背景知识</h2><p>在 <ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>的优化过程中，梯度下降法（Gradient Descent,GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在<strong>收敛速度慢</strong>、<strong>容易陷入局部最小值</strong> 以及在<strong>高维空间中震荡较大</strong>的问题。为了解决这些问题，人们提出了动量法（Momentum）。</p><h2 id="动量法的概念">动量法的概念</h2><p>动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：<strong>在运动方向上保持运动的趋势，从而加速收敛</strong>。</p><h2 id="动量法在深度学习中的应用">动量法在深度学习中的应用</h2><p>在深度学习中，动量法通过记录 <strong>梯度的增量</strong> 并将其与<strong>当前梯度相加</strong>，来 <strong>平滑梯度下降</strong>的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。</p><p>动量法的更新公式如下：</p><p><span class="math display">\[m_t = \beta m_{t-1} + \nabla L(w_t)\]</span> <span class="math display">\[w_{t+1} = w_t - \alpha m_t\]</span></p><p>其中：</p><ul><li><span class="math inline">\(m_t\)</span>是动量项，记录了之前梯度的累积。<br /></li><li><span class="math inline">\(\beta\)</span> 是动量参数，控制<strong>动量项的衰减</strong>，一般取值为 0.9。<br /></li><li><span class="math inline">\(\nabla L(w_t)\)</span>是当前参数的梯度。</li><li><span class="math inline">\(\alpha\)</span> 是学习率。</li></ul><span id="more"></span><h2 id="动量法的优点">动量法的优点</h2><ol type="1"><li><p><strong>加速收敛</strong>：动量法通过积累之前的梯度信息，使得优化过程更为顺畅，避免了曲折路径，提高了收敛速度。<br /></p></li><li><p><strong>跳过局部最小值</strong>：由于动量的累积作用，可以帮助优化算法跳过一些局部最小值，找到更优的解。<br /></p></li><li><p><strong>减少振荡</strong>：动量法可以有效减小学习过程中梯度震荡的现象，使得模型的训练更加稳定。## 动量法的缺点</p></li><li><p><strong>计算复杂度增加</strong>：由于需要维护动量项，会导致计算复杂度的增加</p></li><li><p><strong>参数调节</strong>：动量法引入了新的超参数（动量系数 <spanclass="math inline">\(\beta\)</span>），需要在实际应用中进行调节</p></li></ol><h2 id="动量法的改进及变种">动量法的改进及变种</h2><p>​ 在动量法的基础上，还有一些改进和变种，如 Nesterov 加速梯度（NesterovAccelerated <ahref="https://so.csdn.net/so/search?q=Gradient&amp;spm=1001.2101.3001.7020">Gradient</a>,NAG）、RMSprop、Adam等。这些方法在动量法的基础上进一步优化了收敛速度和稳定性。</p><h2 id="实验代码示例">实验代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据生成</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">X = torch.randn(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span> * X.squeeze() + <span class="number">2</span> + torch.randn(<span class="number">1000</span>) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同梯度下降方法的比较</span></span><br><span class="line">methods = &#123;</span><br><span class="line">    <span class="string">&#x27;SGD&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>),</span><br><span class="line">    <span class="string">&#x27;Momentum&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">losses = &#123;method: [] <span class="keyword">for</span> method <span class="keyword">in</span> methods&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> method_name, optimizer_fn <span class="keyword">in</span> methods.items():</span><br><span class="line">    model = LinearModel()</span><br><span class="line">    optimizer = optimizer_fn(model.parameters())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(X)</span><br><span class="line">        loss = criterion(outputs.squeeze(), y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        losses[method_name].append(loss.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line"><span class="keyword">for</span> method_name, loss_values <span class="keyword">in</span> losses.items():</span><br><span class="line">    plt.plot(loss_values, label=method_name)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Loss Curve Comparison&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112029254.png"alt="image-20240811202901191" /><figcaption aria-hidden="true">image-20240811202901191</figcaption></figure><h2 id="结论">结论</h2><p>动量法通过引入动量项，显著提高了 <ahref="https://so.csdn.net/so/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">梯度下降法</a>的收敛速度和稳定性。尽管在实际应用中引入了额外的计算开销，但其在许多深度学习任务中的表现优异，已经成为常用的优化方法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;动量法momentum&quot;&gt;动量法（Momentum）&lt;/h1&gt;
&lt;h2 id=&quot;背景知识&quot;&gt;背景知识&lt;/h2&gt;
&lt;p&gt;在 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt;
的优化过程中，梯度下降法（Gradient Descent,
GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在
&lt;strong&gt;收敛速度慢&lt;/strong&gt;、&lt;strong&gt;容易陷入局部最小值&lt;/strong&gt; 以及在
&lt;strong&gt;高维空间中震荡较大&lt;/strong&gt;
的问题。为了解决这些问题，人们提出了动量法（Momentum）。&lt;/p&gt;
&lt;h2 id=&quot;动量法的概念&quot;&gt;动量法的概念&lt;/h2&gt;
&lt;p&gt;动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：&lt;strong&gt;在运动方向上保持运动的趋势，从而加速收敛&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;动量法在深度学习中的应用&quot;&gt;动量法在深度学习中的应用&lt;/h2&gt;
&lt;p&gt;在深度学习中，动量法通过记录 &lt;strong&gt;梯度的增量&lt;/strong&gt; 并将其与
&lt;strong&gt;当前梯度相加&lt;/strong&gt;，来 &lt;strong&gt;平滑梯度下降&lt;/strong&gt;
的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。&lt;/p&gt;
&lt;p&gt;动量法的更新公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
m_t = &#92;beta m_{t-1} + &#92;nabla L(w_t)
&#92;]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;&#92;[
w_{t+1} = w_t - &#92;alpha m_t
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(m_t&#92;)&lt;/span&gt;
是动量项，记录了之前梯度的累积。&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;beta&#92;)&lt;/span&gt; 是动量参数，控制
&lt;strong&gt;动量项的衰减&lt;/strong&gt;，一般取值为 0.9。&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;nabla L(w_t)&#92;)&lt;/span&gt;
是当前参数的梯度。&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;alpha&#92;)&lt;/span&gt; 是学习率。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>mini-batch GD</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3.Mini-batch%20GD/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3.Mini-batch%20GD/</id>
    <published>2024-08-12T21:02:27.000Z</published>
    <updated>2025-01-18T16:14:42.166Z</updated>
    
    <content type="html"><![CDATA[<h1id="小批量梯度下降法mini-batch-gradient-descent">小批量梯度下降法（Mini-batchGradient Descent）</h1><p>在 <ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020">深度学习模型</a>的训练过程中，梯度下降法是最常用的优化算法之一。我们前面介绍了梯度下降法（BatchGradient Descent）和随机梯度下降法（Stochastic GradientDescent），两者各有优缺点。为了在 <strong>计算速度</strong> 和<strong>收敛稳定性</strong> 之间找到<strong>平衡</strong>，<strong>小批量梯度下降法</strong>（Mini-batchGradientDescent）应运而生。下面我们详细介绍其基本思想、优缺点，并通过代码实现来比较三种梯度下降法。</p><h2 id="小批量梯度下降法的基本思想">小批量梯度下降法的基本思想</h2><p>​ 小批量梯度下降法在每次迭代中，使用<strong>一小部分随机样本（称为小批量）</strong>来计算梯度，并更新参数值。具体来说，算法步骤如下：</p><ol type="1"><li><p>初始化参数 <span class="math inline">\(w\)</span> 和 <spanclass="math inline">\(b\)</span></p></li><li><p>在每次迭代中，从训练集中随机抽取 <spanclass="math inline">\(m\)</span> 个样本。</p></li><li><p>使用这 <span class="math inline">\(m\)</span>个样本计算损失函数的梯度</p></li><li><p>更新参数 <span class="math inline">\(w\)</span> 和 <spanclass="math inline">\(b\)</span></p></li></ol><p>其梯度计算公式如下：</p><p><span class="math display">\[w_{t+1}= w_{t}-\alpha \cdot \frac{1}{m}\sum_{i = 1}^m{\nabla _w}L(w_{t},b_{t}, x_i, y_i),\\b_{t+1}= b_{t}-\alpha \cdot \frac{1}{m}\sum_{i = 1}^m{\nabla _b}L(w_{t},b_{t}, x_i, y_i),\]</span> 其中，<span class="math inline">\(\alpha\)</span> 是学习率，<span class="math inline">\(m\)</span> 是小批量的大小。</p><ul><li>当 <span class="math inline">\(m=1\)</span> 时，Mini-batch GradientDescent 转换为 SGD</li><li>当 <span class="math inline">\(m=n\)</span> 时，Mini-batch GradientDescent 转换为 GD</li></ul><span id="more"></span><h2 id="优缺点">优缺点</h2><h3 id="优点">优点</h3><ol type="1"><li><strong>计算速度快</strong>：与批量梯度下降法相比，每次迭代只需计算小批量样本的梯度，速度更快。<br /></li><li><strong>减少振荡</strong>：与 <ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>相比，梯度的计算更加稳定，减少了参数更新时的振荡。<br /></li><li><strong>控制灵活</strong>：可以调整小批量的大小，使得训练速度和精度之间达到平衡。</li></ol><h3 id="缺点">缺点</h3><ol type="1"><li><strong>需要调整学习率和小批量大小</strong>：学习率决定每次更新的步长，小批量大小决定每次计算梯度使用的样本数量。<br /></li><li><strong>内存消耗</strong>：小批量大小的选择受限于内存容量，尤其在使用GPU 运算时，需要选择合适的小批量大小。</li></ol><h3 id="代码实现">代码实现</h3><p>下面通过代码实现和比较三种梯度下降法的执行效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span>*X + <span class="number">2</span> + np.random.randn(<span class="number">1000</span>, <span class="number">1</span>) * <span class="number">0.1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 转换为 tensor</span></span><br><span class="line">X_tensor = torch.tensor(X, dtype=torch.float32)</span><br><span class="line">y_tensor = torch.tensor(y, dtype=torch.float32)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 封装为数据集</span></span><br><span class="line">dataset = TensorDataset(X_tensor, y_tensor)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearRegressionModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义梯度下降法的批量大小</span></span><br><span class="line">batch_sizes = [<span class="number">1000</span>, <span class="number">1</span>, <span class="number">128</span>]</span><br><span class="line">batch_labels = [<span class="string">&#x27;Batch Gradient Descent&#x27;</span>, <span class="string">&#x27;Stochastic Gradient Descent&#x27;</span>, <span class="string">&#x27;Mini-batch Gradient Descent&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储损失值</span></span><br><span class="line">losses = &#123;label: [] <span class="keyword">for</span> label <span class="keyword">in</span> batch_labels&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> batch_size, label, color <span class="keyword">in</span> <span class="built_in">zip</span>(batch_sizes, batch_labels, colors):</span><br><span class="line">    model = LinearRegressionModel()</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">    </span><br><span class="line">    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs), desc=label):</span><br><span class="line">        epoch_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> data_loader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(batch_x)</span><br><span class="line">            loss = criterion(outputs, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        losses[label].append(epoch_loss / <span class="built_in">len</span>(data_loader))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制损失值变化曲线</span></span><br><span class="line"><span class="keyword">for</span> label, color <span class="keyword">in</span> <span class="built_in">zip</span>(batch_labels, colors):</span><br><span class="line">    plt.plot(losses[label], color=color, label=label)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="结果分析">结果分析</h3><p>运行上述代码后，会显示三种梯度下降法在每个迭代周期（epoch）中的损失变化曲线。可以看到：</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112012585.png"alt="image-20240811201255444" /><figcaption aria-hidden="true">image-20240811201255444</figcaption></figure><ol type="1"><li>批量梯度下降法：损失曲线平滑，但训练速度较慢。<br /></li><li>随机梯度下降法：训练速度快，但损失曲线波动较大。<br /></li><li>小批量梯度下降法：在训练速度和损失曲线的稳定性之间达到了平衡，效果较为理想。</li></ol><h2 id="总结">总结</h2><p>​小批量梯度下降法结合了批量梯度下降法和随机梯度下降法的优点，是深度学习中常用的<ahref="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a>。通过调整小<strong>批量大小</strong> 和 <strong>学习率</strong>，可以在<strong>训练速度</strong> 和 <strong>收敛稳定性</strong>之间找到最佳平衡。在实际应用中，小批量梯度下降法由于其较高的效率和较好的收敛效果，被广泛应用于各类深度学习模型的训练中。</p>]]></content>
    
    
    <summary type="html">&lt;h1
id=&quot;小批量梯度下降法mini-batch-gradient-descent&quot;&gt;小批量梯度下降法（Mini-batch
Gradient Descent）&lt;/h1&gt;
&lt;p&gt;在 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习模型&lt;/a&gt;
的训练过程中，梯度下降法是最常用的优化算法之一。我们前面介绍了梯度下降法（Batch
Gradient Descent）和随机梯度下降法（Stochastic Gradient
Descent），两者各有优缺点。为了在 &lt;strong&gt;计算速度&lt;/strong&gt; 和
&lt;strong&gt;收敛稳定性&lt;/strong&gt; 之间找到
&lt;strong&gt;平衡&lt;/strong&gt;，&lt;strong&gt;小批量梯度下降法&lt;/strong&gt;（Mini-batch
Gradient
Descent）应运而生。下面我们详细介绍其基本思想、优缺点，并通过代码实现来比较三种梯度下降法。&lt;/p&gt;
&lt;h2 id=&quot;小批量梯度下降法的基本思想&quot;&gt;小批量梯度下降法的基本思想&lt;/h2&gt;
&lt;p&gt;​ 小批量梯度下降法在每次迭代中，使用
&lt;strong&gt;一小部分随机样本（称为小批量）&lt;/strong&gt;
来计算梯度，并更新参数值。具体来说，算法步骤如下：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;初始化参数 &lt;span class=&quot;math inline&quot;&gt;&#92;(w&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(b&#92;)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在每次迭代中，从训练集中随机抽取 &lt;span
class=&quot;math inline&quot;&gt;&#92;(m&#92;)&lt;/span&gt; 个样本。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用这 &lt;span class=&quot;math inline&quot;&gt;&#92;(m&#92;)&lt;/span&gt;
个样本计算损失函数的梯度&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;更新参数 &lt;span class=&quot;math inline&quot;&gt;&#92;(w&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(b&#92;)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其梯度计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
w_{t+1}= w_{t}-&#92;alpha &#92;cdot &#92;frac{1}{m}&#92;sum_{i = 1}^m{&#92;nabla _w}L(w_{t},
b_{t}, x_i, y_i),
&#92;&#92;
b_{t+1}= b_{t}-&#92;alpha &#92;cdot &#92;frac{1}{m}&#92;sum_{i = 1}^m{&#92;nabla _b}L(w_{t},
b_{t}, x_i, y_i),
&#92;]&lt;/span&gt; 其中，&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;alpha&#92;)&lt;/span&gt; 是学习率，
&lt;span class=&quot;math inline&quot;&gt;&#92;(m&#92;)&lt;/span&gt; 是小批量的大小。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 &lt;span class=&quot;math inline&quot;&gt;&#92;(m=1&#92;)&lt;/span&gt; 时，Mini-batch Gradient
Descent 转换为 SGD&lt;/li&gt;
&lt;li&gt;当 &lt;span class=&quot;math inline&quot;&gt;&#92;(m=n&#92;)&lt;/span&gt; 时，Mini-batch Gradient
Descent 转换为 GD&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SGD</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2.SGD/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2.SGD/</id>
    <published>2024-08-12T20:02:27.000Z</published>
    <updated>2025-01-18T16:14:38.281Z</updated>
    
    <content type="html"><![CDATA[<h1 id="随机梯度下降法sgd">随机梯度下降法（SGD）</h1><p>​ 在 深度学习 中，梯度下降法（GradientDescent）是最常用的模型参数优化方法。然而，传统的梯度下降法（Full BatchLearning）存在一些缺点，例如训练时间过长和容易陷入局部最小值。为了解决这些问题，随机梯度下降法（StochasticGradient Descent，简称 SGD）应运而生。</p><h2 id="传统梯度下降法的问题">传统梯度下降法的问题</h2><ol type="1"><li><strong>训练时间长</strong>：传统梯度下降法需要使用所有训练数据来计算梯度，因此数据量大时耗时严重。<br /></li><li><strong>容易陷入局部最小值</strong>：复杂的损失函数可能会导致算法在局部最小值附近来回震荡，无法快速收敛。<br /></li><li><strong>对初始值敏感</strong>：初始值选择不当可能导致算法被卡在局部最小值。</li></ol><h2 id="随机梯度下降法-的基本思想"><ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>的基本思想</h2><p>SGD 每次迭代仅 <strong>使用一个样本</strong> 的<strong>损失值</strong>来计算梯度，而不是全数据集损失值的求和平均来计算梯度。</p><span id="more"></span><h2 id="sgd-的优缺点">SGD 的优缺点</h2><h3 id="优点">优点</h3><ol type="1"><li><strong>速度快</strong>：每次迭代只需计算一个样本的梯度，速度比传统方法快很多。<br /></li><li><strong>避免局部最小值</strong>：因为每次更新参数只使用一个样本，随机性使得算法不容易陷入局部最小值。</li><li><strong>更易实现和调整</strong>：每个样本的梯度可以分别计算，并行处理更加高效。</li></ol><h3 id="缺点">缺点</h3><ol type="1"><li><strong>收敛不稳定</strong>：每次迭代梯度都会有噪声，可能导致收敛不稳定。</li><li><strong>方差较大</strong>：每次更新参数只使用一个样本的梯度，可能导致算法方差较大，难以收敛。</li></ol><h2 id="动态学习率">动态学习率</h2><p>为了提高 SGD的收敛性，可以使用动态学习率。常见的动态学习率策略包括：</p><ol type="1"><li><p><strong>反比例学习率</strong>：初始学习率随着迭代次数增加而减小。</p></li><li><p><strong>反比例平方学习率</strong>：类似反比例学习率，但减小速度更快。</p></li></ol><p><span class="math display">\[\alpha_t = \frac{\alpha_0}{1 + k \cdot t^2}\]</span></p><ol start="3" type="1"><li><strong>指数衰减学习率</strong>：学习率以指数形式衰减。</li></ol><p><span class="math display">\[\alpha_t = \alpha_0 \cdot e^{-\lambda t}\]</span></p><h2 id="实现示例">实现示例</h2><p>下面是使用 Python 实现随机梯度下降法的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_descent</span>(<span class="params">X, y, lr=<span class="number">0.01</span>, epochs=<span class="number">1000</span></span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line">    w = np.zeros(n)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            random_index = np.random.randint(m)</span><br><span class="line">            xi = X[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            gradient_w = <span class="number">2</span> * xi.T.dot(xi.dot(w) + b - yi)</span><br><span class="line">            gradient_b = <span class="number">2</span> * (xi.dot(w) + b - yi)</span><br><span class="line">            w = w - lr * gradient_w</span><br><span class="line">            b = b - lr * gradient_b</span><br><span class="line">        <span class="comment"># 可选：动态学习率调整</span></span><br><span class="line">        lr = lr / (<span class="number">1</span> + epoch / epochs)</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">y = np.array([<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用 SGD 函数</span></span><br><span class="line">w, b = stochastic_gradient_descent(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;权重:&quot;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;偏置:&quot;</span>, b)</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><p>SGD 通过每次迭代使用一个 <strong>随机样本</strong>来计算梯度，从而加快了计算速度并避免陷入局部最小值。动态学习率的使用可以进一步提高SGD 的收敛性。在实际应用中，SGD已成为深度学习领域最常用的优化算法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;随机梯度下降法sgd&quot;&gt;随机梯度下降法（SGD）&lt;/h1&gt;
&lt;p&gt;​ 在 深度学习 中，梯度下降法（Gradient
Descent）是最常用的模型参数优化方法。然而，传统的梯度下降法（Full Batch
Learning）存在一些缺点，例如训练时间过长和容易陷入局部最小值。为了解决这些问题，随机梯度下降法（Stochastic
Gradient Descent，简称 SGD）应运而生。&lt;/p&gt;
&lt;h2 id=&quot;传统梯度下降法的问题&quot;&gt;传统梯度下降法的问题&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;strong&gt;训练时间长&lt;/strong&gt;：传统梯度下降法需要使用所有训练数据来计算梯度，因此数据量大时耗时严重。&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;容易陷入局部最小值&lt;/strong&gt;：复杂的损失函数可能会导致算法在局部最小值附近来回震荡，无法快速收敛。&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对初始值敏感&lt;/strong&gt;：初始值选择不当可能导致算法被卡在局部最小值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;随机梯度下降法-的基本思想&quot;&gt;&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;随机梯度下降法&lt;/a&gt;
的基本思想&lt;/h2&gt;
&lt;p&gt;SGD 每次迭代仅 &lt;strong&gt;使用一个样本&lt;/strong&gt; 的
&lt;strong&gt;损失值&lt;/strong&gt;
来计算梯度，而不是全数据集损失值的求和平均来计算梯度。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>GD</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1.GD/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1.GD/</id>
    <published>2024-08-12T19:02:27.000Z</published>
    <updated>2025-01-18T16:14:33.084Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度下降法gradient-descent">梯度下降法（Gradient Descent）</h1><h2 id="引言">引言</h2><p>​ 在深度学习中，<ahref="https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">损失函数</a>的求解是一个关键步骤。损失函数通常没有解析解，因此需要通过最优化算法来逼近求解。其中，梯度下降法是最常用的优化算法之一。本文将详细介绍梯度下降法的基本概念、理论基础、及其在深度学习中的应用。</p><h2 id="梯度下降法的基本概念">梯度下降法的基本概念</h2><p>梯度下降法（GradientDescent）是一种基于一阶导数的优化算法，用于最小化目标函数。在深度学习中，目标函数通常是损失函数，其目的是通过调整参数来使损失最小化。</p><h3 id="损失函数的定义">损失函数的定义</h3><p>假设损失函数 <span class="math inline">\(L\)</span> 是参数 <spanclass="math inline">\(W\)</span> 的函数：<spanclass="math inline">\(L(W)\)</span>，我们的目标是找到参数 <spanclass="math inline">\(W\)</span> 使得 <spanclass="math inline">\(L(W)\)</span> 最小化。</p><span id="more"></span><h3 id="梯度的定义">梯度的定义</h3><p>梯度是损失函数的导数，表示函数在某一点处的最陡下降方向。对于参数<span class="math inline">\(W\)</span> 的每个分量 <spanclass="math inline">\(w_i\)</span> ，梯度表示为：</p><p><span class="math display">\[\nabla L(W)=\left [\frac{\partial L}{\partial w_1},\frac{\partialL}{\partial w_2},\ldots,\frac{\partial L}{\partial w_n}\right]\]</span></p><blockquote><p><strong>梯度的维度</strong> = <strong>参数的个数</strong></p></blockquote><h3 id="梯度下降算法">梯度下降算法</h3><p>梯度下降法通过以下步骤更新参数：</p><p><span class="math display">\[w_{t+1}= w_{t}-\alpha \cdot \frac{1}{n}\sum_{i = 1}^n{\nabla _w}L(w_{t},b_{t}, x_i, y_i),\\b_{t+1}= b_{t}-\alpha \cdot \frac{1}{n}\sum_{i = 1}^n{\nabla _b}L(w_{t},b_{t}, x_i, y_i),\]</span> 其中，<span class="math inline">\(\alpha\)</span>是学习率（Learning Rate），决定了每次更新的步长；<spanclass="math inline">\(n\)</span> 是样本大小。</p><h2 id="梯度下降法的应用">梯度下降法的应用</h2><h3 id="简单示例二次损失函数">简单示例：二次损失函数</h3><p>为了便于理解，我们假设损失函数是一个简单的二次函数</p><p><span class="math display">\[L(W) = W^2\]</span> 梯度为：</p><p><span class="math display">\[\nabla L(W) = 2W\]</span> 根据梯度下降法的更新规则，参数更新为：</p><p><span class="math display">\[W_{t+1} = W_t - \alpha \cdot 2W_t = W_t(1 - 2\alpha)\]</span></p><h3 id="高维度情况下的梯度下降">高维度情况下的梯度下降</h3><p>​在实际应用中，损失函数往往是高维度的。梯度下降法可以扩展到高维度情况，其中<strong>梯度是一个向量，表示每个参数</strong>的导数。我们将梯度表示为一个向量，并对每个参数进行更新。</p><h3 id="学习率的选择">学习率的选择</h3><p>学习率 <span class="math inline">\(\alpha\)</span>对梯度下降法的收敛速度和稳定性有重大影响。选择合适的学习率非常重要。</p><ul><li>如果学习率过大，算法可能会在最小值附近来回<strong>震荡</strong>；</li><li>如果学习率过小，算法的 <strong>收敛速度会非常慢</strong>。</li></ul><h2 id="梯度下降法的变体">梯度下降法的变体</h2><p>在实际应用中，梯度下降法有多种变体，以提高收敛速度和稳定性。常见的变体包括：</p><ul><li><strong>随机梯度下降法（SGD）</strong>：每次迭代使用一个或几个样本来更新参数，而不是使用整个训练集。这种方法可以显著加快计算速度。<br /></li><li><strong>动量法（Momentum）</strong>：在每次更新时，加入之前更新的动量，以加速收敛。<br /></li><li><strong>自适应学习率</strong> 方法：例如 Adagrad、RMSprop、Adam等，通过 <strong>动态调整学习率</strong> 来提高收敛效果。</li></ul><h2 id="总结">总结</h2><p>梯度下降法是深度学习中最常用的优化算法之一。通过计算损失函数的梯度，确定参数的更新方向和步长，不断逼近损失函数的最小值。<strong>选择合适的学习率和初始点是梯度下降法（GD）成功的关键。</strong>理解梯度下降法的基本概念和应用，对于深入学习 <ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">深度学习算法</a>有重要意义。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;梯度下降法gradient-descent&quot;&gt;梯度下降法（Gradient Descent）&lt;/h1&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;​ 在深度学习中，&lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;损失函数&lt;/a&gt;
的求解是一个关键步骤。损失函数通常没有解析解，因此需要通过最优化算法来逼近求解。其中，梯度下降法是最常用的优化算法之一。本文将详细介绍梯度下降法的基本概念、理论基础、及其在深度学习中的应用。&lt;/p&gt;
&lt;h2 id=&quot;梯度下降法的基本概念&quot;&gt;梯度下降法的基本概念&lt;/h2&gt;
&lt;p&gt;梯度下降法（Gradient
Descent）是一种基于一阶导数的优化算法，用于最小化目标函数。在深度学习中，目标函数通常是损失函数，其目的是通过调整参数来使损失最小化。&lt;/p&gt;
&lt;h3 id=&quot;损失函数的定义&quot;&gt;损失函数的定义&lt;/h3&gt;
&lt;p&gt;假设损失函数 &lt;span class=&quot;math inline&quot;&gt;&#92;(L&#92;)&lt;/span&gt; 是参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(W&#92;)&lt;/span&gt; 的函数：&lt;span
class=&quot;math inline&quot;&gt;&#92;(L(W)&#92;)&lt;/span&gt;，我们的目标是找到参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(W&#92;)&lt;/span&gt; 使得 &lt;span
class=&quot;math inline&quot;&gt;&#92;(L(W)&#92;)&lt;/span&gt; 最小化。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的优化算法探讨</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%92%8C%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%92%8C%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</id>
    <published>2024-08-12T18:02:27.000Z</published>
    <updated>2025-01-18T16:14:29.296Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习中的优化算法探讨">深度学习中的优化算法探讨</h1><p>​ 在深度学习的过程中，<strong>优化算法</strong>扮演着至关重要的角色。训练神经网络通常需要投入大量的时间和资源，而优化算法的选择和应用直接影响模型的训练效率和效果。数值优化是一个庞大的数学学科，本篇文章将探讨与深度学习，特别是训练过程密切相关的优化算法。</p><h2id="最优化理论和深度学习优化算法的区别">最优化理论和深度学习优化算法的区别</h2><h3 id="度量和损失函数">度量和损失函数</h3><ul><li><strong>最优化理论</strong>：研究如何找到函数的<strong>最优解</strong>，即最大值或最小值，通常有明确的度量标准。</li><li><strong>深度学习</strong>：使用代理损失函数（如负对数似然或交叉熵）来进行优化，通过<strong>最小化代理损失函数</strong> 来最大化原始度量。</li></ul><h3 id="数据关注点">数据关注点</h3><ul><li><strong>最优化理论</strong>：只关心现有数据的最优解。</li><li><strong>深度学习</strong>：关注模型的泛化能力，即模型<strong>在测试集</strong> 上的表现，避免过拟合现象。</li></ul><h3 id="研究内容">研究内容</h3><ul><li><strong>最优化理论</strong>：注重算法本身的研究。</li><li><strong>深度学习</strong>：关注实现细节，包括神经网络的结构、参数调整等。</li></ul><span id="more"></span><h2 id="训练误差与泛化误差">训练误差与泛化误差</h2><ul><li><strong>训练误差</strong>：模型在 <strong>训练集</strong>上的误差，只关注 <strong>训练过程</strong> 中的表现。</li><li><strong>泛化误差</strong>：模型在未见过的数据（<strong>测试集</strong>）上的误差，关注模型的<strong>泛化能力</strong>。</li></ul><p>泛化误差的衡量是深度学习优化的<strong>核心</strong>，理想的模型应该在新数据上也能表现良好。</p><h2 id="经验风险与真实风险">经验风险与真实风险</h2><ul><li><strong>经验风险（Empirical Risk）</strong>：<strong>训练集</strong>上的期望 <strong>损失</strong>，通过 <strong>最小化经验风险</strong>来优化模型。</li><li><strong>真实风险（Expected Risk）</strong>：使用<strong>真实数据</strong> 计算损失函数的期望值，由于无法直接计算<strong>真实风险</strong>，因此通过 <strong>优化经验风险</strong>来尽量减少泛化误差。</li></ul><h2 id="深度学习优化中的挑战">深度学习优化中的挑战</h2><h3 id="病态问题ill-conditioned-problem">病态问题（Ill-conditionedProblem）</h3><ul><li>问题解对条件非常敏感，即使微小的变化也会导致解的大幅变化。</li><li>解决方案：正则化技术、数据预处理等。</li></ul><h3 id="局部最小值问题local-minima">局部最小值问题（Local Minima）</h3><ul><li>优化过程中可能陷入局部最小值，而不是全局最优值。</li><li>解决方案：使用不同的优化算法，如随机梯度下降（SGD）、Adam 等。</li></ul><h3 id="鞍点问题saddle-points">鞍点问题（Saddle Points）</h3><ul><li>损失函数在某些点的曲率为零，但不是全局最优点。</li><li>解决方案：减少模型复杂度、增加训练数据、使用随机初始化等。</li></ul><h3 id="悬崖问题cliffs">悬崖问题（Cliffs）</h3><ul><li>多层神经网络中的损失函数可能存在陡峭的区域，导致梯度更新大幅改变参数值。</li><li>解决方案：梯度裁剪（Gradient Clipping）以控制梯度大小。</li></ul><h3id="长期依赖问题long-term-dependency-problem">长期依赖问题（Long-termDependency Problem）</h3><ul><li>深层网络结构使得模型难以学习到先前的信息，导致梯度消失或爆炸。</li><li>解决方案：使用 LSTM 或 GRU 等特殊的循环神经网络结构</li></ul><h2 id="总结">总结</h2><p>​优化算法是深度学习模型训练中的核心工具，两者有密切的联系但也有显著的区别。最优化理论关注的是<strong>训练误差</strong>，而深度学习关注的是<strong>泛化误差</strong>。深度学习通过 <strong>最小化经验风险</strong>来尽量减少<strong>泛化误差</strong>。优化过程中面临的挑战包括病态问题、局部最小值、鞍点、悬崖和长期依赖问题，这些问题需要通过不同的优化算法和策略来解决。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度学习中的优化算法探讨&quot;&gt;深度学习中的优化算法探讨&lt;/h1&gt;
&lt;p&gt;​ 在深度学习的过程中，&lt;strong&gt;优化算法&lt;/strong&gt;
扮演着至关重要的角色。训练神经网络通常需要投入大量的时间和资源，而优化算法的选择和应用直接影响模型的训练效率和效果。数值优化是一个庞大的数学学科，本篇文章将探讨与深度学习，特别是训练过程密切相关的优化算法。&lt;/p&gt;
&lt;h2
id=&quot;最优化理论和深度学习优化算法的区别&quot;&gt;最优化理论和深度学习优化算法的区别&lt;/h2&gt;
&lt;h3 id=&quot;度量和损失函数&quot;&gt;度量和损失函数&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;最优化理论&lt;/strong&gt;：研究如何找到函数的
&lt;strong&gt;最优解&lt;/strong&gt;，即最大值或最小值，通常有明确的度量标准。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习&lt;/strong&gt;：使用代理损失函数（如负对数似然或交叉熵）来进行优化，通过
&lt;strong&gt;最小化代理损失函数&lt;/strong&gt; 来最大化原始度量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;数据关注点&quot;&gt;数据关注点&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;最优化理论&lt;/strong&gt;：只关心现有数据的最优解。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习&lt;/strong&gt;：关注模型的泛化能力，即模型
&lt;strong&gt;在测试集&lt;/strong&gt; 上的表现，避免过拟合现象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;研究内容&quot;&gt;研究内容&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;最优化理论&lt;/strong&gt;：注重算法本身的研究。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度学习&lt;/strong&gt;：关注实现细节，包括神经网络的结构、参数调整等。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>优化算法超参数</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E8%B6%85%E5%8F%82%E6%95%B0/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E8%B6%85%E5%8F%82%E6%95%B0/</id>
    <published>2024-08-12T18:02:27.000Z</published>
    <updated>2025-02-09T09:24:28.829Z</updated>
    
    <content type="html"><![CDATA[<h1 id="优化算法超参数">优化算法超参数</h1><p>先看几个公式！ <span class="math display">\[\begin{equation} w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i}\quad \end{equation} 　........ (1)\]</span></p><p><span class="math display">\[\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\frac{\lambda}{2}\mathbf{w}^2\]</span></p><p><span class="math display">\[\begin{equation} w_i \leftarrow w_i-\eta\frac{\partial E}{\partialw_i}-\eta\lambda w_i \quad \end{equation} ......(2)\]</span></p><p><span class="math display">\[\begin{equation} w_i \leftarrow m^{} w_i-\eta\frac{\partial E}{\partialw_i}-\eta\lambda w_i \end{equation} .........(3)\]</span></p><p>超参数是指机器学习模型里面的框架参数，和训练过程中学习的参数（权重）不一样，超参数通常是手工设定，不断试错调整，或者对一系列穷举出来的参数组合一通进行枚举（网格搜索）。</p><p>深度学习和神经网络模型，有很多这样的参数需要学习。时至今日，非参数学习研究正在帮助深度学习更加自动的优化模型参数选择，当然有经验的专家仍然是必须的。</p><h2 id="learning-rate-gradient-coefficient">Learning Rate (gradientcoefficient)</h2><p>上面(1)式中的 <span class="math inline">\(\eta\)</span>。</p><p>学习率决定了权值更新的速度，设置得太大会使结果超过最优值，太小会使下降速度过慢。</p><h2 id="weight-decay-regularization-coefficient">Weight decay(regularization coefficient)</h2><p>上面(2)式中的 <span class="math inline">\(\lambda\)</span>。</p><p>在实际应用中，为了避免网络的过拟合，必须对误差函数E（网络术语中也叫损失函数 lossfunction，是一个意思）正则化(regularization)处理，即加入正则项。</p><p>在机器学习或者模式识别中，过拟合overfitting的出现，往往是因为网络权值变得过大，换句话说，参数过大往往是因为数据的高频跳跃成份所导致，因此，避免出现overfitting的办法就是想办法抵消这些过大的参数值，也就是给误差函数添加一个惩罚项，如(2)式所示。</p><p>正则化的基本思想就是通过惩罚项来消减不必要的过大的参数（有人把这比喻成奥卡姆剃刀），通过惩罚项，消除这些不必要的值的影响，使曲线更平滑。对最后结果的影响是，网络中重要的权重不会受到Weightdecay影响。进一步参考<ahref="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Regularization_%28mathematics%29%23Regularization_in_statistics_and_machine_learning">wikipediaRegularization</a>.</p><h2 id="momentum">Momentum</h2><p>上面(3)式中的 <span class="math inline">\(m\)</span>。</p><p>动量来源于牛顿定律，基本思想是为了找到最优加入“惯性”的影响。例如，当误差曲面中存在平坦区域时，我们可以通过调整这个值使权值加大（加重），从而在每次递推时(iteration)，迈出更大的步伐，加速收敛。</p><p>一个典型的 <span class="math inline">\(m\)</span>就是，通过积累历史搜索方向（典型的就是在 SGD中，通过线性组合以前的历史搜索方向来更新当前的方向），避免或者说消除与以前搜索方向相反的方向（怎么看都像是共轭梯度法呀，只不过没有明确的共轭方向，因而不能保证共轭；当然，这里确实可以另改为采用共轭的方向）。</p><p>至于说跳离局部最小值，主要还是 SGD 的功劳，momentum在其中只是辅助的。</p><p>建议参考：<ahref="https://www.zhihu.com/question/24529483">https://www.zhihu.com/question/24529483</a></p><p>注：这里只是示意，（3）式一般文献中标准的写法为</p><p><span class="math display">\[v_{t+1} \leftarrow \mu v_{t} -\eta \triangledown E_w\\\\w_i \leftarrow w_i + v_{t+1}\]</span> 其中 <span class="math inline">\(\mu\)</span> 为 momentumcoefficient，它是上一次权重的变化量的系数，另一种写法对应关系如下</p><p><span class="math display">\[\begin{equation} \begin{array}{c} v_{t+1}=\Delta w_i(n)\\\\v_{t}=\Delta w_i(n-1) \end{array} \end{equation}\]</span> 则可写成参考【1】所示的形式</p><p><span class="math display">\[\Delta w_i(n) = \mu \Delta w_i(n-1) - \eta \triangledown E_w\]</span></p><h2 id="learning-rate-decay">Learning Rate Decay</h2><p>该方法是为了提高寻优能力，具体做法就是每次迭代的时候减少学习率的大小。</p><p>在训练模型的时候，通常会遇到这种情况：我们平衡模型的训练速度和损失（loss）后选择了相对合适的学习率（learningrate），但是训练集的损失下降到一定的程度后就不再下降了，比如trainingloss一直在0.8和0.9之间来回震荡，不能进一步下降。如下图所示：</p><p><imgsrc="https://pic4.zhimg.com/v2-c595435775e15abcf508c92f840821cd_1440w.jpg" /></p><p>遇到这种情况通常可以通过适当衰减学习率（learningrate）来实现。也就是让学习率随着训练的进行逐渐衰减，放缓在平缓区的学习，比如说线性衰减（如每过5个epochs学习率减半）或指数衰减（如每过5个epochs将学习率乘以0.9）</p><p>至于如何调节参数，可参考专业文献，如：</p><p><ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.09820.pdf">https://arxiv.org/pdf/1803.09820.pdf</a></p><p>a disciplined approach to neural network <ahref="https://zhida.zhihu.com/search?content_id=9732487&amp;content_type=Article&amp;match_order=1&amp;q=hyper-parameters&amp;zhida_source=entity">hyper-parameters</a>:part 1 – learning rate, batch size, momentum, and weight decay</p><p><ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1708.07120.pdf">https://arxiv.org/pdf/1708.07120.pdf</a></p><p>Super-Convergence: Very Fast Training of Neural Networks Using LargeLearning Rates</p><p>参考 【1】：<ahref="https://link.zhihu.com/?target=https%3A//inst.eecs.berkeley.edu/~cs182/sp06/notes/backprop.pdf">https://inst.eecs.berkeley.edu/~cs182/sp06/notes/backprop.pdf</a></p><p>【2】：<ahref="https://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/%23sgd">CS231nConvolutional Neural Networks for Visual Recognition</a></p><p>【3】：<ahref="https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/~fritz/absps/momentum.pdf">http://www.cs.toronto.edu/~fritz/absps/momentum.pdf</a></p><p>本文转自 <a href="https://zhuanlan.zhihu.com/p/48016051"class="uri">https://zhuanlan.zhihu.com/p/48016051</a>，如有侵权，请联系删除。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;优化算法超参数&quot;&gt;优化算法超参数&lt;/h1&gt;
&lt;p&gt;先看几个公式！ &lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;begin{equation} w_i &#92;leftarrow w_i-&#92;eta&#92;frac{&#92;partial E}{&#92;partial </summary>
      
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>损失函数</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</id>
    <published>2024-08-12T17:02:27.000Z</published>
    <updated>2025-02-09T04:46:49.327Z</updated>
    
    <content type="html"><![CDATA[<h1 id="损失函数">损失函数</h1><p>​ 在 深度学习 和机器学习领域，损失函数（LossFunction）是优化问题的核心，决定了模型参数的调整方向和幅度。尽管损失函数种类繁多，但理解其起源和背后的理论有助于我们更好地选择和应用它们。</p><h2 id="损失函数-的起源">损失函数 的起源</h2><p>所有的优化问题都需要确立一个目标函数，通过最小化（或最大化）该目标函数来求解。在机器学习中，损失函数衡量模型预测值与真实值之间的差异，是优化模型参数的重要工具。</p><h2 id="最小二乘法mse">最小二乘法（MSE）</h2><p>损失函数的起源可以追溯到统计学中的最小二乘回归。其基本思想是最小化预测值与真实值之间的差异。假设预测值为<span class="math inline">\(\hat{y}_{i}\)</span>，真实值为 <spanclass="math inline">\(y_i\)</span>，则最小二乘误差为： <spanclass="math display">\[\mathrm{MSE}=\frac{1}{n}\sum_{i = 1}^{n}(y_{i}-\hat{y}_{i})2\]</span> 通过最小化 MSE，可以找到使损失函数最小的参数 <spanclass="math inline">\(\theta\)</span>。</p><span id="more"></span><h2 id="最大似然估计mle">最大似然估计（MLE）</h2><p>​ 最大似然估计（Maximum Likelihood Estimation,MLE）是另一种基础且重要的参数估计方法，从概率分布的角度来理解<strong>目标函数</strong> 或<strong>损失函数</strong>。假设我们有一组独立的样本数据集 <spanclass="math inline">\(\{x_1, x_2, ...,x_m\}\)</span>，来自于未知的真实数据分布 <spanclass="math inline">\(P_{\text{data}}(x)\)</span>。我们假设另一个分布<span class="math inline">\(P_{\text{model}}(x|\theta)\)</span>来近似真实分布。</p><p>最大似然估计的目标是找到参数 <spanclass="math inline">\(\theta\)</span>，使得在给定数据的情况下，模型的似然函数最大化。即：<span class="math display">\[\hat{\theta}=\arg\max_{\theta}\prod_{i =1}^{m}P_{\mathrm{model}}(x_{i}|\theta)\]</span> 为了简化计算，我们通常使用对数似然： <spanclass="math display">\[\hat{\theta}=\arg\max_{\theta}\sum_{i = 1}^{m}\logP_{\mathrm{model}}(x_{i}|\theta)\]</span> 在假设数据符合 <strong>高斯分布</strong> 的情况下，MLE与最小化均方误差（MSE）等价。</p><h2 id="交叉熵损失">交叉熵损失</h2><p>交叉熵损失（<strong>Cross-Entropy</strong> Loss）是<strong>分类</strong>问题中常用的损失函数。假设数据符合伯努利分布或多项式分布，交叉熵损失用于衡量两个概率分布之间的差异。对于多分类任务，交叉熵损失(<strong>Cross-Entropy</strong> Loss) 定义为： <spanclass="math display">\[H \left (P, Q\right) = -\sum \limits_{i = 1}^n P \left (x_{i}\right) log\left(Q \left (x_{i}\right)\right)\]</span> 对于二分类问题，交叉熵损失 <strong>BCE</strong> (Binary CrossEntropy Loss) Loss 定义为： <span class="math display">\[L =-\frac{1}{m}\sum_{i =1}^{m}[y_{i}\log\hat{y}_{i}+(1-y_{i})\log(1-\hat{y}_{i})]\]</span>交叉熵损失从概率分布角度来看，本质上也是最大似然估计的一种形式。</p><h2 id="正则化与最大后验估计map">正则化与最大后验估计（MAP）</h2><p><strong>正则化</strong> 技术是解决 <strong>过拟合</strong>问题的重要手段措施。正则化可以理解为在损失函数中加入<strong>惩罚项</strong>，以<strong>限制模型的复杂度</strong>，从而提高模型的<strong>泛化能力</strong>。正则化可以视作最大后验估计（Maximum APosteriori Estimation, MAP）的特殊情况。</p><h3 id="l1-正则化lasso-回归">L1 正则化（Lasso 回归）</h3><p>L1 正则化 (<span class="math inline">\(l1-norm\)</span>)通过在损失函数中加入参数的绝对值和项来惩罚过大的参数。其目标函数为：</p><p><span class="math display">\[\text{L1 正则化} = \text{MSE} + \lambda \sum_{j = 1}^{p} |\theta_j|\]</span> L1正则化可以视为假设参数符合拉普拉斯分布时的最大后验估计。</p><h3 id="l2-正则化ridge-回归">L2 正则化（Ridge 回归）</h3><p>L2 正则化 (<span class="math inline">\(l2-norm\)</span>)通过在损失函数中加入 <strong>参数的平方和项</strong>来惩罚过大的参数。其目标函数为：</p><p><span class="math display">\[\text{L2 正则化} = \text{MSE} + \lambda \sum_{j = 1}^{p} \theta _{j}^{2}\]</span></p><p>其中，<span class="math inline">\(\lambda\)</span>是正则化参数，用于控制惩罚项的权重。<spanclass="math inline">\(L2\)</span>正则化可以视为假设参数符合高斯分布时的最大后验估计。</p><h3 id="最大后验估计map">最大后验估计（MAP）</h3><p>MAP 估计在 MLE 的基础上，考虑了参数的先验分布。其目标函数为： <spanclass="math display">\[\hat{\theta}=\arg\max_{\theta}P(\theta|X)\]</span> 利用贝叶斯定理可以展开为：</p><p><span class="math display">\[\hat{\theta}=\arg\max_\theta\left[\log P(X|\theta)+\log P(\theta)\right]\]</span> 前者是似然函数，后者是先验分布。通过对数变换和相加的方式，将<strong>最大化后验概率</strong> 的问题转化为<strong>最大化对数似然函数</strong> 与 <strong>对数先验分布之和</strong>的问题。</p><h2 id="贝叶斯估计bayesian-estimation">贝叶斯估计（BayesianEstimation）</h2><p>贝叶斯估计（BayesianEstimation）与频率学派的视角不同。贝叶斯学派认为数据是固定的，但参数是随机的，并且参数的估计应基于其全分布而不是点估计。</p><p>贝叶斯估计的核心在于求解后验分布： <span class="math display">\[P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}\]</span> 其中，<span class="math inline">\(P(X)\)</span>是证据（evidence），用于归一化。</p><p>在实际应用中，贝叶斯估计通常也会使用对数形式： <spanclass="math display">\[\log P(\theta|X) = \log P(X|\theta) + \log P(\theta) - \log P(X)\]</span>通过这种方式，我们可以更加灵活地处理不确定性，并且可以自然地引入先验信息。</p><h2 id="统一理解">统一理解</h2><p>​损失函数在深度学习中的应用广泛，虽然种类繁多，但从概率分布和参数估计的角度，我们可以将其统一起来理解。通过最大似然估计（MLE）、最大后验估计（MAP）和贝叶斯估计（BayesianEstimation），我们能够更系统地理解损失函数及其背后的统计学原理。</p><h3 id="回归问题">回归问题</h3><p>​回归问题中常用的是均方误差（MSE），其本质是最大似然估计在假设误差服从高斯分布下的特例。L2和 L1 正则化则分别对应参数服从高斯分布和拉普拉斯分布的最大后验估计。</p><h3 id="分类问题">分类问题</h3><p>​分类问题中常用的是交叉熵损失，其本质是最大似然估计在假设数据服从伯努利分布或多项分布下的特例。</p><h3 id="正则化">正则化</h3><p>​正则化可以视为在最大似然估计的基础上引入先验分布，从而转化为最大后验估计。L2正则化对应高斯分布的先验，L1 正则化对应拉普拉斯分布的先验。</p><h2 id="总结">总结</h2><p>通过从概率分布和参数估计的角度重新梳理损失函数的定义，我们可以更高效地理解和应用各种损失函数及其变体。最大似然估计、最大后验估计和贝叶斯估计提供了统一的框架，使我们能够更系统地看待损失函数及其在机器学习和深度学习中的应用。</p><p>希望这篇文章能帮助大家在学习和应用损失函数时，从更高的角度和更深的层次理解其精髓。随着对这些概念的深入理解，我们可以更灵活地选择和设计适合具体问题的损失函数，从而提升模型的性能和泛化能力。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;损失函数&quot;&gt;损失函数&lt;/h1&gt;
&lt;p&gt;​ 在 深度学习 和机器学习领域，损失函数（Loss
Function）是优化问题的核心，决定了模型参数的调整方向和幅度。尽管损失函数种类繁多，但理解其起源和背后的理论有助于我们更好地选择和应用它们。&lt;/p&gt;
&lt;h2 id=&quot;损失函数-的起源&quot;&gt;损失函数 的起源&lt;/h2&gt;
&lt;p&gt;所有的优化问题都需要确立一个目标函数，通过最小化（或最大化）该目标函数来求解。在
机器学习
中，损失函数衡量模型预测值与真实值之间的差异，是优化模型参数的重要工具。&lt;/p&gt;
&lt;h2 id=&quot;最小二乘法mse&quot;&gt;最小二乘法（MSE）&lt;/h2&gt;
&lt;p&gt;损失函数的起源可以追溯到统计学中的最小二乘回归。其基本思想是最小化预测值与真实值之间的差异。假设预测值为
&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;hat{y}_{i}&#92;)&lt;/span&gt;，真实值为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(y_i&#92;)&lt;/span&gt;，则最小二乘误差为： &lt;span
class=&quot;math display&quot;&gt;&#92;[
&#92;mathrm{MSE}=&#92;frac{1}{n}&#92;sum_{i = 1}^{n}(y_{i}-&#92;hat{y}_{i})2
&#92;]&lt;/span&gt; 通过最小化 MSE，可以找到使损失函数最小的参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;theta&#92;)&lt;/span&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="损失函数" scheme="https://luyicui.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>信息熵、交叉熵、相对熵（KL散度）</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%88KL%E6%95%A3%E5%BA%A6%EF%BC%89/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%88KL%E6%95%A3%E5%BA%A6%EF%BC%89/</id>
    <published>2024-08-12T17:02:27.000Z</published>
    <updated>2025-02-09T05:58:24.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息熵交叉熵相对熵kl-散度">信息熵、交叉熵、相对熵（KL散度）</h1><p>之前在代码中经常看见交叉熵损失函数(CrossEntropyLoss)，只知道它是分类问题中经常使用的一种损失函数，对于其内部的原理总是模模糊糊，而且一般使用交叉熵作为损失函数时，在模型的输出层总会接一个softmax函数，至于为什么要怎么做也是不懂，所以专门花了一些时间打算从原理入手，搞懂它，故在此写一篇博客进行总结，以便以后翻阅。</p><h2 id="交叉熵简介">交叉熵简介</h2><p>交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性，要理解交叉熵，需要先了解下面几个概念。</p><h2 id="信息量">信息量</h2><p>信息奠基人香农（Shannon）认为“信息是用来 <strong>消除</strong> 随机<strong>不确定性</strong>的东西”，也就是说衡量信息量的大小就是看这个信息消除不确定性的程度。</p><p><strong>“太阳从东边升起”</strong>，这条信息并没有减少不确定性，因为太阳肯定是从东边升起的，这是一句废话，信息量为0。</p><p><strong>”2018年中国队成功进入世界杯“</strong>，从直觉上来看，这句话具有很大的信息量。因为中国队进入世界杯的不确定性因素很大，而这句话消除了进入世界杯的不确定性，所以按照定义，这句话的信息量很大。</p><p>根据上述可总结如下：<strong>信息量的大小与信息发生的概率成反比</strong>。概率越大，信息量越小。概率越小，信息量越大。</p><p>设某一事件 <span class="math inline">\(x\)</span> 发生的概率为 <spanclass="math inline">\(P(x)\)</span>，其信息量表示为：<br /><span class="math display">\[I\left( x \right) =-\log \left( P\left( x \right) \right)\]</span> 其中 <span class="math inline">\(I\left ( x \right )\)</span>表示信息量，这里 <span class="math inline">\(\log\)</span> 表示以 <spanclass="math inline">\(e\)</span> 为底的自然对数。</p><span id="more"></span><h2 id="信息熵">信息熵</h2><p>信息熵也被称为熵，用来表示所有信息量的期望。</p><p>期望是试验中每次可能结果的概率乘以其结果的总和。</p><p>所以信息量的熵可表示为：（这里的 <spanclass="math inline">\(X\)</span> 是一个离散型随机变量）<br /><span class="math display">\[H\left( \mathbf{X} \right) =-\sum_{i=1}^n{P(x_i)\log \left( P\left( x_i\right) \right)}\qquad \left( \mathbf{X}=x_1,x_2,x_3...,x_n \right)\]</span> 使用明天的天气概率来计算其信息熵：</p><table><thead><tr class="header"><th style="text-align: center;">序号</th><th style="text-align: center;">事件</th><th style="text-align: center;">概率 P</th><th style="text-align: center;">信息量</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">明天是晴天</td><td style="text-align: center;">0.5</td><td style="text-align: center;"><span class="math inline">\(-\log \left( 0.5 \right )\)</span></td></tr><tr class="even"><td style="text-align: center;">2</td><td style="text-align: center;">明天出雨天</td><td style="text-align: center;">0.2</td><td style="text-align: center;"><span class="math inline">\(-\log \left( 0.2 \right )\)</span></td></tr><tr class="odd"><td style="text-align: center;">3</td><td style="text-align: center;">多云</td><td style="text-align: center;">0.3</td><td style="text-align: center;"><span class="math inline">\(-\log \left( 0.3 \right )\)</span></td></tr></tbody></table><p><span class="math display">\[H\left ( \mathbf{X} \right ) = -\left ( 0.5 * \log \left ( 0.5 \right )+ 0.2 * \log \left ( 0.2 \right ) + 0.3 * \log \left ( 0.3 \right )\right)\]</span></p><p>对于 0-1 分布的问题，由于其结果只用两种情况，是或不是，设某一件事情<span class="math inline">\(x\)</span> 发生的概率为 <spanclass="math inline">\(P\left ( x \right )\)</span>，则另一件事情发生的概率为 <span class="math inline">\(1-P\left ( x\right )\)</span> ，所以对于 0-1分布的问题，计算熵的公式可以简化如下：<br /><span class="math display">\[H\left( \mathbf{X} \right) =-\sum_{i=1}^n{P(x_i)\log \left( P\left( x_i\right) \right)}\\=-\left[ P\left( x \right) \log \left( P\left( x \right) \right) +\left(1-P\left( x \right) \right) \log \left( 1-P\left( x \right) \right)\right]\\=-P\left( x \right) \log \left( P\left( x \right) \right) -\left(1-P\left( x \right) \right) \log \left( 1-P\left( x \right) \right)\]</span></p><h2 id="相对熵kl-散度">相对熵（KL 散度）</h2><p>如果对于同一个随机变量 <span class="math inline">\(X\)</span>有两个单独的概率分布 <spanclass="math inline">\(P\left(x\right)\)</span> 和 <spanclass="math inline">\(Q\left(x\right)\)</span> ，则我们可以使用 KL散度来衡量这 <strong>两个概率分布之间的差异</strong>。</p><p>我们设定两个概率分布分别为 <span class="math inline">\(P\)</span> 和<spanclass="math inline">\(Q\)</span>，在设定为连续随机变量的前提下，他们对应的概率密度函数分别为<span class="math inline">\(P(x)\)</span> 和 <spanclass="math inline">\(Q(x)\)</span>。如果我们用 <spanclass="math inline">\(Q(x)\)</span> 去近似 <spanclass="math inline">\(P(x)\)</span>，则KL散度可以表示为：</p><p><span class="math display">\[KL(P||Q) = \int P(x)\log \frac{P(x)}{Q(x)}dx\]</span></p><p>从上面的公式可以看出</p><ul><li>当且仅当 <span class="math inline">\(P=Q\)</span> 时，<spanclass="math inline">\(KL(P||Q) = 0\)</span></li><li>KL 散度具备<strong>非负性</strong>，即 <spanclass="math inline">\(KL(P||Q) &gt;= 0\)</span></li><li>KL散度<strong>不具备对称性</strong>，也就是说 <spanclass="math inline">\(P\)</span> 对于 <spanclass="math inline">\(Q\)</span> 的 KL 散度并不等于 <spanclass="math inline">\(Q\)</span> 对于 <spanclass="math inline">\(P\)</span> 的 KL 散度。因此，<strong>KL散度并不是一个度量（metric），即 KL 散度并非距离</strong>。</li></ul><p>我们再来看看离散的情况下用 <span class="math inline">\(Q(x)\)</span>去近似 <span class="math inline">\(P(x)\)</span> 的 KL 散度的公式：<span class="math display">\[D_{KL}\left ( P || Q \right) = \sum \limits_{i = 1}^n P\left (x_{i}\right ) \log \left ( \frac{P\left ( x_{i} \right )}{Q\left ( x_{i}\right )} \right )\]</span></p><p><strong>在机器学习中，常常使用 <spanclass="math inline">\(P\left(x\right)\)</span>来表示样本的真实分布，<span class="math inline">\(Q\left(x\right)\)</span>来表示模型所预测的分布</strong>，比如在一个三分类任务中（例如，猫狗马分类器），<spanclass="math inline">\(x_{1}, x_{2}, x_{3}\)</span>​分别代表猫，狗，马，例如一张猫的图片真实分布 <spanclass="math inline">\(P\left(X\right) = [1, 0, 0]\)</span> , 预测分布<span class="math inline">\(Q\left(X\right) = [0.7, 0.2, 0.1]\)</span>，计算 KL 散度： <span class="math display">\[D_{KL}\left( P||Q \right) =\sum_{i=1}^n{P\left( x_i \right) \log \left(\frac{P\left( x_i \right)}{Q\left( x_i \right)} \right)}\\=P\left( x_1 \right) \log \left( \frac{P\left( x_1 \right)}{Q\left( x_1\right)} \right) +P\left( x_2 \right) \log \left( \frac{P\left( x_2\right)}{Q\left( x_2 \right)} \right) +P\left( x_3 \right) \log \left(\frac{P\left( x_3 \right)}{Q\left( x_3 \right)} \right)\\=1*\log \left( \frac{1}{0.7} \right) =0.36\]</span></p><p><strong>KL 散度越小，表示 <spanclass="math inline">\(P\left(x\right)\)</span> 与 <spanclass="math inline">\(Q\left(x\right)\)</span>的分布更加接近</strong>，可以通过反复训练 <spanclass="math inline">\(Q\left(x \right)\)</span> 来使 <spanclass="math inline">\(Q\left(x \right)\)</span> 的分布逼近 <spanclass="math inline">\(P\left(x \right)\)</span>。</p><h2 id="交叉熵">交叉熵</h2><p>首先将 KL 散度公式拆开：<br /><span class="math display">\[D_{KL}\left( P || Q \right) = \sum \limits_{i = 1}^n P\left( x_{i}\right) \log \left( \frac{P\left( x_{i} \right)}{Q\left( x_{i} \right)}\right)\\= \sum \limits_{i = 1}^n P\left( x_{i} \right) \log \left( P\left(x_{i} \right) \right) - \sum \limits_{i = 1}^n P\left( x_{i} \right)\log \left( Q\left( x_{i} \right) \right)\\= -H\left( P \right) + \left [ - \sum \limits_{i = 1}^n P\left( x_{i}\right) \log \left( Q\left( x_{i} \right) \right)\right]\]</span> 前者 <span class="math inline">\(H \left (P \left (x\right)\right)\)</span> 表示信息熵，后者即为交叉熵，<strong>KL 散度 =交叉熵 - 信息熵</strong></p><p>交叉熵公式表示为：<br /><span class="math display">\[H \left (P, Q\right) = -\sum \limits_{i = 1}^n P \left (x_{i}\right)\log \left(Q \left (x_{i}\right)\right)\]</span></p><p>在机器学习训练网络时，输入数据与标签常常已经确定，那么真实概率分布<span class="math inline">\(P\left(x \right)\)</span>也就确定下来了，所以信息熵在这里就是一个常量。由于 KL散度的值表示真实概率分布 <spanclass="math inline">\(P\left(x\right)\)</span> 与预测概率分布 <spanclass="math inline">\(Q \left(x\right)\)</span>之间的差异，值越小表示预测的结果越好，所以需要最小化 KL散度，而交叉熵等于 KL 散度加上一个常量（信息熵），且公式相比 KL散度更加容易计算，所以在机器学习中常常使用交叉熵损失函数来计算 loss就行了。</p><h2 id="交叉熵在单分类问题中的应用">交叉熵在单分类问题中的应用</h2><p>在线性回归问题中，常常使用 MSE(Mean Squared Error)作为 loss函数，而在分类问题中常常使用交叉熵作为 loss 函数。</p><p>下面通过一个例子来说明如何计算交叉熵损失值。假设我们输入一张狗的图片，标签与预测值如下：</p><table><thead><tr class="header"><th>*</th><th>猫</th><th>狗</th><th>马</th></tr></thead><tbody><tr class="odd"><td>Label</td><td>0</td><td>1</td><td>0</td></tr><tr class="even"><td>Pred</td><td>0.2</td><td>0.7</td><td>0.1</td></tr></tbody></table><p>那么 loss<br /><span class="math display">\[loss = -\left ( 0 * \log \left ( 0.2 \right ) + 1 * \log \left ( 0.7\right ) + 0 * \log \left ( 0.1 \right )\right) = 0.36\]</span></p><p>一个 batch 的 loss 为<br /><span class="math display">\[loss = -\frac{1}{m}\sum \limits_{i = 1}^m \sum \limits_{j = 1}^n P \left(x_{ij}\right) \log \left(Q \left (x_{ij}\right)\right)\]</span></p><p>其中 <span class="math inline">\(m\)</span> 表示样本个数。</p><h2 id="总结">总结</h2><ul><li><p>交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小，模型预测效果就越好。</p></li><li><p>交叉熵在分类问题中常常与 softmax 是标配，softmax将输出的结果进行处理，使其多个分类的预测值和为1，再通过交叉熵来计算损失。</p></li></ul><h2 id="参考">参考</h2><p><em>https://blog.csdn.net/tsyccnh/article/details/79163834</em></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;信息熵交叉熵相对熵kl-散度&quot;&gt;信息熵、交叉熵、相对熵（KL
散度）&lt;/h1&gt;
&lt;p&gt;之前在代码中经常看见交叉熵损失函数(CrossEntropy
Loss)，只知道它是分类问题中经常使用的一种损失函数，对于其内部的原理总是模模糊糊，而且一般使用交叉熵作为损失函数时，在模型的输出层总会接一个
softmax
函数，至于为什么要怎么做也是不懂，所以专门花了一些时间打算从原理入手，搞懂它，故在此写一篇博客进行总结，以便以后翻阅。&lt;/p&gt;
&lt;h2 id=&quot;交叉熵简介&quot;&gt;交叉熵简介&lt;/h2&gt;
&lt;p&gt;交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性，要理解交叉熵，需要先了解下面几个概念。&lt;/p&gt;
&lt;h2 id=&quot;信息量&quot;&gt;信息量&lt;/h2&gt;
&lt;p&gt;信息奠基人香农（Shannon）认为“信息是用来 &lt;strong&gt;消除&lt;/strong&gt; 随机
&lt;strong&gt;不确定性&lt;/strong&gt;
的东西”，也就是说衡量信息量的大小就是看这个信息消除不确定性的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“太阳从东边升起”&lt;/strong&gt;，这条信息并没有减少不确定性，因为太阳肯定是从东边升起的，这是一句废话，信息量为
0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;”2018
年中国队成功进入世界杯“&lt;/strong&gt;，从直觉上来看，这句话具有很大的信息量。因为中国队进入世界杯的不确定性因素很大，而这句话消除了进入世界杯的不确定性，所以按照定义，这句话的信息量很大。&lt;/p&gt;
&lt;p&gt;根据上述可总结如下：&lt;strong&gt;信息量的大小与信息发生的概率成反比&lt;/strong&gt;。概率越大，信息量越小。概率越小，信息量越大。&lt;/p&gt;
&lt;p&gt;设某一事件 &lt;span class=&quot;math inline&quot;&gt;&#92;(x&#92;)&lt;/span&gt; 发生的概率为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(P(x)&#92;)&lt;/span&gt;，其信息量表示为：&lt;br /&gt;
&lt;span class=&quot;math display&quot;&gt;&#92;[
I&#92;left( x &#92;right) =-&#92;log &#92;left( P&#92;left( x &#92;right) &#92;right)
&#92;]&lt;/span&gt; 其中 &lt;span class=&quot;math inline&quot;&gt;&#92;(I&#92;left ( x &#92;right )&#92;)&lt;/span&gt;
表示信息量，这里 &lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;log&#92;)&lt;/span&gt; 表示以 &lt;span
class=&quot;math inline&quot;&gt;&#92;(e&#92;)&lt;/span&gt; 为底的自然对数。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://luyicui.github.io/categories/AI/"/>
    
    
    <category term="损失函数" scheme="https://luyicui.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>scanf()函数与printf()中的格式说明符</title>
    <link href="https://luyicui.github.io/2024/08/11/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/C++/%E3%80%90%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E3%80%91/"/>
    <id>https://luyicui.github.io/2024/08/11/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/C++/%E3%80%90%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E3%80%91/</id>
    <published>2024-08-11T09:29:49.000Z</published>
    <updated>2024-09-25T04:42:57.810Z</updated>
    
    <content type="html"><![CDATA[<h1id="scanf函数与printf中的格式说明符">scanf()函数与printf()中的格式说明符</h1><h2id="scanf与printf中格式说明符几乎相同的一部分">scanf（）与printf（）中格式说明符几乎相同的一部分</h2><table border="1" cellpadding="1" cellspacing="1" style="width: 600px; text-align: central;"><tbody><tr><td>%d</td><td>用来输入和输出int</td></tr><tr><td>%ld</td><td>用来输入和输出long</td></tr><tr><td>%lld</td><td>用来输入和输出long long</td></tr><tr><td>%hd</td><td>用来输入和输出short</td></tr><tr><td>%i</td><td>用来输入和输出有符号十进制整数</td></tr><tr><td>%u</td><td>用来输入和输出无符号十进制整数</td></tr><tr><td>%lu</td><td>用来输入和输出无符号十进制长整数</td></tr><tr><td>%llu</td><td>用来输入和输出无符号十进制长长整数</td></tr><tr><td>%hu</td><td>用来输入和输出无符号短十进制整数</td></tr><tr><td>%o</td><td>用来输入和输出八进制整数</td></tr><tr><td>%lo</td><td>用来输入和输出长八进制整数</td></tr><tr><td>%ho</td><td>用来输入和输出短八进制整数</td></tr><tr><td>%#o</td><td>用来输出八进制整数，数字前有0</td></tr><tr><td>%x</td><td>用来输入和输出十六制整数，字母小写</td></tr><tr><td>%#x</td><td>用来输出十六制整数，字母小写，数字前有0x</td></tr><tr><td>%lx</td><td>用来输入和输出长十六制整数，字母小写</td></tr><tr><td>%X</td><td>用来输入和输出十六制整数，字母大写 </td></tr><tr><td>%#X</td><td>用来输出十六制整数，字母大写 ，数字前有0X</td></tr><tr><td>%lX</td><td>用来输入和输出长十六制整数，字母大写</td></tr><tr><td>%c</td><td>用来输入和输出单个字符</td></tr><tr><td>%s</td><td><p>用来输入和输出一串字符串</p><p>输入时遇空格，<a            href="https://so.csdn.net/so/search?q=%E5%88%B6%E8%A1%A8%E7%AC%A6&amp;spm=1001.2101.3001.7020"            target="_blank"            class="hl hl-1"            data-report-click='{"spm":"1001.2101.3001.7020","dest":"https://so.csdn.net/so/search?q=%E5%88%B6%E8%A1%A8%E7%AC%A6&amp;spm=1001.2101.3001.7020","extra":"{\"searchword\":\"制表符\"}"}'            data-tit="制表符"            data-pretit="制表符"            >制表符</a          >或换行符结束</p><p>输出时连格式说明符一起输出</p><p>printf（"%s","%d%f",a,b）;输出 %d%f</p></td></tr><tr><td>%f</td><td>用来输入和输出float，输出double</td></tr><tr><td>%lf</td><td>用来输入和输出double（double输出用%f和%lf都可以）</td></tr><tr><td>%Lf</td><td>用来输入和输出long double</td></tr><tr><td>%e</td><td>用来输入和输出指数，字母小写</td></tr><tr><td>%le</td><td>用来输入和输出长指数，字母小写</td></tr><tr><td>%E</td><td>用来输入和输出指数，字母大写</td></tr><tr><td>%lE</td><td>用来输入和输出长指数，字母大写</td></tr><tr><td>%g</td><td>用来输入和输出指数或float（输出最短的一种），字母小写</td></tr><tr><td>%lg</td><td>用来输入和输出长指数或double（输出最短的一种），字母小写</td></tr><tr><td>%G</td><td>用来输入和输出指数或float（输出最短的一种），字母大写</td></tr><tr><td>%lG</td><td>用来输入和输出长指数或double（输出最短的一种），字母大写</td></tr></tbody></table><span id="more"></span><h2 id="scanf独有">scanf()独有</h2><table border="1" cellpadding="1" cellspacing="1" style="width: 600px"><tbody><tr><td>%<em>（所有类型），如%</em>d</td><td><p>用来输入一个数，字符或字符串而不赋值（跳过无关输入）</p><p>如scanf("%d%*c%d",&amp;a,&amp;b);</p><p>这样就可以只将1+2中的1和2赋值给a和b。</p></td></tr><tr><td><p>%m（所有类型），其中m为常数</p></td><td>限定输入范围，如scanf（“%4d”，&amp;a）时输入123456，只把1234赋值给a</td></tr><tr><td>，(逗号）</td><td>无实际用处，仅用于美观。如scanf（“%d,%d,%d”,&amp;a,&amp;b,&amp;c）;</td></tr><tr><td>-（横杠）    ：（冒号）</td><td><p>方便日期等输入，但不赋值</p><p>scanf（“%d-%d-%d”,&amp;a,&amp;b,&amp;c）;需输入2018-11-20</p><p>scanf（“%d：%d：%d”：&amp;a,&amp;b,&amp;c）;需输入2018:11:20</p></td></tr><tr><td><p>所有字符串，符号（包括空格）</p><p>数字（不与输入数相挨）</p></td><td><p>任何所写的东西都必须如横杠一般先输入（不赋值），不然系统报错</p><p>scanf（“%d 456 %d”,&amp;a,&amp;b）;</p><p>需输入  1 456 7（1和7之间有 456 （前后各一个空格））</p><p>结果为a=1  b=7</p></td></tr></tbody></table><h2 id="printf独有">printf()独有</h2><table border="1" cellpadding="1" cellspacing="1" style="width: 600px"><tbody><tr><td><p>%m.nd     %-m.nd</p><p>（m和n为常数）</p></td><td><p>m用于在d位数小于m时补空格（<a            href="https://so.csdn.net/so/search?q=%E5%8F%B3%E5%AF%B9%E9%BD%90&amp;spm=1001.2101.3001.7020"            target="_blank"            class="hl hl-1"            data-report-click='{"spm":"1001.2101.3001.7020","dest":"https://so.csdn.net/so/search?q=%E5%8F%B3%E5%AF%B9%E9%BD%90&amp;spm=1001.2101.3001.7020","extra":"{\"searchword\":\"右对齐\"}"}'            data-tit="右对齐"            data-pretit="右对齐"            >右对齐</a          >）d位数大于m时忽略</p><p>如%5d，输出123，          <u            ><span style="color: #3399ea">  </span></u          >123（123前面两个空格）</p><p></p><p>.n用于在d位数小于n时补0（右对齐）d位数大于n时忽略</p><p>如%.5d，输出123，          00123（123前面两个0）</p><p></p><p>%-m.nd则为左对齐</p></td></tr><tr><td><p>%m.nf  %m.nlf  %m.nLf</p><p>%-m.nf %-m.nlf %-m.nLf</p></td><td><p>m用于在小数位数小于m时补空格（右对齐）</p><p>小数位数大于m时忽略      (小数点算一位）</p><p>如%6f  需输出3.14            结果为<u>  </u>3.14（3.14前面两个空格）</p><p></p><p>.n用于控制小数位数      小数部分长度大于n则四舍五入</p><p>小数部分长度小于于n则补0</p><p>如%.6f  需输出3.14           结果为3.140000</p><p>如%6f  需输出3.1415926  结果为3.141593</p><p></p><p>%-m.nf %-m.nlf %-m.nLf   则为左对齐</p></td></tr><tr><td>%m.ns     %-m.ns</td><td><p>m用于在字符串位数小于m时补空格（右对齐）字符串位数大于m时忽略</p><p>如%5s，输出abc，          <u            ><span style="color: #3399ea">  </span></u          >abc（abc前面两个空格）</p><p></p><p>.n用于控制字符串位数      长度大于n则仅输出前n位</p><p>字符串长度小于于n时忽略</p><p>如%.6s 需输出abcdefg      结果为abcdef</p><p>如%6f  需输出abc              结果为abc</p><p></p><p>%-m.ns则为左对齐</p></td></tr><tr><td>%mc        %-mc</td><td><p>m限制char的输出长度       当m&gt;1时，在左方补m-1个空格</p><p></p><p>%-mc则为左对齐</p></td></tr><tr><td><p>%m.ne     %-m.ne</p><p>%m.nE     %-m.nE</p></td><td><p>m用于控制指数长度，在QT中，指数部分占五位（如 e+001 ）</p><p>位数小于m时左方补空格   位数大于m时忽略</p><p>如printf("%15.5e",a);          设a为123.456789</p><p>结果为 <u>   </u>1.23457e+002（三个空格）</p><p></p><p>.n用于控制小数长度           小数部分长度大于n则四舍五入</p><p>小数部分长度小于于n则补0</p><p>如printf("%15.5e",a);          设a为123.456789</p><p>结果为 <u>   </u>1.23457e+002（三个空格）</p><p></p><p>%-m.ne   %-m.nE则为左对齐</p></td></tr><tr><td><p>%<em>（整型）如%</em>d</p><p>%-*（整型）</p></td><td><p>在输出项中规定整型数据的宽度，少于限制补空格，大于忽略</p><p>如printf（“%*d”,a,b）;</p><p>a=5  b=123</p><p>结果为 <u>  </u>123（前面有两个空格）</p><p></p><p>%-*（整型） 左对齐</p></td></tr><tr><td>%0*（整型）</td><td><p>在输出项中规定整型数据的宽度，少于限制补0，大于忽略</p><p>如printf（“%0*d”,a,b）;</p><p>a=5  b=123</p><p>结果为 00123</p></td></tr><tr><td>注意</td><td><p>printf（）中的运算是从右至左，而输出是从做左至右</p><p>如a=1，printf（“%d %d %d”,a++,a++,a++）;</p><p>结果为3 2 1</p></td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;h1
id=&quot;scanf函数与printf中的格式说明符&quot;&gt;scanf()函数与printf()中的格式说明符&lt;/h1&gt;
&lt;h2
id=&quot;scanf与printf中格式说明符几乎相同的一部分&quot;&gt;scanf（）与printf（）中格式说明符几乎相同的一部分&lt;/h2&gt;
&lt;table border=&quot;1&quot; cellpadding=&quot;1&quot; cellspacing=&quot;1&quot; style=&quot;width: 600px; text-align: central;&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
%d
&lt;/td&gt;
&lt;td&gt;
用来输入和输出int
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%ld
&lt;/td&gt;
&lt;td&gt;
用来输入和输出long
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lld
&lt;/td&gt;
&lt;td&gt;
用来输入和输出long long
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%hd
&lt;/td&gt;
&lt;td&gt;
用来输入和输出short
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%i
&lt;/td&gt;
&lt;td&gt;
用来输入和输出有符号十进制整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%u
&lt;/td&gt;
&lt;td&gt;
用来输入和输出无符号十进制整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lu
&lt;/td&gt;
&lt;td&gt;
用来输入和输出无符号十进制长整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%llu
&lt;/td&gt;
&lt;td&gt;
用来输入和输出无符号十进制长长整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%hu
&lt;/td&gt;
&lt;td&gt;
用来输入和输出无符号短十进制整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%o
&lt;/td&gt;
&lt;td&gt;
用来输入和输出八进制整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lo
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长八进制整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%ho
&lt;/td&gt;
&lt;td&gt;
用来输入和输出短八进制整数
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%#o
&lt;/td&gt;
&lt;td&gt;
用来输出八进制整数，数字前有0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%x
&lt;/td&gt;
&lt;td&gt;
用来输入和输出十六制整数，字母小写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%#x
&lt;/td&gt;
&lt;td&gt;
用来输出十六制整数，字母小写，数字前有0x
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lx
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长十六制整数，字母小写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%X
&lt;/td&gt;
&lt;td&gt;
用来输入和输出十六制整数，字母大写 
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%#X
&lt;/td&gt;
&lt;td&gt;
用来输出十六制整数，字母大写 ，数字前有0X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lX
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长十六制整数，字母大写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%c
&lt;/td&gt;
&lt;td&gt;
用来输入和输出单个字符
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%s
&lt;/td&gt;
&lt;td&gt;
&lt;p&gt;
用来输入和输出一串字符串
&lt;/p&gt;
&lt;p&gt;
输入时遇空格，&lt;a
            href=&quot;https://so.csdn.net/so/search?q=%E5%88%B6%E8%A1%A8%E7%AC%A6&amp;amp;spm=1001.2101.3001.7020&quot;
            target=&quot;_blank&quot;
            class=&quot;hl hl-1&quot;
            data-report-click=&#39;{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E5%88%B6%E8%A1%A8%E7%AC%A6&amp;amp;spm=1001.2101.3001.7020&quot;,&quot;extra&quot;:&quot;{&#92;&quot;searchword&#92;&quot;:&#92;&quot;制表符&#92;&quot;}&quot;}&#39;
            data-tit=&quot;制表符&quot;
            data-pretit=&quot;制表符&quot;
            &gt;制表符&lt;/a
          &gt;或换行符结束
&lt;/p&gt;
&lt;p&gt;
输出时连格式说明符一起输出
&lt;/p&gt;
&lt;p&gt;
printf（&quot;%s&quot;,&quot;%d%f&quot;,a,b）;输出 %d%f
&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%f
&lt;/td&gt;
&lt;td&gt;
用来输入和输出float，输出double
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lf
&lt;/td&gt;
&lt;td&gt;
用来输入和输出double（double输出用%f和%lf都可以）
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%Lf
&lt;/td&gt;
&lt;td&gt;
用来输入和输出long double
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%e
&lt;/td&gt;
&lt;td&gt;
用来输入和输出指数，字母小写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%le
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长指数，字母小写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%E
&lt;/td&gt;
&lt;td&gt;
用来输入和输出指数，字母大写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lE
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长指数，字母大写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%g
&lt;/td&gt;
&lt;td&gt;
用来输入和输出指数或float（输出最短的一种），字母小写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lg
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长指数或double（输出最短的一种），字母小写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%G
&lt;/td&gt;
&lt;td&gt;
用来输入和输出指数或float（输出最短的一种），字母大写
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
%lG
&lt;/td&gt;
&lt;td&gt;
用来输入和输出长指数或double（输出最短的一种），字母大写
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</summary>
    
    
    
    
    <category term="C++" scheme="https://luyicui.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>REST API</title>
    <link href="https://luyicui.github.io/2024/05/19/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/REST%20API/"/>
    <id>https://luyicui.github.io/2024/05/19/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/REST%20API/</id>
    <published>2024-05-19T01:46:27.000Z</published>
    <updated>2025-02-04T15:35:22.850Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rest-api">REST API</h1><p>这是一个开发中经常会遇到的名词，也许你已经了解过一些这方面的知识，也有可能对这个概念比较模糊。本文将会诠释REST的基础以及如何给应用创建一个API，我们开始正文。</p><h2 id="什么是api">什么是API？</h2><p>首先介绍API的概念，Application Programming Interface（<ahref="https://zhida.zhihu.com/search?content_id=207341858&amp;content_type=Article&amp;match_order=1&amp;q=%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3&amp;zhida_source=entity">应用程序接口</a>）是它的全称。简单的理解就是，API是一个接口。那么它是一个怎样的接口呢，现在我们常将它看成一个HTTP接口即HTTPAPI。也就是说这个接口得通过HTTP的方式来调用，做过前后端开发的小伙伴可能知道，后端开发又叫做<ahref="https://zhida.zhihu.com/search?content_id=207341858&amp;content_type=Article&amp;match_order=1&amp;q=%E9%9D%A2%E5%90%91%E6%8E%A5%E5%8F%A3%E5%BC%80%E5%8F%91&amp;zhida_source=entity">面向接口开发</a>，我们往往会提供一个接口供前端调用，或者供其他服务调用。举个例子，我们程序中往往会涉及到调用第三方接口，比如说，调用支付宝或者微信的支付接口来实现我们程序中的支付功能、调用带三方的短信接口来向用户发送验证码短信等等...</p><p>这样说吧，比如说我们有一个可以允许我们查看（view），创建（create），编辑（edit）以及删除（delete）部件的应用程序。我们可以创建一个可以让我们执行这些功能的HTTPAPI:</p><blockquote><p>http://demo.com/view_bookshttp://demo.com/create_new_book?name=shuxuehttp://demo.com/update_book?id=1&amp;name=shuxuehttp://demo.com/delete_book?id=1</p></blockquote><p>这是4个HTTPAPI，分别实现了图书的查看、新增、编辑、删除的操作，当我们把接口发布出去的时候，别人就可以通过这四个接口来调用相关的服务了。但是这样做有什么不方便的地方呢？你可能发现了，这种API的写法有一个缺点，那就是没有一个统一的风格，比如说第一个接口表示查询全部图书的信息，我们也可以写成这样：</p><blockquote><p>http://demo.com/books/list</p></blockquote><p>那这样就会造成使用我们接口的其他人必须得参考API才能知道它是怎么运作的。</p><p>不用担心，REST会帮我们解决这个问题。</p><h2 id="什么是rest">什么是REST？</h2><p>有了上面的介绍，你可能也大概有了直观的了解，说白了，<strong>REST是一种风格！</strong></p><p>REST的作用是将我们上面提到的查看（view），创建（create），编辑（edit）和删除（delete）直接<strong>映射</strong>到HTTP中已实现的GET、POST、PUT和DELETE方法。</p><p>这四种方法是比较常用的，HTTP总共包含<strong>八种</strong>方法：</p><blockquote><p>GET POST PUT DELETE OPTIONS HEAD TRACE CONNECT</p></blockquote><p>当我们在浏览器点点点的时候我们通常只用到了GET方法，当我们提交表单，例如注册用户的时候我们就用到了POST方法...</p><p>介绍到这里，我们重新将上面的四个接口改写成REST风格：</p><p><strong>查看所有图书：</strong></p><blockquote><p>GET http://demo.com/books</p></blockquote><p><strong>新增一本书：</strong></p><blockquote><p>POST http://demo.com/books Data: name=shuxue</p></blockquote><p><strong>修改一本书：</strong></p><blockquote><p>PUT http://demo.com/books Data:id=1,name=shuxue</p></blockquote><p><strong>删除一本书：</strong></p><blockquote><p>DELETE http://demo.com/books Data:id=1</p></blockquote><p>大家有没有发现，这样改动之后API变得统一了，我们只需要改变请求方式就可以完成相关的操作，这样大大简化了我们接口的理解难度，变得易于调用。</p><p><strong>这就是REST风格的意义！</strong></p><p><strong>HTTP状态码</strong></p><p>REST的另一重要部分就是为既定好请求的类型来响应正确的状态码。如果你对<strong>HTTP状态码</strong>陌生，以下是一个简易总结。当你请求HTTP时，服务器会响应一个状态码来判断你的请求是否成功，然后客户端应如何继续。以下是四种不同层次的状态码：</p><ul><li>2xx = Success（成功）</li><li>3xx = Redirect（重定向）</li><li>4xx = User error（客户端错误）</li><li>5xx = Server error（服务器端错误）</li></ul><p>我们常见的是200（请求成功）、404（未找到）、401（未授权）、500（服务器错误）...</p><p><strong>API格式响应</strong></p><p>上面介绍了RESTAPI的写法，响应状态码，剩下就是请求的数据格式以及响应的数据格式。说的通俗点就是，我们用什么格式的参数去请求接口并且我们能得到什么格式的响应结果。</p><p>我这里只介绍一种用的最多的格式——JSON格式</p><p>目前json已经发展成了一种最常用的数据格式，由于其轻量、易读的优点。</p><p>所以我们经常会看到一个请求的header信息中有这样的参数：</p><blockquote><p>Accept:application/json</p></blockquote><p>这个参数的意思就是接收来自后端的json格式的信息。</p><p>我举个json响应的例子：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;code&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;books&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;yuwen&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;shuxue&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>这样返回是不是一目了然，而且冗余信息很少！</p><h2 id="rest-api例子"><strong>REST API例子</strong></h2><p>说了这么多，最后我以一个实际RESTAPI的例子结尾（这里以java语言为例）</p><p>我们新建一个SpringBoot的项目demo</p><p><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202502041525162.jpeg" /></p><p>然后写上查看、新增、修改、删除四个接口（这里为了方便返回的格式我用字符串代替json格式）</p><p><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202502041525214.jpeg" /></p><p>最后通过Apifox工具对四个接口依次进行测试。如下图。</p><p><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202502041525183.jpeg" /></p><p><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202502041525175.jpeg" /></p><p><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202502041525192.jpeg" /></p><p><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202502041525170.jpeg" /></p><p>问：REST API是什么呢？</p><p>答：是一种REST风格的HTTP接口！</p><hr /><p>补充一个重要的知识点：一般都不直接用 put 和delete，存在安全漏洞，常用的就是post来替代实现 put、delete 的方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;rest-api&quot;&gt;REST API&lt;/h1&gt;
&lt;p&gt;这是一个开发中经常会遇到的名词，也许你已经了解过一些这方面的知识，也有可能对这个概念比较模糊。本文将会诠释REST的基础以及如何给应用创建一个API，我们开始正文。&lt;/p&gt;
&lt;h2 id=&quot;什么是api&quot;&gt;</summary>
      
    
    
    
    <category term="前端" scheme="https://luyicui.github.io/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="前端" scheme="https://luyicui.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>CRC</title>
    <link href="https://luyicui.github.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/CRC/"/>
    <id>https://luyicui.github.io/2024/05/19/[object%20Object]/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/CRC/</id>
    <published>2024-05-19T01:46:27.000Z</published>
    <updated>2025-01-18T16:31:48.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="crc">CRC</h1><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656975.jpg"alt="258a1611faa096e4f1367a79335fb90" /><figcaptionaria-hidden="true">258a1611faa096e4f1367a79335fb90</figcaption></figure><span id="more"></span><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656977.jpg" alt="3b36f8ff943c741c889d81022a745df" style="zoom:67%;" /></p><blockquote><ol type="1"><li>生成多项式==&gt;除数</li><li>除数为n位<ul><li>余数为n-1位<strong><font color='red'>(不够的在前面补0)</font></strong></li><li>被除数后面加上<strong><font color='red'>n-1位‘0’</font></strong></li></ul></li></ol></blockquote><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656978.jpg"alt="05c133fba1b7e11497708c5df82366d" /><figcaptionaria-hidden="true">05c133fba1b7e11497708c5df82366d</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;crc&quot;&gt;CRC&lt;/h1&gt;
&lt;figure&gt;
&lt;img
src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202405191656975.jpg&quot;
alt=&quot;258a1611faa096e4f1367a79335fb90&quot; /&gt;
&lt;figcaption
aria-hidden=&quot;true&quot;&gt;258a1611faa096e4f1367a79335fb90&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Courses" scheme="https://luyicui.github.io/categories/Courses/"/>
    
    
    <category term="计算机网络" scheme="https://luyicui.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>前端Tutorial</title>
    <link href="https://luyicui.github.io/2024/05/19/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/%E5%89%8D%E7%AB%AFTutorial/"/>
    <id>https://luyicui.github.io/2024/05/19/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/%E5%89%8D%E7%AB%AFTutorial/</id>
    <published>2024-05-19T01:46:27.000Z</published>
    <updated>2025-02-04T07:37:57.060Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前端tutorial">前端Tutorial</h1><ul><li><a href="https://developer.mozilla.org/zh-CN/">MDN Web Docs</a></li></ul><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202412061056151.png"alt="image-20241206105627960" /><figcaption aria-hidden="true">image-20241206105627960</figcaption></figure><ul><li><a href="https://www.runoob.com/">菜鸟教程 -学的不仅是技术，更是梦想！</a></li></ul><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202412061054007.png"alt="image-20241206105434913" /><figcaption aria-hidden="true">image-20241206105434913</figcaption></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202412061055814.png"alt="image-20241206105508651" /><figcaption aria-hidden="true">image-20241206105508651</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前端tutorial&quot;&gt;前端Tutorial&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://developer.mozilla.org/zh-CN/&quot;&gt;MDN Web Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img
</summary>
      
    
    
    
    <category term="前端" scheme="https://luyicui.github.io/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="前端" scheme="https://luyicui.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
</feed>
