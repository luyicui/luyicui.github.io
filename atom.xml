<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>弘毅 の blog</title>
  
  
  <link href="https://luyicui.github.io/atom.xml" rel="self"/>
  
  <link href="https://luyicui.github.io/"/>
  <updated>2025-01-01T04:33:19.396Z</updated>
  <id>https://luyicui.github.io/</id>
  
  <author>
    <name>弘毅</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2025/01/01/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/%E3%80%90pytorch%E3%80%91/Base/Tensor%E8%AE%A1%E7%AE%97/%E3%80%90tutorial%E3%80%91/"/>
    <id>https://luyicui.github.io/2025/01/01/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/%E3%80%90pytorch%E3%80%91/Base/Tensor%E8%AE%A1%E7%AE%97/%E3%80%90tutorial%E3%80%91/</id>
    <published>2025-01-01T04:26:44.594Z</published>
    <updated>2025-01-01T04:33:19.396Z</updated>
    
    <content type="html"><![CDATA[<p>张量的加减乘除是按元素进行计算的，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x + y)</span><br><span class="line">tensor([<span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x - y)</span><br><span class="line">tensor([-<span class="number">3.</span>, -<span class="number">3.</span>, -<span class="number">3.</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x * y)</span><br><span class="line">tensor([ <span class="number">4.</span>, <span class="number">10.</span>, <span class="number">18.</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x / y)</span><br><span class="line">tensor([<span class="number">0.2500</span>, <span class="number">0.4000</span>, <span class="number">0.5000</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>Pytorch 还提供了许多常用的计算函数，如 <code>torch.dot()</code> 计算向量点积、<code>torch.mm()</code> 计算矩阵相乘、三角函数和各种数学函数等：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.dot(y)</span><br><span class="line">tensor(<span class="number">32.</span>, dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.sin()</span><br><span class="line">tensor([<span class="number">0.8415</span>, <span class="number">0.9093</span>, <span class="number">0.1411</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.exp()</span><br><span class="line">tensor([ <span class="number">2.7183</span>,  <span class="number">7.3891</span>, <span class="number">20.0855</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>除了数学运算，Pytorch 还提供了多种张量操作函数，如聚合 (aggregation)、拼接 (concatenation)、比较、随机采样、序列化等，详细使用方法可以参见 <a href="https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html">Pytorch 官方文档</a>。</p><blockquote><p>  对张量进行聚合（如求平均、求和、最大值和最小值等）或拼接操作时，可以指定进行操作的维度 (dim)。例如，计算张量的平均值，在默认情况下会计算所有元素的平均值。：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean()</span><br><span class="line">tensor(<span class="number">3.5000</span>, dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>  更常见的情况是需要计算某一行或某一列的平均值，此时就需要设定计算的维度，例如分别对第 0 维和第 1 维计算平均值：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean(dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">2.5000</span>, <span class="number">3.5000</span>, <span class="number">4.5000</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean(dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">5.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>  注意，上面的计算自动去除了多余的维度，因此结果从矩阵变成了向量，如果要保持维度不变，可以设置 <code>keepdim=True</code>：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">2.5000</span>, <span class="number">3.5000</span>, <span class="number">4.5000</span>]], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">5.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><p>  拼接 <code>torch.cat</code> 操作类似，通过指定拼接维度，可以获得不同的拼接结果：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]], dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]], dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure></blockquote><p>组合使用这些操作就可以写出复杂的数学计算表达式。例如对于</p><script type="math/tex; mode=display">z=(x+y)×(y−2)</script><p>当 $x=2,y=3$ 时，很容易计算出 $z=5$。使用 Pytorch 来实现这一计算过程与 Python 非常类似，唯一的不同是数据使用张量进行保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">2.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([<span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = (x + y) * (y - <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(z)</span><br><span class="line">tensor([<span class="number">5.</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;张量的加减乘除是按元素进行计算的，例如：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/31/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/%E3%80%90transformers%E3%80%91/%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/"/>
    <id>https://luyicui.github.io/2024/12/31/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/%E3%80%90transformers%E3%80%91/%E7%AC%AC%E4%BA%94%E7%AB%A0%EF%BC%9A%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8/</id>
    <published>2024-12-31T15:49:46.928Z</published>
    <updated>2024-12-31T15:49:47.098Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第五章：模型与分词器"><a href="#第五章：模型与分词器" class="headerlink" title="第五章：模型与分词器"></a>第五章：模型与分词器</h1><p>本章我们将介绍 Transformers 库中的两个重要组件：<strong>模型</strong>和<strong>分词器</strong>。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>除了像之前使用 <code>AutoModel</code> 根据 checkpoint 自动加载模型以外，我们也可以直接使用模型对应的 <code>Model</code> 类，例如 BERT 对应的就是 <code>BertModel</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br></pre></td></tr></table></figure><p>注意，<strong>在大部分情况下，我们都应该使用 <code>AutoModel</code> 来加载模型。</strong>这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。</p><h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><p>所有存储在 HuggingFace <a href="https://huggingface.co/models">Model Hub</a> 上的模型都可以通过 <code>Model.from_pretrained()</code> 来加载权重，参数可以像上面一样是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录），例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&quot;./models/bert/&quot;</span>)</span><br></pre></td></tr></table></figure><p><code>Model.from_pretrained()</code> 会自动缓存下载的模型权重，默认保存到 <em>~/.cache/huggingface/transformers</em>，我们也可以通过 HF_HOME 环境变量自定义缓存目录。</p><blockquote><p>  由于 checkpoint 名称加载方式需要连接网络，因此在大部分情况下我们都会采用本地路径的方式加载模型。</p><p>  部分模型的 Hub 页面中会包含很多文件，我们通常只需要下载模型对应的 <em>config.json</em> 和 <em>pytorch_model.bin</em>，以及分词器对应的 <em>tokenizer.json</em>、<em>tokenizer_config.json</em> 和 <em>vocab.txt</em>。</p></blockquote><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><p>保存模型通过调用 <code>Model.save_pretrained()</code> 函数实现，例如保存加载的 BERT 模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"></span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">model.save_pretrained(<span class="string">&quot;./models/bert-base-cased/&quot;</span>)</span><br></pre></td></tr></table></figure><p>这会在保存路径下创建两个文件：</p><ul><li><em>config.json</em>：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；</li><li><em>pytorch_model.bin</em>：又称为 state dictionary，存储模型的权重。</li></ul><p>简单来说，配置文件记录模型的<strong>结构</strong>，模型权重记录模型的<strong>参数</strong>，这两个文件缺一不可。我们自己保存的模型同样通过 <code>Model.from_pretrained()</code> 加载，只需要传递保存目录的路径。</p><h2 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h2><p>由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为<strong>编码 (Encoding)</strong>，其包含两个步骤：</p><ol><li>使用分词器 (tokenizer) 将文本按词、子词、字符切分为 tokens；</li><li>将所有的 token 映射到对应的 token ID。</li></ol><h3 id="分词策略"><a href="#分词策略" class="headerlink" title="分词策略"></a>分词策略</h3><p>根据切分粒度的不同，分词策略可以分为以下几种：</p><ul><li><p><strong>按词切分 (Word-based)</strong></p><p><img src="https://transformers.run/assets/img/transformers-note-2/word_based_tokenization.png" alt="word_based_tokenization"></p><p>例如直接利用 Python 的 <code>split()</code> 函数按空格进行分词：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenized_text = <span class="string">&quot;Jim Henson was a puppeteer&quot;</span>.split()</span><br><span class="line"><span class="built_in">print</span>(tokenized_text)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;Jim&#x27;, &#x27;Henson&#x27;, &#x27;was&#x27;, &#x27;a&#x27;, &#x27;puppeteer&#x27;]</span><br></pre></td></tr></table></figure><p>这种策略的问题是会将文本中所有出现过的独立片段都作为不同的 token，从而产生巨大的词表。而实际上很多词是相关的，例如 “dog” 和 “dogs”、“run” 和 “running”，如果给它们赋予不同的编号就无法表示出这种关联性。</p><blockquote><p>  词表就是一个映射字典，负责将 token 映射到对应的 ID（从 0 开始）。神经网络模型就是通过这些 token ID 来区分每一个 token。</p></blockquote><p>当遇到不在词表中的词时，分词器会使用一个专门的 [UNK] token 来表示它是 unknown 的。显然，如果分词结果中包含很多 [UNK] 就意味着丢失了很多文本信息，因此一个好的分词策略，应该尽可能不出现 unknown token。</p></li><li><p><strong>按字符切分 (Character-based)</strong></p><p><img src="https://transformers.run/assets/img/transformers-note-2/character_based_tokenization.png" alt="character_based_tokenization"></p><p>这种策略把文本切分为字符而不是词语，这样就只会产生一个非常小的词表，并且很少会出现词表外的 tokens。</p><p>但是从直觉上来看，字符本身并没有太大的意义，因此将文本切分为字符之后就会变得不容易理解。这也与语言有关，例如中文字符会比拉丁字符包含更多的信息，相对影响较小。此外，这种方式切分出的 tokens 会很多，例如一个由 10 个字符组成的单词就会输出 10 个 tokens，而实际上它们只是一个词。</p><p>因此现在广泛采用的是一种同时结合了按词切分和按字符切分的方式——按子词切分 (Subword tokenization)。</p></li><li><p><strong>按子词切分 (Subword) </strong></p><p>高频词直接保留，低频词被切分为更有意义的子词。例如 “annoyingly” 是一个低频词，可以切分为 “annoying” 和 “ly”，这两个子词不仅出现频率更高，而且词义也得以保留。下图展示了对 “Let’s do tokenization!“ 按子词切分的结果：</p><p><img src="https://transformers.run/assets/img/transformers-note-2/bpe_subword.png" alt="bpe_subword"></p><p>可以看到，“tokenization” 被切分为了 “token” 和 “ization”，不仅保留了语义，而且只用两个 token 就表示了一个长词。这种策略只用一个较小的词表就可以覆盖绝大部分文本，基本不会产生 unknown token。尤其对于土耳其语等黏着语，几乎所有的复杂长词都可以通过串联多个子词构成。</p></li></ul><h3 id="加载与保存分词器"><a href="#加载与保存分词器" class="headerlink" title="加载与保存分词器"></a>加载与保存分词器</h3><p>分词器的加载与保存与模型相似，使用 <code>Tokenizer.from_pretrained()</code> 和 <code>Tokenizer.save_pretrained()</code> 函数。例如加载并保存 BERT 模型的分词器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;./models/bert-base-cased/&quot;</span>)</span><br></pre></td></tr></table></figure><p>同样地，在大部分情况下我们都应该使用 <code>AutoTokenizer</code> 来加载分词器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;./models/bert-base-cased/&quot;</span>)</span><br></pre></td></tr></table></figure><p>调用 <code>Tokenizer.save_pretrained()</code> 函数会在保存路径下创建三个文件：</p><ul><li><em>special_tokens_map.json</em>：映射文件，里面包含 unknown token 等特殊字符的映射关系；</li><li><em>tokenizer_config.json</em>：分词器配置文件，存储构建分词器需要的参数；</li><li><em>vocab.txt</em>：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。</li></ul><h3 id="编码与解码文本"><a href="#编码与解码文本" class="headerlink" title="编码与解码文本"></a>编码与解码文本</h3><p>前面说过，文本编码 (Encoding) 过程包含两个步骤：</p><ol><li><strong>分词：</strong>使用分词器按某种策略将文本切分为 tokens；</li><li><strong>映射：</strong>将 tokens 转化为对应的 token IDs。</li></ol><p>下面我们首先使用 BERT 分词器来对文本进行分词：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line">sequence = <span class="string">&quot;Using a Transformer network is simple&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(sequence)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;using&#x27;, &#x27;a&#x27;, &#x27;transform&#x27;, &#x27;##er&#x27;, &#x27;network&#x27;, &#x27;is&#x27;, &#x27;simple&#x27;]</span><br></pre></td></tr></table></figure><p>可以看到，BERT 分词器采用的是子词切分策略，它会不断切分词语直到获得词表中的 token，例如 “transformer” 会被切分为 “transform” 和 “##er”。</p><p>然后，我们通过 <code>convert_tokens_to_ids()</code> 将切分出的 tokens 转换为对应的 token IDs：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ids)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[7993, 170, 13809, 23763, 2443, 1110, 3014]</span><br></pre></td></tr></table></figure><p>还可以通过 <code>encode()</code> 函数将这两个步骤合并，并且 <code>encode()</code> 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 [CLS] 和 [SEP]：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line">sequence = <span class="string">&quot;Using a Transformer network is simple&quot;</span></span><br><span class="line">sequence_ids = tokenizer.encode(sequence)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sequence_ids)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]</span><br></pre></td></tr></table></figure><p>其中 101 和 102 分别是 [CLS] 和 [SEP] 对应的 token IDs。</p><p>注意，上面这些只是为了演示。<strong>在实际编码文本时，最常见的是直接使用分词器进行处理</strong>，这样不仅会返回分词后的 token IDs，还包含模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 <code>token_type_ids</code> 和 <code>attention_mask</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">tokenized_text = tokenizer(<span class="string">&quot;Using a Transformer network is simple&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenized_text)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], </span><br><span class="line"> &#x27;token_type_ids&#x27;: [0, 0, 0, 0, 0, 0, 0, 0, 0], </span><br><span class="line"> &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;</span><br></pre></td></tr></table></figure><p>文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。下面我们通过 <code>decode()</code> 函数解码前面生成的 token IDs：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line">decoded_string = tokenizer.decode([<span class="number">7993</span>, <span class="number">170</span>, <span class="number">11303</span>, <span class="number">1200</span>, <span class="number">2443</span>, <span class="number">1110</span>, <span class="number">3014</span>])</span><br><span class="line"><span class="built_in">print</span>(decoded_string)</span><br><span class="line"></span><br><span class="line">decoded_string = tokenizer.decode([<span class="number">101</span>, <span class="number">7993</span>, <span class="number">170</span>, <span class="number">13809</span>, <span class="number">23763</span>, <span class="number">2443</span>, <span class="number">1110</span>, <span class="number">3014</span>, <span class="number">102</span>])</span><br><span class="line"><span class="built_in">print</span>(decoded_string)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Using a transformer network is simple</span><br><span class="line">[CLS] Using a Transformer network is simple [SEP]</span><br></pre></td></tr></table></figure><p>解码文本是一个重要的步骤，在进行文本生成、翻译或者摘要等 Seq2Seq (Sequence-to-Sequence) 任务时都会调用这一函数。</p><h2 id="处理多段文本"><a href="#处理多段文本" class="headerlink" title="处理多段文本"></a>处理多段文本</h2><p>现实场景中，我们往往会同时处理多段文本，而且模型也只接受批 (batch) 数据作为输入，即使只有一段文本，也需要将它组成一个只包含一个样本的 batch，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequence = <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span></span><br><span class="line"></span><br><span class="line">tokens = tokenizer.tokenize(sequence)</span><br><span class="line">ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"><span class="comment"># input_ids = torch.tensor(ids), This line will fail.</span></span><br><span class="line">input_ids = torch.tensor([ids])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input IDs:\n&quot;</span>, input_ids)</span><br><span class="line"></span><br><span class="line">output = model(input_ids)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Logits:\n&quot;</span>, output.logits)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Input IDs: </span><br><span class="line">tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,</span><br><span class="line">          2026,  2878,  2166,  1012]])</span><br><span class="line">Logits: </span><br><span class="line">tensor([[-2.7276,  2.8789]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>这里我们通过 <code>[ids]</code> 构建了一个只包含一段文本的 batch，更常见的是送入包含多段文本的 batch：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batched_ids = [ids, ids, ids, ...]</span><br></pre></td></tr></table></figure><p>注意，上面的代码仅作为演示。<strong>实际场景中，我们应该直接使用分词器对文本进行处理</strong>，例如对于上面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequence = <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span></span><br><span class="line"></span><br><span class="line">tokenized_inputs = tokenizer(sequence, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Inputs Keys:\n&quot;</span>, tokenized_inputs.keys())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nInput IDs:\n&quot;</span>, tokenized_inputs[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"></span><br><span class="line">output = model(**tokenized_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nLogits:\n&quot;</span>, output.logits)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Inputs Keys:</span><br><span class="line"> dict_keys([&#x27;input_ids&#x27;, &#x27;attention_mask&#x27;])</span><br><span class="line"></span><br><span class="line">Input IDs:</span><br><span class="line">tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,</span><br><span class="line">          2607,  2026,  2878,  2166,  1012,   102]])</span><br><span class="line"></span><br><span class="line">Logits:</span><br><span class="line">tensor([[-1.5607,  1.6123]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>可以看到，分词器输出的结果中不仅包含 token IDs（<code>input_ids</code>），还会包含模型需要的其他输入项。前面我们之所以只输入 token IDs 模型也能正常运行，是因为它自动地补全了其他的输入项，例如 <code>attention_mask</code> 等，后面我们会具体介绍。</p><blockquote><p>  由于分词器自动在序列的首尾添加了 [CLS] 和 [SEP] token，所以上面两个例子中模型的输出是有差异的。因为 DistilBERT 预训练时是包含 [CLS] 和 [SEP] 的，所以下面的例子才是正确的使用方法。</p></blockquote><h3 id="Padding-操作"><a href="#Padding-操作" class="headerlink" title="Padding 操作"></a>Padding 操作</h3><p>按批输入多段文本产生的一个直接问题就是：batch 中的文本有长有短，而输入张量必须是严格的二维矩形，维度为 (batch size,sequence length)，即每一段文本编码后的 token IDs 数量必须一样多。例如下面的 ID 列表是无法转换为张量的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>我们需要通过 Padding 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">padding_id = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, padding_id],</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>模型的 padding token ID 可以通过其分词器的 <code>pad_token_id</code> 属性获得。下面我们尝试将两段文本分别以独立以及 batch 的形式送入到模型中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequence1_ids = [[<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>]]</span><br><span class="line">sequence2_ids = [[<span class="number">200</span>, <span class="number">200</span>]]</span><br><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)</span><br><span class="line"><span class="built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)</span><br><span class="line"><span class="built_in">print</span>(model(torch.tensor(batched_ids)).logits)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">tensor([[ 1.5694, -1.3895],</span><br><span class="line">        [ 1.3374, -1.2163]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><blockquote><p>  问题出现了，使用 padding token 填充的序列的结果竟然与其单独送入模型时不同！</p><p>  这是因为模型默认会编码输入序列中的所有 token 以建模完整的上下文，因此这里会将填充的 padding token 也一同编码进去，从而生成不同的语义表示。</p></blockquote><p>因此，在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了，在前面的例子中相信你已经多次见过它了。</p><h3 id="Attention-Mask"><a href="#Attention-Mask" class="headerlink" title="Attention Mask"></a>Attention Mask</h3><p>Attention Mask 是一个尺寸与 input IDs 完全相同，且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。当然，一些特殊的模型结构也会借助 Attention Mask 来遮蔽掉指定的 tokens。</p><p>对于上面的例子，如果我们通过 <code>attention_mask</code> 标出填充的 padding token 的位置，计算结果就不会有问题了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequence1_ids = [[<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>]]</span><br><span class="line">sequence2_ids = [[<span class="number">200</span>, <span class="number">200</span>]]</span><br><span class="line">batched_ids = [</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, <span class="number">200</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">200</span>, tokenizer.pad_token_id],</span><br><span class="line">]</span><br><span class="line">batched_attention_masks = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)</span><br><span class="line"><span class="built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)</span><br><span class="line">outputs = model(</span><br><span class="line">    torch.tensor(batched_ids), </span><br><span class="line">    attention_mask=torch.tensor(batched_attention_masks))</span><br><span class="line"><span class="built_in">print</span>(outputs.logits)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.5694, -1.3895]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">tensor([[ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">tensor([[ 1.5694, -1.3895],</span><br><span class="line">        [ 0.5803, -0.4125]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>正如前面强调的那样，在实际使用时，我们应该直接使用分词器对文本进行处理，它不仅会向 token 序列中添加模型需要的特殊字符（例如 [CLS],[SEP]），还会自动生成对应的 Attention Mask。</p><p>目前大部分 Transformer 模型只能接受长度不超过 512 或 1024 的 token 序列，因此对于长序列，有以下三种处理方法：</p><ol><li>使用一个支持长文的 Transformer 模型，例如 <a href="https://huggingface.co/transformers/model_doc/longformer.html">Longformer</a> 和 <a href="https://huggingface.co/transformers/model_doc/led.html">LED</a>（最大长度 4096）；</li><li>设定最大长度 <code>max_sequence_length</code> 以<strong>截断</strong>输入序列：<code>sequence = sequence[:max_sequence_length]</code>。</li><li>将长文切片为短文本块 (chunk)，然后分别对每一个 chunk 编码。在后面的<a href="https://transformers.run/nlp/2022-03-08-transformers-note-5.html">快速分词器</a>中，我们会详细介绍。</li></ol><h3 id="直接使用分词器"><a href="#直接使用分词器" class="headerlink" title="直接使用分词器"></a>直接使用分词器</h3><p>正如前面所说，在实际使用时，我们应该直接使用分词器来完成包括分词、转换 token IDs、Padding、构建 Attention Mask、截断等操作。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequences = [</span><br><span class="line">    <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, </span><br><span class="line">    <span class="string">&quot;So have I!&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer(sequences)</span><br><span class="line"><span class="built_in">print</span>(model_inputs)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: [</span><br><span class="line">    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], </span><br><span class="line">    [101, 2061, 2031, 1045, 999, 102]], </span><br><span class="line"> &#x27;attention_mask&#x27;: [</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], </span><br><span class="line">    [1, 1, 1, 1, 1, 1]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，分词器的输出包含了模型需要的所有输入项。对于 DistilBERT 模型，就是 input IDs（<code>input_ids</code>）和 Attention Mask（<code>attention_mask</code>）。</p><p><strong>Padding 操作</strong>通过 <code>padding</code> 参数来控制：</p><ul><li><code>padding=&quot;longest&quot;</code>： 将序列填充到当前 batch 中最长序列的长度；</li><li><code>padding=&quot;max_length&quot;</code>：将所有序列填充到模型能够接受的最大长度，例如 BERT 模型就是 512。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequences = [</span><br><span class="line">    <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, </span><br><span class="line">    <span class="string">&quot;So have I!&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="string">&quot;longest&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_inputs)</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="string">&quot;max_length&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_inputs)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: [</span><br><span class="line">    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], </span><br><span class="line">    [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], </span><br><span class="line"> &#x27;attention_mask&#x27;: [</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], </span><br><span class="line">    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#x27;input_ids&#x27;: [</span><br><span class="line">    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, ...], </span><br><span class="line">    [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, ...]], </span><br><span class="line"> &#x27;attention_mask&#x27;: [</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...], </span><br><span class="line">    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>截断操作</strong>通过 <code>truncation</code> 参数来控制，如果 <code>truncation=True</code>，那么大于模型最大接受长度的序列都会被截断，例如对于 BERT 模型就会截断长度超过 512 的序列。此外，也可以通过 <code>max_length</code> 参数来控制截断长度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequences = [</span><br><span class="line">    <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, </span><br><span class="line">    <span class="string">&quot;So have I!&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer(sequences, max_length=<span class="number">8</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(model_inputs)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: [</span><br><span class="line">    [101, 1045, 1005, 2310, 2042, 3403, 2005, 102], </span><br><span class="line">    [101, 2061, 2031, 1045, 999, 102]], </span><br><span class="line"> &#x27;attention_mask&#x27;: [</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1], </span><br><span class="line">    [1, 1, 1, 1, 1, 1]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分词器还可以通过 <code>return_tensors</code> 参数指定返回的张量格式：设为 <code>pt</code> 则返回 PyTorch 张量；<code>tf</code> 则返回 TensorFlow 张量，<code>np</code> 则返回 NumPy 数组。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequences = [</span><br><span class="line">    <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, </span><br><span class="line">    <span class="string">&quot;So have I!&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_inputs)</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer(sequences, padding=<span class="literal">True</span>, return_tensors=<span class="string">&quot;np&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_inputs)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: tensor([</span><br><span class="line">    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,</span><br><span class="line">      2607,  2026,  2878,  2166,  1012,   102],</span><br><span class="line">    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,</span><br><span class="line">         0,     0,     0,     0,     0,     0]]), </span><br><span class="line"> &#x27;attention_mask&#x27;: tensor([</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;&#x27;input_ids&#x27;: array([</span><br><span class="line">    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,</span><br><span class="line">     12172,  2607,  2026,  2878,  2166,  1012,   102],</span><br><span class="line">    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,</span><br><span class="line">         0,     0,     0,     0,     0,     0,     0]]), </span><br><span class="line"> &#x27;attention_mask&#x27;: array([</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样就可以直接将分词结果送入模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sequences = [</span><br><span class="line">    <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, </span><br><span class="line">    <span class="string">&quot;So have I!&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">tokens = tokenizer(sequences, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br><span class="line">output = model(**tokens)</span><br><span class="line"><span class="built_in">print</span>(output.logits)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: tensor([</span><br><span class="line">    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,</span><br><span class="line">      2607,  2026,  2878,  2166,  1012,   102],</span><br><span class="line">    [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,</span><br><span class="line">         0,     0,     0,     0,     0,     0]]), </span><br><span class="line"> &#x27;attention_mask&#x27;: tensor([</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])&#125;</span><br><span class="line"></span><br><span class="line">tensor([[-1.5607,  1.6123],</span><br><span class="line">        [-3.6183,  3.9137]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>在 <code>padding=True, truncation=True</code> 设置下，同一个 batch 中的序列都会 padding 到相同的长度，并且大于模型最大接受长度的序列会被自动截断。</p><h3 id="编码句子对"><a href="#编码句子对" class="headerlink" title="编码句子对"></a>编码句子对</h3><p>除了对单段文本进行编码以外（batch 只是并行地编码多个单段文本），对于 BERT 等包含“句子对”预训练任务的模型，它们的分词器都支持对“句子对”进行编码，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(<span class="string">&quot;This is the first sentence.&quot;</span>, <span class="string">&quot;This is the second one.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line"></span><br><span class="line">tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], </span><br><span class="line"> &#x27;token_type_ids&#x27;: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], </span><br><span class="line"> &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;</span><br><span class="line"></span><br><span class="line">[&#x27;[CLS]&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;the&#x27;, &#x27;first&#x27;, &#x27;sentence&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;the&#x27;, &#x27;second&#x27;, &#x27;one&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span><br></pre></td></tr></table></figure><p>此时分词器会使用 [SEP] token 拼接两个句子，输出形式为“[CLS] sentence1 [SEP] sentence2 [SEP]”的 token 序列，这也是 BERT 模型预期的“句子对”输入格式。</p><p>返回结果中除了前面我们介绍过的 <code>input_ids</code> 和 <code>attention_mask</code> 之外，还包含了一个 <code>token_type_ids</code> 项，用于标记哪些 token 属于第一个句子，哪些属于第二个句子。如果我们将上面例子中的 <code>token_type_ids</code> 项与 token 序列对齐：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;[CLS]&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;the&#x27;, &#x27;first&#x27;, &#x27;sentence&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;, &#x27;this&#x27;, &#x27;is&#x27;, &#x27;the&#x27;, &#x27;second&#x27;, &#x27;one&#x27;, &#x27;.&#x27;, &#x27;[SEP]&#x27;]</span><br><span class="line">[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]</span><br></pre></td></tr></table></figure><p>就可以看到第一个句子“[CLS] sentence1 [SEP]”所有 token 的 type ID 都为 0，而第二个句子“sentence2 [SEP]”对应的 token type ID 都为 1。</p><blockquote><p>  如果我们选择其他模型，分词器的输出不一定会包含 <code>token_type_ids</code> 项（例如 DistilBERT 模型）。分词器只需保证输出格式与模型预训练时的输入一致即可。</p></blockquote><p>实际使用时，我们不需要去关注编码结果中是否包含 <code>token_type_ids</code> 项，分词器会根据 checkpoint 自动调整输出格式，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sentence1_list = [<span class="string">&quot;First sentence.&quot;</span>, <span class="string">&quot;This is the second sentence.&quot;</span>, <span class="string">&quot;Third one.&quot;</span>]</span><br><span class="line">sentence2_list = [<span class="string">&quot;First sentence is short.&quot;</span>, <span class="string">&quot;The second sentence is very very very long.&quot;</span>, <span class="string">&quot;ok.&quot;</span>]</span><br><span class="line"></span><br><span class="line">tokens = tokenizer(</span><br><span class="line">    sentence1_list,</span><br><span class="line">    sentence2_list,</span><br><span class="line">    padding=<span class="literal">True</span>,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">    return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br><span class="line"><span class="built_in">print</span>(tokens[<span class="string">&#x27;input_ids&#x27;</span>].shape)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: tensor([</span><br><span class="line">        [ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,</span><br><span class="line">            0,    0,    0,    0,    0,    0],</span><br><span class="line">        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,</span><br><span class="line">         2200, 2200, 2200, 2146, 1012,  102],</span><br><span class="line">        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,</span><br><span class="line">            0,    0,    0,    0,    0,    0]]), </span><br><span class="line"> &#x27;token_type_ids&#x27;: tensor([</span><br><span class="line">        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span><br><span class="line">        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), </span><br><span class="line"> &#x27;attention_mask&#x27;: tensor([</span><br><span class="line">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span><br><span class="line">        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line">&#125;</span><br><span class="line">torch.Size([3, 18])</span><br></pre></td></tr></table></figure><p>可以看到分词器成功地输出了形式为“[CLS] sentence1 [SEP] sentence2 [SEP]”的 token 序列，并且将三个序列都 padding 到了相同的长度。</p><h2 id="添加-Token"><a href="#添加-Token" class="headerlink" title="添加 Token"></a>添加 Token</h2><p>实际操作中，我们还经常会遇到输入中需要包含特殊标记符的情况，例如使用 [ENT_START] 和 [ENT_END] 标记出文本中的实体。由于这些自定义 token 并不在预训练模型原来的词表中，因此直接运用分词器处理就会出现问题。</p><p>例如直接使用 BERT 分词器处理下面的句子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&#x27;Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.tokenize(sentence))</span><br><span class="line">[<span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;start&#x27;</span>, <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;cars&#x27;</span>, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;collided&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;start&#x27;</span>, <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;tunnel&#x27;</span>, <span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;end&#x27;</span>, <span class="string">&#x27;]&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;morning&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure><p>由于分词器无法识别 [ENT<em>START] 和 [ENT_END] ，因此将它们都当作未知字符处理，例如“[ENT_END]”被切分成了 <code>&#39;[&#39;</code>, <code>&#39;en&#39;</code>, <code>&#39;##t&#39;</code>, `’</em>‘<code>,</code>‘end’<code>,</code>‘]’` 六个 token。</p><p>此外，一些领域的专业词汇，例如使用多个词语的缩写拼接而成的医学术语，同样也不在模型的词表中，因此也会出现上面的问题。此时我们就需要将这些新 token 添加到模型的词表中，让分词器与模型可以识别并处理这些 token。</p><h3 id="添加新-token"><a href="#添加新-token" class="headerlink" title="添加新 token"></a>添加新 token</h3><p>Transformers 库提供了两种方式来添加新 token，分别是：</p><ul><li><p><strong><a href="https://huggingface.co/docs/transformers/v4.25.1/en/internal/tokenization_utils#transformers.SpecialTokensMixin.add_tokens"><code>add_tokens()</code></a> 添加普通 token：</strong>参数是新 token 列表，如果 token 不在词表中，就会被添加到词表的最后。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">    </span><br><span class="line">num_added_toks = tokenizer.add_tokens([<span class="string">&quot;new_token1&quot;</span>, <span class="string">&quot;my_new-token2&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;We have added&quot;</span>, num_added_toks, <span class="string">&quot;tokens&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">We have added 2 tokens</span><br></pre></td></tr></table></figure><p>为了防止 token 已经包含在词表中，我们还可以预先对新 token 列表进行过滤：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_tokens = [<span class="string">&quot;new_token1&quot;</span>, <span class="string">&quot;my_new-token2&quot;</span>]</span><br><span class="line">new_tokens = <span class="built_in">set</span>(new_tokens) - <span class="built_in">set</span>(tokenizer.vocab.keys())</span><br><span class="line">tokenizer.add_tokens(<span class="built_in">list</span>(new_tokens))</span><br></pre></td></tr></table></figure></li><li><p><strong><a href="https://huggingface.co/docs/transformers/v4.25.1/en/internal/tokenization_utils#transformers.SpecialTokensMixin.add_special_tokens"><code>add_special_tokens()</code></a> 添加特殊 token：</strong>参数是包含特殊 token 的字典，键值只能从 <code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>, <code>additional_special_tokens</code> 中选择。同样地，如果 token 不在词表中，就会被添加到词表的最后。添加后，还可以通过特殊属性来访问这些 token，例如 <code>tokenizer.cls_token</code> 就指向 cls token。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">    </span><br><span class="line">special_tokens_dict = &#123;<span class="string">&quot;cls_token&quot;</span>: <span class="string">&quot;[MY_CLS]&quot;</span>&#125;</span><br><span class="line">    </span><br><span class="line">num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;We have added&quot;</span>, num_added_toks, <span class="string">&quot;tokens&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">assert</span> tokenizer.cls_token == <span class="string">&quot;[MY_CLS]&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">We have added 1 tokens</span><br></pre></td></tr></table></figure><p>我们也可以使用 <code>add_tokens()</code> 添加特殊 token，只需要额外设置参数 <code>special_tokens=True</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">    </span><br><span class="line">num_added_toks = tokenizer.add_tokens([<span class="string">&quot;[NEW_tok1]&quot;</span>, <span class="string">&quot;[NEW_tok2]&quot;</span>])</span><br><span class="line">num_added_toks = tokenizer.add_tokens([<span class="string">&quot;[NEW_tok3]&quot;</span>, <span class="string">&quot;[NEW_tok4]&quot;</span>], special_tokens=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;We have added&quot;</span>, num_added_toks, <span class="string">&quot;tokens&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.tokenize(<span class="string">&#x27;[NEW_tok1] Hello [NEW_tok2] [NEW_tok3] World [NEW_tok4]!&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">We have added 2 tokens</span><br><span class="line">[&#x27;[new_tok1]&#x27;, &#x27;hello&#x27;, &#x27;[new_tok2]&#x27;, &#x27;[NEW_tok3]&#x27;, &#x27;world&#x27;, &#x27;[NEW_tok4]&#x27;, &#x27;!&#x27;]</span><br></pre></td></tr></table></figure><blockquote><p>  特殊 token 的标准化 (normalization) 与普通 token 有一些不同，比如不会被小写。</p><p>  这里我们使用的是不区分大小写的 BERT 模型，因此分词后添加的普通 token [NEW_tok1] 和 [NEW_tok2] 都被处理为了小写，而添加的特殊 token [NEW_tok3] 和 [NEW_tok4] 则保持大写。</p></blockquote></li></ul><p>对于前面的例子，很明显实体标记符 [ENT_START] 和 [ENT_END] 属于特殊 token，因此按添加特殊 token 的方式进行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line">num_added_toks = tokenizer.add_tokens([<span class="string">&#x27;[ENT_START]&#x27;</span>, <span class="string">&#x27;[ENT_END]&#x27;</span>], special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># num_added_toks = tokenizer.add_special_tokens(&#123;&#x27;additional_special_tokens&#x27;: [&#x27;[ENT_START]&#x27;, &#x27;[ENT_END]&#x27;]&#125;)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;We have added&quot;</span>, num_added_toks, <span class="string">&quot;tokens&quot;</span>)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&#x27;Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenizer.tokenize(sentence))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">We have added 2 tokens</span><br><span class="line">[&#x27;two&#x27;, &#x27;[ENT_START]&#x27;, &#x27;cars&#x27;, &#x27;[ENT_END]&#x27;, &#x27;collided&#x27;, &#x27;in&#x27;, &#x27;a&#x27;, &#x27;[ENT_START]&#x27;, &#x27;tunnel&#x27;, &#x27;[ENT_END]&#x27;, &#x27;this&#x27;, &#x27;morning&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure><p>可以看到，分词器成功地将 [ENT_START] 和 [ENT_END] 识别为 token，并且保持大写。</p><h3 id="调整-embedding-矩阵"><a href="#调整-embedding-矩阵" class="headerlink" title="调整 embedding 矩阵"></a>调整 embedding 矩阵</h3><blockquote><p>  向词表中添加新 token 后，必须重置模型 embedding 矩阵的大小，也就是向矩阵中添加新 token 对应的 embedding，这样模型才可以正常工作，将 token 映射到对应的 embedding。</p></blockquote><p>调整 embedding 矩阵通过 <code>resize_token_embeddings()</code> 函数来实现，例如对于前面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModel.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;vocabulary size:&#x27;</span>, <span class="built_in">len</span>(tokenizer))</span><br><span class="line">num_added_toks = tokenizer.add_tokens([<span class="string">&#x27;[ENT_START]&#x27;</span>, <span class="string">&#x27;[ENT_END]&#x27;</span>], special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After we add&quot;</span>, num_added_toks, <span class="string">&quot;tokens&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;vocabulary size:&#x27;</span>, <span class="built_in">len</span>(tokenizer))</span><br><span class="line"></span><br><span class="line">model.resize_token_embeddings(<span class="built_in">len</span>(tokenizer))</span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly generated matrix</span></span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight[-<span class="number">2</span>:, :])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vocabulary size: 30522</span><br><span class="line">After we add 2 tokens</span><br><span class="line">vocabulary size: 30524</span><br><span class="line">torch.Size([30524, 768])</span><br><span class="line"></span><br><span class="line">tensor([[-0.0325, -0.0224,  0.0044,  ..., -0.0088, -0.0078, -0.0110],</span><br><span class="line">        [-0.0005, -0.0167, -0.0009,  ...,  0.0110, -0.0282, -0.0013]],</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><p>可以看到，在添加 [ENT_START] 和 [ENT_END] 之后，分词器的词表大小从 30522 增加到了 30524，模型 embedding 矩阵的大小也成功调整为了 30524×768。</p><blockquote><p>  在默认情况下，新添加 token 的 embedding 是随机初始化的。</p></blockquote><p>我们尝试打印出新添加 token 对应的 embedding（新 token 会添加在词表的末尾，因此只需打印出最后两行）。如果你多次运行上面的代码，就会发现每次打印出的 [ENT_START] 和 [ENT_END] 的 embedding 是不同的。</p><h2 id="Token-embedding-初始化"><a href="#Token-embedding-初始化" class="headerlink" title="Token embedding 初始化"></a>Token embedding 初始化</h2><p>如果有充分的语料对模型进行微调或者继续预训练，那么将新添加 token 初始化为随机向量没什么问题。但是如果训练语料较少，甚至是只有很少语料的 few-shot learning 场景下，这种做法就存在问题。研究表明，在训练数据不够多的情况下，这些新添加 token 的 embedding 只会在初始值附近小幅波动。换句话说，即使经过训练，它们的值事实上还是随机的。</p><h3 id="直接赋值"><a href="#直接赋值" class="headerlink" title="直接赋值"></a>直接赋值</h3><p>因此，在很多情况下，我们需要手工初始化新添加 token 的 embedding，这可以通过直接对 embedding 矩阵赋值来实现。例如我们将上面例子中两个新 token 的 embedding 都初始化为全零向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.embeddings.word_embeddings.weight[-<span class="number">2</span>:, :] = torch.zeros([<span class="number">2</span>, model.config.hidden_size], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight[-<span class="number">2</span>:, :])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><p>注意，初始化 embedding 的过程并不可导，因此这里通过 <code>torch.no_grad()</code> 暂停梯度的计算。</p><p>现实场景中，更为常见的做法是使用已有 token 的 embedding 来初始化新添加 token。例如对于上面的例子，我们可以将 <code>[ENT_START]</code> 和 <code>[ENT_END]</code> 的值都初始化为“entity” token 对应的 embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">token_id = tokenizer.convert_tokens_to_ids(<span class="string">&#x27;entity&#x27;</span>)</span><br><span class="line">token_embedding = model.embeddings.word_embeddings.weight[token_id]</span><br><span class="line"><span class="built_in">print</span>(token_id)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_added_toks+<span class="number">1</span>):</span><br><span class="line">        model.embeddings.word_embeddings.weight[-i:, :] = token_embedding.clone().detach().requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight[-<span class="number">2</span>:, :])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">9178</span><br><span class="line">tensor([[-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419],</span><br><span class="line">        [-0.0039, -0.0131, -0.0946,  ..., -0.0223,  0.0107, -0.0419]],</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><blockquote><p>  因为 token ID 就是 token 在 embedding 矩阵中的索引，因此这里我们直接通过 <code>weight[token_id]</code> 取出“entity”对应的 embedding。</p></blockquote><p>可以看到最终结果符合我们的预期，<code>[ENT_START]</code> 和 <code>[ENT_END]</code> 被初始化为相同的 embedding。</p><h3 id="初始化为已有-token-的值"><a href="#初始化为已有-token-的值" class="headerlink" title="初始化为已有 token 的值"></a>初始化为已有 token 的值</h3><p>更为高级的做法是根据新添加 token 的语义来进行初始化。例如将值初始化为 token 语义描述中所有 token 的平均值，假设新 token $t<em>i$ 的语义描述为 $w</em>{i,1},w<em>{i,2},…,w</em>{i,n}$，那么初始化 $t<em>i$ 的 embedding 为：$\boldsymbol{E}(t_i)=\frac{1}{n}\sum</em>{j=1}^n{\boldsymbol{E}(w_{i,j})}$</p><p>这里 $\boldsymbol{E}$ 表示预训练模型的 embedding 矩阵。对于上面的例子，我们可以分别为 [ENT_START] 和 [ENT_END] 编写对应的描述，然后再对它们的值进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">descriptions = [<span class="string">&#x27;start of entity&#x27;</span>, <span class="string">&#x27;end of entity&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(descriptions), start=<span class="number">1</span>):</span><br><span class="line">        tokenized = tokenizer.tokenize(token)</span><br><span class="line">        <span class="built_in">print</span>(tokenized)</span><br><span class="line">        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)</span><br><span class="line">        new_embedding = model.embeddings.word_embeddings.weight[tokenized_ids].mean(axis=<span class="number">0</span>)</span><br><span class="line">        model.embeddings.word_embeddings.weight[-i, :] = new_embedding.clone().detach().requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight[-<span class="number">2</span>:, :])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;end&#x27;, &#x27;of&#x27;, &#x27;entity&#x27;]</span><br><span class="line">[&#x27;start&#x27;, &#x27;of&#x27;, &#x27;entity&#x27;]</span><br><span class="line">tensor([[-0.0340, -0.0144, -0.0441,  ..., -0.0016,  0.0318, -0.0151],</span><br><span class="line">        [-0.0060, -0.0202, -0.0312,  ..., -0.0084,  0.0193, -0.0296]],</span><br><span class="line">       grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><p>可以看到，这里成功地将 [ENT_START] 的 embedding 初始化为“start”、“of”、“entity”三个 token 的平均值，将 [ENT_END] 初始化为“end”、“of”、“entity”的平均值。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://huggingface.co/docs/transformers/index">[1]</a> Transformers 官方文档<br><a href="https://huggingface.co/course/chapter1/1">[2]</a> HuggingFace 在线教程<br><a href="https://www.depends-on-the-definition.com/how-to-add-new-tokens-to-huggingface-transformers/">[3]</a> How to add new tokens to huggingface transformers vocabulary. <a href="https://www.depends-on-the-definition.com/about/">Tobias Sterbak</a><br><a href="https://github.com/huggingface/transformers/issues/1413">[4]</a> Github 讨论 Adding New Vocabulary Tokens to the Models</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第五章：模型与分词器&quot;&gt;&lt;a href=&quot;#第五章：模型与分词器&quot; class=&quot;headerlink&quot; title=&quot;第五章：模型与分词器&quot;&gt;&lt;/a&gt;第五章：模型与分词器&lt;/h1&gt;&lt;p&gt;本章我们将介绍 Transformers 库中的两个重要组件：&lt;strong</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/31/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/Python%E4%B8%AD%E7%9C%81%E7%95%A5%E5%8F%B7%E7%9A%84%E5%90%AB%E4%B9%89/"/>
    <id>https://luyicui.github.io/2024/12/31/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/Python%E4%B8%AD%E7%9C%81%E7%95%A5%E5%8F%B7%E7%9A%84%E5%90%AB%E4%B9%89/</id>
    <published>2024-12-31T04:58:21.166Z</published>
    <updated>2024-12-31T04:58:21.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python中省略号的含义"><a href="#Python中省略号的含义" class="headerlink" title="Python中省略号的含义"></a>Python中省略号的含义</h1><p>在Python编程语言中，省略号（三个点“…”）是一个有趣且多用途的符号，它不仅具有语法上的意义，还在实际编程中提供了诸多便利。本文将深入探讨Python中省略号的用法，并重点介绍其在函数参数中的应用。</p><h2 id="省略号的基本概念"><a href="#省略号的基本概念" class="headerlink" title="省略号的基本概念"></a>省略号的基本概念</h2><p>在Python中，省略号（…）实际上是一个名为<code>Ellipsis</code>的单例对象。可以通过以下代码验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(...))            <span class="comment"># output: &lt;class &#x27;ellipsis&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="literal">Ellipsis</span> == ...)      <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(...)                  <span class="comment"># Ellipsis</span></span><br></pre></td></tr></table></figure><blockquote><p>  如上的代码输出说明：<strong>Ellipsis就是省略号（…），省略号（…）就是Ellipsis</strong>。<strong>而Ellipsis是ellipsis类的唯一实例（singleton object）</strong>，这种唯一实例的模式也称为单例模式（singleton pattern）。</p></blockquote><p><code>Ellipsis</code>对象在Python中没有直接的方法，但它却在多维数组处理、函数参数定义等方面发挥着重要作用。</p><h2 id="省略号在多维数组中的应用"><a href="#省略号在多维数组中的应用" class="headerlink" title="省略号在多维数组中的应用"></a>省略号在多维数组中的应用</h2><p>在处理多维数组时，省略号常用于简化切片操作。例如，在NumPy或PyTorch中，省略号可以用来选择所有前面的维度。</p><p>以 PyTorch 为例，假设有一个形状为<code>(batch_size, channels, height, width)</code>的 4D 张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tensor = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)  <span class="comment"># 随机生成一个4D张量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特定(height, width)位置的元素</span></span><br><span class="line">specific_element = tensor[:, :, <span class="number">32</span>, <span class="number">32</span>]</span><br><span class="line"><span class="built_in">print</span>(specific_element.shape)  <span class="comment"># 输出: torch.Size([10, 3])</span></span><br></pre></td></tr></table></figure><p>使用省略号可以简化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">specific_element = tensor[..., <span class="number">32</span>, <span class="number">32</span>]</span><br><span class="line"><span class="built_in">print</span>(specific_element.shape)  <span class="comment"># 输出: torch.Size([10, 3])</span></span><br></pre></td></tr></table></figure><p>这里，<code>...</code>表示选择所有前面的维度，使得代码更加简洁易读。</p><h2 id="省略号在函数参数中的应用"><a href="#省略号在函数参数中的应用" class="headerlink" title="省略号在函数参数中的应用"></a>省略号在函数参数中的应用</h2><p>省略号在函数参数中的应用主要体现在两个方面：作为参数的占位符和用于类型注解。</p><h3 id="作为参数的占位符"><a href="#作为参数的占位符" class="headerlink" title="作为参数的占位符"></a>作为参数的占位符</h3><p>在某些情况下，我们可能需要定义一个函数，但暂时不确定具体的参数。此时，可以使用省略号作为占位符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">placeholder_function</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="keyword">pass</span>  <span class="comment"># 待实现</span></span><br></pre></td></tr></table></figure><p>这种方式类似于使用<code>pass</code>语句，但更明确地表示函数参数部分尚未确定。</p><h3 id="用于类型注解"><a href="#用于类型注解" class="headerlink" title="用于类型注解"></a>用于类型注解</h3><p>在 Python 的类型注解中，<code>...</code>（省略号）通常表示 <strong>灵活性</strong> 或 <strong>不确定性</strong>，其具体含义取决于使用的上下文：</p><h4 id="在-Tuple-T-中"><a href="#在-Tuple-T-中" class="headerlink" title="在 Tuple[T, ...] 中"></a>在 <code>Tuple[T, ...]</code> 中</h4><p>表示一个元组，其 <strong>长度可以任意</strong>，但所有元素的类型必须是 <code>T</code>。</p><ul><li><p>用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line">numbers: <span class="type">Tuple</span>[<span class="built_in">int</span>, ...] = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)  <span class="comment"># 任意长度的 int 元组</span></span><br></pre></td></tr></table></figure></li><li><p>含义：元组的每个元素类型为 <code>T</code>（如 <code>int</code>），长度不限。</p></li></ul><h4 id="在-Callable-R-中"><a href="#在-Callable-R-中" class="headerlink" title="在 Callable[..., R] 中"></a>在 <code>Callable[..., R]</code> 中</h4><p>表示一个可调用对象，其 <strong>参数数量和类型不限</strong>，但返回值类型为 <code>R</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">fn: <span class="type">Callable</span>[..., <span class="built_in">int</span>], a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> fn(a, b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(main(add, <span class="number">10</span>, <span class="number">2</span>))  <span class="comment"># 输出: 12</span></span><br></pre></td></tr></table></figure><ul><li>含义：在这里，<code>Callable[..., int]</code>表示<code>fn</code>是一个可调用的对象，可以接受任何类型、任何数量的参数，并返回一个<code>int</code>类型的值。</li></ul><h4 id="表示“待定”"><a href="#表示“待定”" class="headerlink" title="表示“待定”"></a>表示“待定”</h4><p>在类型注解中，<code>...</code> 也可以作为一种 <strong>占位符</strong>，表示尚未明确的逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x: <span class="type">Any</span></span>) -&gt; ...:  <span class="comment"># 返回值类型待定</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="省略号的其他用法"><a href="#省略号的其他用法" class="headerlink" title="省略号的其他用法"></a>省略号的其他用法</h2><p>除了上述应用外，省略号还有一些其他有趣的用法：</p><h3 id="在类定义中"><a href="#在类定义中" class="headerlink" title="在类定义中"></a>在类定义中</h3><p>在类定义中，省略号可以用来表示类的某些部分尚未实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PlaceholderClass</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">method</span>(<span class="params">self, ...</span>):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h3 id="在文档字符串中"><a href="#在文档字符串中" class="headerlink" title="在文档字符串中"></a>在文档字符串中</h3><p>在文档字符串中，省略号可以用来表示省略的内容或待补充的信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_function</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    这是一个示例函数...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Python中的省略号（三个点“…”）是一个多功能的符号，它在多维数组处理、函数参数定义、类定义等方面都发挥着重要作用。通过合理使用省略号，可以使代码更加简洁、易读，并且在设计阶段提供灵活性。</p><p>希望本文的介绍能帮助你更好地理解和应用Python中的省略号，提升你的编程效率和代码质量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python中省略号的含义&quot;&gt;&lt;a href=&quot;#Python中省略号的含义&quot; class=&quot;headerlink&quot; title=&quot;Python中省略号的含义&quot;&gt;&lt;/a&gt;Python中省略号的含义&lt;/h1&gt;&lt;p&gt;在Python编程语言中，省略号（三个点“…”）是一</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/29/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/Python_Data_Science/%E3%80%90numpy%E3%80%91/%E3%80%90References%E3%80%91/"/>
    <id>https://luyicui.github.io/2024/12/29/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/Python_Data_Science/%E3%80%90numpy%E3%80%91/%E3%80%90References%E3%80%91/</id>
    <published>2024-12-29T05:26:57.964Z</published>
    <updated>2024-12-29T05:27:07.299Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.pythontutorial.net/python-numpy/">Python Numpy Tutorial</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.pythontutorial.net/python-numpy/&quot;&gt;Python Numpy Tutorial&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/29/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E3%80%90tutorial%E3%80%91/%E3%80%90Python%20Concurrency%E3%80%91/"/>
    <id>https://luyicui.github.io/2024/12/29/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E3%80%90tutorial%E3%80%91/%E3%80%90Python%20Concurrency%E3%80%91/</id>
    <published>2024-12-29T05:26:10.062Z</published>
    <updated>2024-12-29T05:26:24.991Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.pythontutorial.net/python-concurrency/">Python Concurrency</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.pythontutorial.net/python-concurrency/&quot;&gt;Python Concurrency&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/06/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/vue/6.3%20%E8%AF%BE%E4%B8%8A%E9%A1%B9%E7%9B%AE%E7%9A%84API/"/>
    <id>https://luyicui.github.io/2024/12/06/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/vue/6.3%20%E8%AF%BE%E4%B8%8A%E9%A1%B9%E7%9B%AE%E7%9A%84API/</id>
    <published>2024-12-06T14:39:44.085Z</published>
    <updated>2024-12-06T14:39:44.854Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导航：6-Vue3"><a href="#导航：6-Vue3" class="headerlink" title="导航：6. Vue3"></a>导航：<a href="../6. Vue3.md">6. Vue3</a></h2><h2 id="1-获取Json-Web-Token（JWT）"><a href="#1-获取Json-Web-Token（JWT）" class="headerlink" title="1. 获取Json Web Token（JWT）"></a>1. 获取Json Web Token（JWT）</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/api/token/">https://app165.acapp.acwing.com.cn/api/token/</a></li><li>方法：<code>POST</code></li><li>是否验证jwt：否</li><li>输入参数：<ul><li><code>username</code>: 用户名</li><li><code>password</code>: 密码</li></ul></li><li>返回结果：<ul><li><code>access</code>: 访问令牌，有效期5分钟</li><li><code>refresh</code>: 刷新令牌，有效期14天</li></ul></li></ul><h2 id="2-刷新JWT令牌"><a href="#2-刷新JWT令牌" class="headerlink" title="2. 刷新JWT令牌"></a>2. 刷新JWT令牌</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/api/token/refresh/">https://app165.acapp.acwing.com.cn/api/token/refresh/</a></li><li>方法：<code>POST</code></li><li>是否验证jwt：否</li><li>输入参数：<ul><li><code>refresh</code>: 刷新令牌</li></ul></li><li>返回结果：<ul><li><code>access</code>: 访问令牌，有效期5分钟</li></ul></li></ul><h2 id="3-获取用户列表"><a href="#3-获取用户列表" class="headerlink" title="3. 获取用户列表"></a>3. 获取用户列表</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/userlist/">https://app165.acapp.acwing.com.cn/myspace/userlist/</a></li><li>方法：<code>GET</code></li><li>是否验证jwt：否</li><li>输入参数：无</li><li>返回结果：返回10个用户的信息</li></ul><h2 id="4-获取某个用户的信息"><a href="#4-获取某个用户的信息" class="headerlink" title="4. 获取某个用户的信息"></a>4. 获取某个用户的信息</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/getinfo/">https://app165.acapp.acwing.com.cn/myspace/getinfo/</a></li><li>方法：<code>GET</code></li><li>是否验证jwt：是</li><li>输入参数：<ul><li><code>user_id</code>：用户的ID</li></ul></li><li>返回结果：该用户的信息</li></ul><h2 id="5-获取某个用户的所有帖子"><a href="#5-获取某个用户的所有帖子" class="headerlink" title="5. 获取某个用户的所有帖子"></a>5. 获取某个用户的所有帖子</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/post/">https://app165.acapp.acwing.com.cn/myspace/post/</a></li><li>方法：<code>GET</code></li><li>是否验证jwt：是</li><li>输入参数：<ul><li><code>user_id</code>：用户的ID</li></ul></li><li>返回结果：该用户的所有帖子</li></ul><h2 id="6-创建一个帖子"><a href="#6-创建一个帖子" class="headerlink" title="6. 创建一个帖子"></a>6. 创建一个帖子</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/post/">https://app165.acapp.acwing.com.cn/myspace/post/</a></li><li>方法：<code>POST</code></li><li>是否验证jwt：是</li><li>输入参数：<ul><li><code>content</code>：帖子的内容</li></ul></li><li>返回结果：<code>result: success</code></li></ul><h2 id="7-删除一个帖子"><a href="#7-删除一个帖子" class="headerlink" title="7. 删除一个帖子"></a>7. 删除一个帖子</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/post/">https://app165.acapp.acwing.com.cn/myspace/post/</a></li><li>方法：<code>DELETE</code></li><li>是否验证jwt：是</li><li>输入参数：<ul><li><code>post_id</code>：被删除帖子的ID</li></ul></li><li>返回结果：<code>result: success</code></li></ul><h2 id="8-更改关注状态"><a href="#8-更改关注状态" class="headerlink" title="8. 更改关注状态"></a>8. 更改关注状态</h2><p>如果未关注，则关注；如果已关注，则取消关注。</p><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/follow/">https://app165.acapp.acwing.com.cn/myspace/follow/</a></li><li>方法：<code>POST</code></li><li>是否验证jwt：是</li><li>输入参数：<ul><li><code>target_id</code>: 被关注的用户ID</li></ul></li><li>返回结果：<code>result: success</code></li></ul><h2 id="9-注册账号"><a href="#9-注册账号" class="headerlink" title="9. 注册账号"></a>9. 注册账号</h2><ul><li>地址：<a href="https://app165.acapp.acwing.com.cn/myspace/user/">https://app165.acapp.acwing.com.cn/myspace/user/</a></li><li>方法：<code>POST</code></li><li>是否验证jwt：否</li><li>输入参数：<ul><li><code>username</code>: 用户名</li><li><code>password</code>：密码</li><li><code>password_confirm</code>：确认密码</li></ul></li><li>返回结果：<ul><li><code>result: success</code></li><li><code>result: 用户名和密码不能为空</code></li><li><code>result: 两个密码不一致</code></li><li><code>result: 用户名已存在</code></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;导航：6-Vue3&quot;&gt;&lt;a href=&quot;#导航：6-Vue3&quot; class=&quot;headerlink&quot; title=&quot;导航：6. Vue3&quot;&gt;&lt;/a&gt;导航：&lt;a href=&quot;../6. Vue3.md&quot;&gt;6. Vue3&lt;/a&gt;&lt;/h2&gt;&lt;h2 id=&quot;1-获取Jso</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/06/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/vue/6.2%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://luyicui.github.io/2024/12/06/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/vue/6.2%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</id>
    <published>2024-12-06T14:39:44.071Z</published>
    <updated>2024-12-06T14:39:44.752Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导航：6-Vue3"><a href="#导航：6-Vue3" class="headerlink" title="导航：6. Vue3"></a>导航：<a href="../6. Vue3.md">6. Vue3</a></h2><h2 id="script部分"><a href="#script部分" class="headerlink" title="script部分"></a>script部分</h2><p><code>export default</code>对象的属性：</p><ul><li><code>name</code>：组件的名称</li><li><code>components</code>：存储<code>&lt;template&gt;</code>中用到的所有组件</li><li><code>props</code>：存储父组件传递给子组件的数据</li><li><code>watch()</code>：当某个数据发生变化时触发</li><li><code>computed</code>：动态计算某个数据</li><li><code>setup(props, context)</code>：初始化变量、函数<ul><li><code>ref</code>定义变量，可以用<code>.value</code>属性重新赋值</li><li><code>reactive</code>定义对象，不可重新赋值</li><li><code>props</code>存储父组件传递过来的数据</li><li><code>context.emit()</code>：触发父组件绑定的函数</li></ul></li></ul><h2 id="template部分"><a href="#template部分" class="headerlink" title="template部分"></a>template部分</h2><ul><li><code>&lt;slot&gt;&lt;/slot&gt;</code>：存放父组件传过来的<code>children</code>。</li><li><code>v-on:click</code>或<code>@click</code>属性：绑定事件</li><li><code>v-if</code>、<code>v-else</code>、<code>v-else-if</code>属性：判断</li><li><code>v-for</code>属性：循环，<code>:key</code>循环的每个元素需要有唯一的<code>key</code></li><li><code>v-bind:</code>或<code>:</code>：绑定属性</li></ul><h2 id="style部分"><a href="#style部分" class="headerlink" title="style部分"></a>style部分</h2><ul><li><code>&lt;style&gt;</code>标签添加`属性后，不同组件间的css不会相互影响。</li></ul><h2 id="第三方组件"><a href="#第三方组件" class="headerlink" title="第三方组件"></a>第三方组件</h2><ul><li><code>view-router</code>包：实现路由功能。</li><li><code>vuex</code>：存储全局状态，全局唯一。<ul><li><code>state</code>: 存储所有数据，可以用<code>modules</code>属性划分成若干模块</li><li><code>getters</code>：根据<code>state</code>中的值计算新的值</li><li><code>mutations</code>：所有对<code>state</code>的修改操作都需要定义在这里，不支持异步，可以通过<code>$store.commit()</code>触发</li><li><code>actions</code>：定义对<code>state</code>的复杂修改操作，支持异步，可以通过<code>$store.dispatch()</code>触发。注意不能直接修改<code>state</code>，只能通过<code>mutations</code>修改<code>state</code>。</li><li><code>modules</code>：定义<code>state</code>的子模块</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;导航：6-Vue3&quot;&gt;&lt;a href=&quot;#导航：6-Vue3&quot; class=&quot;headerlink&quot; title=&quot;导航：6. Vue3&quot;&gt;&lt;/a&gt;导航：&lt;a href=&quot;../6. Vue3.md&quot;&gt;6. Vue3&lt;/a&gt;&lt;/h2&gt;&lt;h2 id=&quot;script部</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/12/06/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/vue/6.1%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83/"/>
    <id>https://luyicui.github.io/2024/12/06/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/vue/6.1%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83/</id>
    <published>2024-12-06T14:39:44.059Z</published>
    <updated>2024-12-06T14:39:44.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导航：6-Vue3"><a href="#导航：6-Vue3" class="headerlink" title="导航：6. Vue3"></a>导航：<a href="../6. Vue3.md">6. Vue3</a></h2><p><a href="https://vuejs.org/">Vue官网</a></p><h2 id="终端"><a href="#终端" class="headerlink" title="终端"></a>终端</h2><p><code>Linux</code>和<code>Mac</code>上可以用自带的终端。</p><p><code>Windows</code>上推荐用<code>powershell</code>或者<code>cmd</code>。<code>Git Bash</code>有些指令不兼容。</p><h2 id="安装Nodejs"><a href="#安装Nodejs" class="headerlink" title="安装Nodejs"></a>安装<code>Nodejs</code></h2><p><a href="https://nodejs.org/en/">安装地址</a></p><h2 id="安装-vue-cli"><a href="#安装-vue-cli" class="headerlink" title="安装@vue/cli"></a>安装<code>@vue/cli</code></h2><p>打开<code>Git Bash</code>，执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i -g @vue/cli</span><br></pre></td></tr></table></figure><p>如果执行后面的操作有bug，可能是最新版有问题，可以尝试安装早期版本，比如：<code>npm i -g @vue/cli@4</code></p><h2 id="启动vue自带的图形化项目管理界面"><a href="#启动vue自带的图形化项目管理界面" class="headerlink" title="启动vue自带的图形化项目管理界面"></a>启动<code>vue</code>自带的图形化项目管理界面</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vue ui</span><br></pre></td></tr></table></figure><p>常见问题1：Windows上运行<code>vue</code>，提示无法加载文件，表示用户权限不足。</p><p>解决方案：用管理员身份打开终端，输入<code>set-ExecutionPolicy RemoteSigned</code>，然后输入<code>y</code></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;导航：6-Vue3&quot;&gt;&lt;a href=&quot;#导航：6-Vue3&quot; class=&quot;headerlink&quot; title=&quot;导航：6. Vue3&quot;&gt;&lt;/a&gt;导航：&lt;a href=&quot;../6. Vue3.md&quot;&gt;6. Vue3&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;htt</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/11/27/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/%E3%80%900.Reference%E3%80%91/"/>
    <id>https://luyicui.github.io/2024/11/27/[object%20Object]/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91/%E3%80%900.Reference%E3%80%91/</id>
    <published>2024-11-27T09:32:44.309Z</published>
    <updated>2024-12-31T06:58:08.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【0-Reference】"><a href="#【0-Reference】" class="headerlink" title="【0.Reference】"></a>【0.Reference】</h1><ul><li><a href="https://developer.mozilla.org/zh-CN/">MDN Web Docs</a></li></ul><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202412061056151.png" alt="image-20241206105627960"></p><ul><li><a href="https://www.runoob.com/">菜鸟教程 - 学的不仅是技术，更是梦想！</a></li></ul><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202412061054007.png" alt="image-20241206105434913"></p><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202412061055814.png" alt="image-20241206105508651"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;【0-Reference】&quot;&gt;&lt;a href=&quot;#【0-Reference】&quot; class=&quot;headerlink&quot; title=&quot;【0.Reference】&quot;&gt;&lt;/a&gt;【0.Reference】&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://dev</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/11/24/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/Python_Data_Science/%E3%80%90numpy%E3%80%91/astype()/"/>
    <id>https://luyicui.github.io/2024/11/24/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/Python_Data_Science/%E3%80%90numpy%E3%80%91/astype()/</id>
    <published>2024-11-24T06:55:36.030Z</published>
    <updated>2024-12-20T09:26:39.894Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://luyicui.github.io/2024/10/20/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/zip%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%83%E9%87%8D%E5%A2%83%E7%95%8C/"/>
    <id>https://luyicui.github.io/2024/10/20/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/python/zip%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%83%E9%87%8D%E5%A2%83%E7%95%8C/</id>
    <published>2024-10-20T06:40:35.107Z</published>
    <updated>2024-12-30T13:58:53.314Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>【二叉树】</title>
    <link href="https://luyicui.github.io/2024/08/23/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/C++/%E7%AE%97%E6%B3%95/%E3%80%90%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91/"/>
    <id>https://luyicui.github.io/2024/08/23/[object%20Object]/%E3%80%90%E5%90%8E%E7%AB%AF%E3%80%91/C++/%E7%AE%97%E6%B3%95/%E3%80%90%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91/</id>
    <published>2024-08-22T17:02:27.000Z</published>
    <updated>2024-11-13T14:13:34.984Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h1><h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r;<span class="comment">// l[i] 和 r[i] 分别存储节点 i 的左、右孩子编号</span></span><br></pre></td></tr></table></figure><blockquote><p>  不定义为 <code>int l[N], r[N]</code> 的原因是：二叉树的结点个数最大为 N，但是结点权值可以大于 N，此时就会导致段错误，而定义成哈希表就避免了很多麻烦</p></blockquote><h2 id="非递归遍历"><a href="#非递归遍历" class="headerlink" title="非递归遍历"></a>非递归遍历</h2><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>【<strong>结论</strong>】用 <strong>栈</strong> 模拟实现 <strong>中序遍历</strong>，<font color='red'> <strong>Push</strong> </font> 操作的数据过程是 <font color='blue'> <strong>先序</strong> </font> 遍历，<font color='red'> <strong>Pop</strong> </font> 操作的数据过程是 <font color='blue'> <strong>中序</strong> </font> 遍历</p><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408232259283.png" alt="树的遍历.png"></p><p>​    如图所示，⊗ 是先序遍历，☆ 是中序遍历，△ 是后序遍历。我们发现：树的 <strong>前序、中序、后序</strong> 实际上都是将整棵树以 <strong>上图所示的路线</strong> 跑了 $1$ 遍，每个结点都碰到了 $3$ 次，三者唯一不同之处在于 <strong>访问节点的时机不同</strong></p><ul><li><strong>先序</strong> 遍历在第 $1$ 次碰到结点时访问</li><li><strong>中序</strong> 遍历在第 $2$ 次碰到结点时访问</li><li><strong>后序</strong> 遍历在第 $3$ 次碰到结点时访问</li></ul><h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><ul><li><a href="https://www.acwing.com/activity/content/code/content/8792912/">AcWing 1576. 再次树遍历 - AcWing</a></li></ul><span id="more"></span><h2 id="层序遍历"><a href="#层序遍历" class="headerlink" title="层序遍历"></a>层序遍历</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> q[N];</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">bfs</span><span class="params">(<span class="type">int</span> root)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> hh = <span class="number">0</span>, tt = <span class="number">-1</span>;</span><br><span class="line">    q[++ tt] = root;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(hh &lt;= tt)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">auto</span> t = q[hh ++];</span><br><span class="line">        <span class="keyword">if</span>(l.count(t))<span class="comment">// 存在左孩子</span></span><br><span class="line">            q[++ tt] = l[t];</span><br><span class="line">        <span class="keyword">if</span>(r.count(t))<span class="comment">// 存在右孩子</span></span><br><span class="line">            q[++ tt] = r[t];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bfs(root);</span><br><span class="line"><span class="comment">// 输出层序序列</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, q[i]);</span><br></pre></td></tr></table></figure><h2 id="后序中序建树"><a href="#后序中序建树" class="headerlink" title="后序中序建树"></a>后序中序建树</h2><h3 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h3><ul><li><strong>时间复杂度</strong>：$O(n)$</li></ul><blockquote><p>  注意：前提是二叉树中 <strong>节点编号或权值互不相同</strong>，我们才能用 <strong>哈希表</strong> 记录中序序列各节点对应的下标，从而将时间复杂度优化为 $O(n)$。如果 <strong>二叉树节点编号或权值可能重复</strong>，则只能遍历搜索位置，此时时间复杂度为 $O(n^2)$</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> in[N], post[N];<span class="comment">// n 个节点的中序序列、后序序列</span></span><br><span class="line"><span class="type">int</span> pre[N], cnt;<span class="comment">// 建图的同时记录 n 个节点的前序序列</span></span><br><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r, pos;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">build</span><span class="params">(<span class="type">int</span> il, <span class="type">int</span> ir, <span class="type">int</span> pl, <span class="type">int</span> pr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> root = post[pr];</span><br><span class="line">    <span class="type">int</span> k = pos[root];<span class="comment">// 优化时间复杂度 O(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 记录前序序列 */</span></span><br><span class="line">    <span class="comment">// pre[cnt ++] = root;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(il &lt; k)</span><br><span class="line">        l[root] = build(il, k - <span class="number">1</span>, pl, pl + (k - <span class="number">1</span> - il));</span><br><span class="line">    <span class="keyword">if</span>(k &lt; ir)</span><br><span class="line">        r[root] = build(k + <span class="number">1</span>, ir, pl + (k - <span class="number">1</span> - il) + <span class="number">1</span>, pr - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;post[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;in[i]);</span><br><span class="line">        pos[in[i]] = i;        <span class="comment">// 哈希表记录每个数在中序遍历的下标</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// root 为二叉树的根节点</span></span><br><span class="line">    <span class="type">int</span> root = build(<span class="number">0</span>, n - <span class="number">1</span>, <span class="number">0</span>, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h3><ul><li><a href="https://www.acwing.com/activity/content/code/content/8790598/">AcWing 1497. 树的遍历 - AcWing</a></li><li><a href="https://www.acwing.com/activity/content/code/content/8790623/">AcWing 1620. Z 字形遍历二叉树 - AcWing</a></li></ul><h2 id="中序建树"><a href="#中序建树" class="headerlink" title="中序建树"></a>中序建树</h2><h3 id="模板-1"><a href="#模板-1" class="headerlink" title="模板"></a>模板</h3><ul><li><strong>时间复杂度</strong>：$O(n)$</li></ul><blockquote><p>  注意：前提是二叉树中每个节点的权值互不相同，我们才能用哈希表记录中序序列各节点对应的下标</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> in[N], pre[N];<span class="comment">// n 个节点的中序序列、前序序列</span></span><br><span class="line"><span class="type">int</span> post[N], cnt;<span class="comment">// 建图的同时记录 n 个节点的后序序列</span></span><br><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r, pos;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">build</span><span class="params">(<span class="type">int</span> il, <span class="type">int</span> ir, <span class="type">int</span> pl, <span class="type">int</span> pr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> root = pre[pl];</span><br><span class="line">    <span class="type">int</span> k = pos[root];<span class="comment">// 优化时间复杂度 O(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(il &lt; k)</span><br><span class="line">        l[root] = build(il, k - <span class="number">1</span>, pl + <span class="number">1</span>, pl + <span class="number">1</span> + k - <span class="number">1</span> - il);</span><br><span class="line">    <span class="keyword">if</span>(k &lt; ir)</span><br><span class="line">        r[root] = build(k + <span class="number">1</span>, ir, pl + <span class="number">1</span> + k - <span class="number">1</span> - il + <span class="number">1</span>, pr);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 记录后序序列 */</span></span><br><span class="line">    <span class="comment">// post[cnt ++] = root;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;pre[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;in[i]);</span><br><span class="line">        pos[in[i]] = i;        <span class="comment">// 哈希表记录每个数在中序遍历的下标</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// root 为二叉树的根节点</span></span><br><span class="line">    <span class="type">int</span> root = build(<span class="number">0</span>, n - <span class="number">1</span>, <span class="number">0</span>, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题-2"><a href="#例题-2" class="headerlink" title="例题"></a>例题</h3><ul><li><a href="https://www.acwing.com/activity/content/code/content/8790607/">AcWing 1631. 后序遍历 - AcWing</a></li><li><a href="https://www.acwing.com/solution/content/251933/">AcWing 2019 清华软院 T2. 二叉树算权 - AcWing</a></li></ul><h2 id="前序和后序"><a href="#前序和后序" class="headerlink" title="前序和后序"></a>前序和后序</h2><p>已知二叉树的前序序列和后序序列，<strong>无法唯一确定</strong> 这个二叉树，但是我们可以确定每个子树的形状和个数，仅仅是子树的位置不能确定</p><ul><li><a href="https://www.acwing.com/activity/content/code/content/8793054/">AcWing 1609. 前序和后序遍历 - AcWing</a></li><li><a href="https://www.acwing.com/solution/content/251849/">AcWing 3486. 前序和后序 - AcWing</a></li></ul><h1 id="完全二叉树"><a href="#完全二叉树" class="headerlink" title="完全二叉树"></a>完全二叉树</h1><h2 id="存储-1"><a href="#存储-1" class="headerlink" title="存储"></a>存储</h2><p>完全二叉树采用 <strong>数组</strong> 存储</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br></pre></td></tr></table></figure><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>完全二叉树的性质如下：</p><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408230026116.jpeg" alt="7476a7cbe9d1fbd33df03cf13ab37b4"></p><ol><li><strong><font color='red'> 从 1 号单元开始存储树节点 </font></strong>（0 号单元存节点的个数）</li><li>节点 $i$ 左子树是 $2i$，右子树是 $2i+1$，根节点是 $\lfloor i/2 \rfloor$，左兄弟是 $i-1$，右兄弟 $i+1$</li><li>第 $d$ 层最多有 $2^{d-1}$，<strong>起始节点</strong> 编号为 $2^{d-1}$（$d$ 从 $1$ 开始）</li><li>如果完全二叉树一共有 $n$ 个节点，则 <strong>非</strong> 叶子节点为 $T[0…n/2]$，叶节点为 $T[n/2+1,…,n]$</li></ol><h2 id="例题-3"><a href="#例题-3" class="headerlink" title="例题"></a>例题</h2><ul><li><a href="https://www.acwing.com/solution/content/251776/">AcWing 1240. 完全二叉树的权值 - AcWing</a></li></ul><h1 id="二叉搜索树-BST"><a href="#二叉搜索树-BST" class="headerlink" title="二叉搜索树(BST)"></a>二叉搜索树(BST)</h1><p>==<strong><font color='blue'> 二叉搜索树 </font></strong> == <strong><font color='blue'> 二叉查找树 </font></strong> （Binary Search Tree） == <strong><font color='blue'> 二叉排序树 </font></strong> （Binary Sort Tree）==</p><ul><li>若它的左子树不空，则 <strong>左子树</strong> 上 <strong>所有</strong> 结点的值均 <font color='red'> <strong>小于</strong> </font> 根结点的值;</li><li>若它的右子树不空，则 <strong>右子树</strong> 上 <strong>所有</strong> 结点的值均 <font color='red'> <strong>大于等于</strong> </font> 根结点的值;</li><li>它的左、右子树也都分别是 <strong>二又搜索树</strong></li></ul><blockquote><p>  注意：上述定义在不同题目中，等号的位置可能不一样（即也有可能左子树均小于等于根节点，右子树均大于根节点）</p></blockquote><h2 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h2><ul><li><strong>二叉排序树</strong> 的 <strong><font color='red'> 中序遍历 </font></strong> 是 <strong><font color='gree'> 递增 </font></strong> 序列</li></ul><blockquote><p>  在构造二叉排序树时，若关键字序列有序，则二叉排序树的高度最大</p></blockquote><h2 id="例题-4"><a href="#例题-4" class="headerlink" title="例题"></a>例题</h2><ul><li><a href="https://www.acwing.com/activity/content/code/content/8790764/">AcWing 1527. 判断二叉搜索树 - AcWing</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;二叉树&quot;&gt;&lt;a href=&quot;#二叉树&quot; class=&quot;headerlink&quot; title=&quot;二叉树&quot;&gt;&lt;/a&gt;二叉树&lt;/h1&gt;&lt;h2 id=&quot;存储&quot;&gt;&lt;a href=&quot;#存储&quot; class=&quot;headerlink&quot; title=&quot;存储&quot;&gt;&lt;/a&gt;存储&lt;/h2&gt;&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;unordered_map&lt;/span&gt;&amp;lt;&lt;span class=&quot;type&quot;&gt;int&lt;/span&gt;, &lt;span class=&quot;type&quot;&gt;int&lt;/span&gt;&amp;gt; l, r;	&lt;span class=&quot;comment&quot;&gt;// l[i] 和 r[i] 分别存储节点 i 的左、右孩子编号&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;  不定义为 &lt;code&gt;int l[N], r[N]&lt;/code&gt; 的原因是：二叉树的结点个数最大为 N，但是结点权值可以大于 N，此时就会导致段错误，而定义成哈希表就避免了很多麻烦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;非递归遍历&quot;&gt;&lt;a href=&quot;#非递归遍历&quot; class=&quot;headerlink&quot; title=&quot;非递归遍历&quot;&gt;&lt;/a&gt;非递归遍历&lt;/h2&gt;&lt;h3 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h3&gt;&lt;p&gt;【&lt;strong&gt;结论&lt;/strong&gt;】用 &lt;strong&gt;栈&lt;/strong&gt; 模拟实现 &lt;strong&gt;中序遍历&lt;/strong&gt;，&lt;font color=&#39;red&#39;&gt; &lt;strong&gt;Push&lt;/strong&gt; &lt;/font&gt; 操作的数据过程是 &lt;font color=&#39;blue&#39;&gt; &lt;strong&gt;先序&lt;/strong&gt; &lt;/font&gt; 遍历，&lt;font color=&#39;red&#39;&gt; &lt;strong&gt;Pop&lt;/strong&gt; &lt;/font&gt; 操作的数据过程是 &lt;font color=&#39;blue&#39;&gt; &lt;strong&gt;中序&lt;/strong&gt; &lt;/font&gt; 遍历&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408232259283.png&quot; alt=&quot;树的遍历.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;​    如图所示，⊗ 是先序遍历，☆ 是中序遍历，△ 是后序遍历。我们发现：树的 &lt;strong&gt;前序、中序、后序&lt;/strong&gt; 实际上都是将整棵树以 &lt;strong&gt;上图所示的路线&lt;/strong&gt; 跑了 $1$ 遍，每个结点都碰到了 $3$ 次，三者唯一不同之处在于 &lt;strong&gt;访问节点的时机不同&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;先序&lt;/strong&gt; 遍历在第 $1$ 次碰到结点时访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中序&lt;/strong&gt; 遍历在第 $2$ 次碰到结点时访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后序&lt;/strong&gt; 遍历在第 $3$ 次碰到结点时访问&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;例题&quot;&gt;&lt;a href=&quot;#例题&quot; class=&quot;headerlink&quot; title=&quot;例题&quot;&gt;&lt;/a&gt;例题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.acwing.com/activity/content/code/content/8792912/&quot;&gt;AcWing 1576. 再次树遍历 - AcWing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="算法" scheme="https://luyicui.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam详解</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</id>
    <published>2024-08-13T02:02:27.000Z</published>
    <updated>2024-11-13T14:14:43.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Adam-算法详解"><a href="#Adam-算法详解" class="headerlink" title="Adam 算法详解"></a>Adam 算法详解</h1><p>Adam 算法在 RMSProp 算法基础上对 <a href="https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;spm=1001.2101.3001.7020">小批量</a> 随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。</p><blockquote><p>所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。adam 算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶 moment 和二阶 moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了 Adagrad 算法的处理稀疏型数据，又结合了 RMSProp 算法的可以处理非稳态的数据。</p></blockquote><p>小 tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个 <a href="https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;vd_source=12c80a98ec9426002a2f54318421082c">优化算法系列合集</a>，个人觉得比较容易听懂</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>Adam 算法使用了动量变量 $\boldsymbol{v}_t$ ​和 RMSProp 算法中小批量随机梯度按元素平方的指数加权移动平均变量 $\boldsymbol{s}_t$ ​，并在时间步 $0$ 将它们中每个元素初始化为 $0$。给定超参数 $0 \leq \beta_1 &lt; 1$ （算法作者建议设为 $0.9$），时间步 $t$ 的动量变量 $\boldsymbol{v}_t$ ​即小批量随机梯度 $\boldsymbol{g}_t$ ​的指数加权移动平均：</p><script type="math/tex; mode=display">\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t</script><p>和 RMSProp 算法中一样，给定超参数 $0 \leq \beta_2 &lt; 1$ （算法作者建议设为 0.999）</p><p>将小批量随机梯度按元素平方后的项 $\boldsymbol{g}_t \odot \boldsymbol{g}_t$ ​做指数加权移动平均得到 $\boldsymbol{s}_t$​：</p><script type="math/tex; mode=display">\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t</script><p>由于我们将 $\boldsymbol{v}_0$ 和 $\boldsymbol{s}_0$ 中的元素都初始化为 $0$</p><p>在时间步 $t$ 我们得到 $\boldsymbol{v}<em>t = (1-\beta_1) \sum</em>{i=1}^t \beta<em>1^{t-i} \boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\beta_1) \sum</em>{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t$。需要注意的是，当 $t$ 较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 $\beta_1 = 0.9$ 时，$\boldsymbol{v}_1 = 0.1\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步 $t$ ，我们可以将 $\boldsymbol{v}_t$ 再除以 $1 - \beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为 1。这也叫作 <strong>偏差修正</strong>。在 Adam 算法中，我们对变量 $\boldsymbol{v}_t$ 和 $\boldsymbol{s}_t$ 均作偏差修正：</p><script type="math/tex; mode=display">\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}</script><script type="math/tex; mode=display">\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}</script><p>接下来，Adam 算法使用以上偏差修正后的变量 $\hat{\boldsymbol{v}}_t $ 和 $\hat{\boldsymbol{s}}_t$ ，将模型参数中每个元素的学习率通过按元素运算重新调整：</p><script type="math/tex; mode=display">\boldsymbol{g}_t' \leftarrow \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}</script><p>其中 $\eta$ 是学习率，$\epsilon$ 是为了维持数值稳定性而添加的常数，如 $10^{-8}$ 。和 AdaGrad 算法、RMSProp 算法以及 AdaDelta 算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用 $\boldsymbol{g}_t’$ ​迭代自变量：</p><script type="math/tex; mode=display">\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t'</script><span id="more"></span><h2 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h2><p>我们按照 Adam 算法中的公式实现该算法。其中时间步 t t t 通过 <code>hyperparams</code> 参数传入 <code>adam</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>():</span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>使用学习率为 0.01 的 Adam 算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.245370, 0.065155 sec per epoch</span><br></pre></td></tr></table></figure><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202411132136580.png" alt="在这里插入图片描述"></p><h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><p>通过名称为“Adam”的优化器实例，我们便可使用 PyTorch 提供的 Adam 算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.242066, 0.056867 sec per epoch</span><br></pre></td></tr></table></figure><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202411132136204.png" alt="在这里插入图片描述"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>Adam 算法在 RMSProp 算法的基础上对小批量随机梯度也做了指数加权移动平均。</li><li>Adam 算法使用了偏差修正。</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv: 1412.6980.</p><hr><blockquote><p>注：除代码外本节与原书此节基本相同，<a href="https://zh.d2l.ai/chapter_optimization/adam.html">原书传送门</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Adam-算法详解&quot;&gt;&lt;a href=&quot;#Adam-算法详解&quot; class=&quot;headerlink&quot; title=&quot;Adam 算法详解&quot;&gt;&lt;/a&gt;Adam 算法详解&lt;/h1&gt;&lt;p&gt;Adam 算法在 RMSProp 算法基础上对 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;amp;spm=1001.2101.3001.7020&quot;&gt;小批量&lt;/a&gt; 随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。adam 算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶 moment 和二阶 moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了 Adagrad 算法的处理稀疏型数据，又结合了 RMSProp 算法的可以处理非稳态的数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;小 tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个 &lt;a href=&quot;https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;amp;vd_source=12c80a98ec9426002a2f54318421082c&quot;&gt;优化算法系列合集&lt;/a&gt;，个人觉得比较容易听懂&lt;/p&gt;
&lt;h2 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算法&quot;&gt;&lt;/a&gt;算法&lt;/h2&gt;&lt;p&gt;Adam 算法使用了动量变量 $&#92;boldsymbol{v}_t$ ​和 RMSProp 算法中小批量随机梯度按元素平方的指数加权移动平均变量 $&#92;boldsymbol{s}_t$ ​，并在时间步 $0$ 将它们中每个元素初始化为 $0$。给定超参数 $0 &#92;leq &#92;beta_1 &amp;lt; 1$ （算法作者建议设为 $0.9$），时间步 $t$ 的动量变量 $&#92;boldsymbol{v}_t$ ​即小批量随机梯度 $&#92;boldsymbol{g}_t$ ​的指数加权移动平均：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;boldsymbol{v}_t &#92;leftarrow &#92;beta_1 &#92;boldsymbol{v}_{t-1} + (1 - &#92;beta_1) &#92;boldsymbol{g}_t&lt;/script&gt;&lt;p&gt;和 RMSProp 算法中一样，给定超参数 $0 &#92;leq &#92;beta_2 &amp;lt; 1$ （算法作者建议设为 0.999）&lt;/p&gt;
&lt;p&gt;将小批量随机梯度按元素平方后的项 $&#92;boldsymbol{g}_t &#92;odot &#92;boldsymbol{g}_t$ ​做指数加权移动平均得到 $&#92;boldsymbol{s}_t$​：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;boldsymbol{s}_t &#92;leftarrow &#92;beta_2 &#92;boldsymbol{s}_{t-1} + (1 - &#92;beta_2) &#92;boldsymbol{g}_t &#92;odot &#92;boldsymbol{g}_t&lt;/script&gt;&lt;p&gt;由于我们将 $&#92;boldsymbol{v}_0$ 和 $&#92;boldsymbol{s}_0$ 中的元素都初始化为 $0$&lt;/p&gt;
&lt;p&gt;在时间步 $t$ 我们得到 $&#92;boldsymbol{v}&lt;em&gt;t = (1-&#92;beta_1) &#92;sum&lt;/em&gt;{i=1}^t &#92;beta&lt;em&gt;1^{t-i} &#92;boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-&#92;beta_1) &#92;sum&lt;/em&gt;{i=1}^t &#92;beta_1^{t-i} = 1 - &#92;beta_1^t$。需要注意的是，当 $t$ 较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 $&#92;beta_1 = 0.9$ 时，$&#92;boldsymbol{v}_1 = 0.1&#92;boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步 $t$ ，我们可以将 $&#92;boldsymbol{v}_t$ 再除以 $1 - &#92;beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为 1。这也叫作 &lt;strong&gt;偏差修正&lt;/strong&gt;。在 Adam 算法中，我们对变量 $&#92;boldsymbol{v}_t$ 和 $&#92;boldsymbol{s}_t$ 均作偏差修正：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;hat{&#92;boldsymbol{v}}_t &#92;leftarrow &#92;frac{&#92;boldsymbol{v}_t}{1 - &#92;beta_1^t}&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;hat{&#92;boldsymbol{s}}_t &#92;leftarrow &#92;frac{&#92;boldsymbol{s}_t}{1 - &#92;beta_2^t}&lt;/script&gt;&lt;p&gt;接下来，Adam 算法使用以上偏差修正后的变量 $&#92;hat{&#92;boldsymbol{v}}_t $ 和 $&#92;hat{&#92;boldsymbol{s}}_t$ ，将模型参数中每个元素的学习率通过按元素运算重新调整：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;boldsymbol{g}_t&#39; &#92;leftarrow &#92;frac{&#92;eta &#92;hat{&#92;boldsymbol{v}}_t}{&#92;sqrt{&#92;hat{&#92;boldsymbol{s}}_t} + &#92;epsilon}&lt;/script&gt;&lt;p&gt;其中 $&#92;eta$ 是学习率，$&#92;epsilon$ 是为了维持数值稳定性而添加的常数，如 $10^{-8}$ 。和 AdaGrad 算法、RMSProp 算法以及 AdaDelta 算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用 $&#92;boldsymbol{g}_t’$ ​迭代自变量：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;boldsymbol{x}_t &#92;leftarrow &#92;boldsymbol{x}_{t-1} - &#92;boldsymbol{g}_t&#39;&lt;/script&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/</id>
    <published>2024-08-13T01:02:27.000Z</published>
    <updated>2024-11-13T14:14:39.956Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Adam-算法"><a href="#Adam-算法" class="headerlink" title="Adam 算法"></a>Adam 算法</h1><p>​    接下来，我们将介绍目前常用的梯度下降法中的王者——Adam 算法。Adam（Adaptive Moment Estimation）是目前深度学习中最常用的优化算法之一。Adam 算法的核心思想是 <strong>利用梯度一阶动量和二阶动量来动态自适应调整学习率</strong>，既保持了 <strong>Momentum 收敛速度快</strong> 的优点，又结合了 <strong>RMSProp 自适应学习率</strong> 的优点</p><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p>Adam 算法通过计算梯度的 <strong>一阶动量</strong>（即 <strong>梯度的指数加权移动平均）</strong> 和梯度的 <strong>二阶动量</strong>（即 <strong>梯度平方的指数加权移动平均</strong>）来 <strong>动态调整</strong> 每个参数的 <strong>学习率</strong>。具体公式如下：</p><ol><li>梯度的一阶动量：  </li></ol><script type="math/tex; mode=display">m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t</script><ol><li>梯度的二阶动量：  </li></ol><script type="math/tex; mode=display">v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2</script><ol><li>偏差修正：  </li></ol><script type="math/tex; mode=display">\hat{m}_t = \frac{m_t}{1 - \beta_1^t}</script><script type="math/tex; mode=display">\hat{v}_t = \frac{v_t}{1 - \beta_2^t}</script><ol><li>更新参数：  </li></ol><script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\alpha \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}</script><p>其中：  </p><ul><li>$\beta_1$ 和 $\beta_2$ 分别是 <strong>动量</strong> 和 <strong>均方根动量</strong> 的衰减率，常用值为 $\beta_1 = 0.9$ 和 $\beta_2 = 0.999$</li><li>$\epsilon$ 是一个很小的常数，用于防止分母为零，常用值为 $10^{-8}$</li></ul><span id="more"></span><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点</strong>：  </p><ul><li><strong>自适应调整学习率</strong>：根据一阶动量和二阶动量动态调整每个参数的学习率，使得训练过程更加稳定。</li><li><strong>收敛速度快</strong>：结合动量法的 <strong>加速特性</strong> 和 RMSProp 的 <strong>平稳特性</strong>，能够快速收敛到最优解。</li><li>能处理 <strong>稀疏梯度</strong>，适用于大规模数据和参数。  </li></ul><p><strong>缺点</strong>：  </p><ul><li>对于某些特定问题，Adam 可能会出现不稳定的收敛行为。  </li><li>参数较多：Adam 算法需要调整的 <strong>超参数较多</strong>（例如 $\beta_1$ , $\beta_2$ , $\epsilon$），调参复杂度高。</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义Adam优化器</span></span><br><span class="line">optimizer = torch.optim.Adam([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adam&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Adam-与其他算法的比较"><a href="#Adam-与其他算法的比较" class="headerlink" title="Adam 与其他算法的比较"></a>Adam 与其他算法的比较</h2><p>Adam 算法集成了 SGD、动量法、Adagrad、Adadelta 等多种优化算法的优点，具有快速收敛和稳定的特点。以下是它与其他算法的对比：</p><ol><li>SGD：基本的随机梯度下降法，收敛速度较慢，易陷入局部最优。</li><li>动量法：在 SGD 基础上加入一阶动量，加速收敛，但仍然可能陷入局部最优。</li><li>Adagrad：自适应学习率，但对历史梯度的累积会导致学习率不断减小，后期训练缓慢。</li><li>RMSProp：改进了 Adagrad，通过引入衰减系数解决学习率不断减小的问题。</li><li>Adam：结合动量法和 RMSProp 的优点，具有快速收敛和稳定的特点，是目前最常用的优化算法。</li></ol><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Adam 算法作为一种自适应的梯度下降优化算法，结合了动量法和 RMSProp 的优点，能够有效地加速模型的收敛，同时保持稳定性。它通过计算一阶和二阶动量来动态调整学习率，使得模型在训练过程中能够快速收敛，并适应不同的优化问题。尽管 Adam 需要调整的超参数较多，但其优越的性能使得它成为深度学习中最广泛使用的优化算法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Adam-算法&quot;&gt;&lt;a href=&quot;#Adam-算法&quot; class=&quot;headerlink&quot; title=&quot;Adam 算法&quot;&gt;&lt;/a&gt;Adam 算法&lt;/h1&gt;&lt;p&gt;​    接下来，我们将介绍目前常用的梯度下降法中的王者——Adam 算法。Adam（Adaptive Moment Estimation）是目前深度学习中最常用的优化算法之一。Adam 算法的核心思想是 &lt;strong&gt;利用梯度一阶动量和二阶动量来动态自适应调整学习率&lt;/strong&gt;，既保持了 &lt;strong&gt;Momentum 收敛速度快&lt;/strong&gt; 的优点，又结合了 &lt;strong&gt;RMSProp 自适应学习率&lt;/strong&gt; 的优点&lt;/p&gt;
&lt;h2 id=&quot;基本思想&quot;&gt;&lt;a href=&quot;#基本思想&quot; class=&quot;headerlink&quot; title=&quot;基本思想&quot;&gt;&lt;/a&gt;基本思想&lt;/h2&gt;&lt;p&gt;Adam 算法通过计算梯度的 &lt;strong&gt;一阶动量&lt;/strong&gt;（即 &lt;strong&gt;梯度的指数加权移动平均）&lt;/strong&gt; 和梯度的 &lt;strong&gt;二阶动量&lt;/strong&gt;（即 &lt;strong&gt;梯度平方的指数加权移动平均&lt;/strong&gt;）来 &lt;strong&gt;动态调整&lt;/strong&gt; 每个参数的 &lt;strong&gt;学习率&lt;/strong&gt;。具体公式如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;梯度的一阶动量：  &lt;/li&gt;
&lt;/ol&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
m_t = &#92;beta_1 m_{t-1} + (1 - &#92;beta_1) g_t&lt;/script&gt;&lt;ol&gt;
&lt;li&gt;梯度的二阶动量：  &lt;/li&gt;
&lt;/ol&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
v_t = &#92;beta_2 v_{t-1} + (1 - &#92;beta_2) g_t^2&lt;/script&gt;&lt;ol&gt;
&lt;li&gt;偏差修正：  &lt;/li&gt;
&lt;/ol&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;hat{m}_t = &#92;frac{m_t}{1 - &#92;beta_1^t}&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;hat{v}_t = &#92;frac{v_t}{1 - &#92;beta_2^t}&lt;/script&gt;&lt;ol&gt;
&lt;li&gt;更新参数：  &lt;/li&gt;
&lt;/ol&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
&#92;theta_{t+1} = &#92;theta_t - &#92;frac{&#92;alpha &#92;hat{m}_t}{&#92;sqrt{&#92;hat{v}_t} + &#92;epsilon}&lt;/script&gt;&lt;p&gt;其中：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;beta_1$ 和 $&#92;beta_2$ 分别是 &lt;strong&gt;动量&lt;/strong&gt; 和 &lt;strong&gt;均方根动量&lt;/strong&gt; 的衰减率，常用值为 $&#92;beta_1 = 0.9$ 和 $&#92;beta_2 = 0.999$&lt;/li&gt;
&lt;li&gt;$&#92;epsilon$ 是一个很小的常数，用于防止分母为零，常用值为 $10^{-8}$&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>AdaGrad</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.AdaGrad/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.AdaGrad/</id>
    <published>2024-08-13T00:02:27.000Z</published>
    <updated>2024-11-13T14:14:34.313Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AdaGrad-算法"><a href="#AdaGrad-算法" class="headerlink" title="AdaGrad 算法"></a>AdaGrad 算法</h1><p>在前面我们讲解了 <a href="https://so.csdn.net/so/search?q=%E5%8A%A8%E9%87%8F%E6%B3%95&amp;spm=1001.2101.3001.7020">动量法</a>（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的 AdaGrad 算法。二阶动量的出现，才意味着真正的 <strong>自适应学习率</strong> 优化算法时代的到来。</p><h2 id="AdaGrad-算法的基本思想"><a href="#AdaGrad-算法的基本思想" class="headerlink" title="AdaGrad 算法的基本思想"></a>AdaGrad 算法的基本思想</h2><p>我们先回顾一下传统的 <a href="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>（SGD）及其各种变种。它们都是以 <strong>同样的学习率</strong> 来更新 <strong>每一个参数</strong> 的。但深度神经网络往往包含大量参数，这些参数并不总是 <strong>均匀更新</strong> 的。有些参数更新得频繁，有些则很少更新。</p><ul><li>对于 <strong>经常更新</strong> 的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的 <strong>学习率小一些</strong></li><li>对于 <strong>偶尔更新</strong> 的参数，我们了解的信息较少，希望从每一个样本中多学一些，即 <strong>学习率大一些</strong></li></ul><p>要动态度量历史更新的频率，我们引入 <strong>二阶动量</strong>。二阶动量通过将每一位各自的历史梯度的 <strong>平方</strong> 叠加起来来计算。具体公式如下：  </p><script type="math/tex; mode=display">v_t = v_{t-1} + g_t^2</script><p>其中，$g_t$ 是当前的梯度。</p><span id="more"></span><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol><li><strong>计算当前梯度 $g_t$ </strong>：  </li></ol><script type="math/tex; mode=display">g_t = \nabla f(w_t)</script><ol><li><strong>更新二阶动量 $v_t$ </strong>：  </li></ol><script type="math/tex; mode=display">v_t =  v_{t-1} + g_t^2</script><ol><li><strong>计算当前时刻的下降梯度</strong>：  </li></ol><script type="math/tex; mode=display">w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t} + \epsilon} g_t</script><p>其中，$\alpha$ 是学习率，$\epsilon$ 是一个小的平滑项，防止分母为 0。</p><h2 id="稀疏特征处理"><a href="#稀疏特征处理" class="headerlink" title="稀疏特征处理"></a>稀疏特征处理</h2><p>AdaGrad 算法主要针对 <strong>稀疏特征</strong> 进行了优化。<strong>稀疏特征</strong> 在很多样本中只出现少数几次，在训练模型时，这些稀疏特征的更新很少，但每次更新可能带来较大影响。AdaGrad 通过调整每个特征的学习率，针对这种情况进行了优化。</p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>优点</strong>：  </p><ol><li><strong>有效处理稀疏特征</strong>：自动调整每个参数的学习率，使得稀疏特征的更新更少。  </li><li><strong>加速收敛</strong>：在自动调整学习率的同时，使得模型在训练过程中更快收敛。</li></ol><p><strong>缺点</strong>：  </p><ol><li><strong>学习率逐渐减小</strong>：每次迭代中学习率都会减小，导致训练后期学习率变得非常小，从而使收敛速度变慢。  </li><li><strong>固定调整方式</strong>：对于不同参数，学习率调整方式是固定的，无法根据不同任务自动调整。</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>下面是一个简单的 PyTorch 实现 AdaGrad 算法的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成一些数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 AdaGrad 优化器</span></span><br><span class="line">optimizer = torch.optim.Adagrad([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112049596.png" alt="image-20240811204910528"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本节我们介绍了一种新的梯度下降算法变体——AdaGrad。与动量法相比，它最大的改进在于 <strong>使用二阶动量来动态调整学习率</strong>，能够记住历史上的梯度信息，以动态调整学习率。其主要优点是能够处理稀疏特征问题，但也有学习率逐渐减小和调整方式固定的缺点。</p><p>到目前为止，我们一共讲了五种梯度下降算法。AdaGrad 是 2011 年提出的，而动量法在 1993 年提出，SGD 在 1951 年提出。通过时间轴的对比，我们可以看出人们在不断研究和改进梯度下降算法，从最早的梯度下降法到 SGD，再到动量法、小批量梯度下降，最后到 2011 年的 AdaGrad。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;AdaGrad-算法&quot;&gt;&lt;a href=&quot;#AdaGrad-算法&quot; class=&quot;headerlink&quot; title=&quot;AdaGrad 算法&quot;&gt;&lt;/a&gt;AdaGrad 算法&lt;/h1&gt;&lt;p&gt;在前面我们讲解了 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E5%8A%A8%E9%87%8F%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;动量法&lt;/a&gt;（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的 AdaGrad 算法。二阶动量的出现，才意味着真正的 &lt;strong&gt;自适应学习率&lt;/strong&gt; 优化算法时代的到来。&lt;/p&gt;
&lt;h2 id=&quot;AdaGrad-算法的基本思想&quot;&gt;&lt;a href=&quot;#AdaGrad-算法的基本思想&quot; class=&quot;headerlink&quot; title=&quot;AdaGrad 算法的基本思想&quot;&gt;&lt;/a&gt;AdaGrad 算法的基本思想&lt;/h2&gt;&lt;p&gt;我们先回顾一下传统的 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;随机梯度下降法&lt;/a&gt;（SGD）及其各种变种。它们都是以 &lt;strong&gt;同样的学习率&lt;/strong&gt; 来更新 &lt;strong&gt;每一个参数&lt;/strong&gt; 的。但深度神经网络往往包含大量参数，这些参数并不总是 &lt;strong&gt;均匀更新&lt;/strong&gt; 的。有些参数更新得频繁，有些则很少更新。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;strong&gt;经常更新&lt;/strong&gt; 的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的 &lt;strong&gt;学习率小一些&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对于 &lt;strong&gt;偶尔更新&lt;/strong&gt; 的参数，我们了解的信息较少，希望从每一个样本中多学一些，即 &lt;strong&gt;学习率大一些&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要动态度量历史更新的频率，我们引入 &lt;strong&gt;二阶动量&lt;/strong&gt;。二阶动量通过将每一位各自的历史梯度的 &lt;strong&gt;平方&lt;/strong&gt; 叠加起来来计算。具体公式如下：  &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
v_t = v_{t-1} + g_t^2&lt;/script&gt;&lt;p&gt;其中，$g_t$ 是当前的梯度。&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>RMSProp 和 Adadelta</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.RMSProp%20%E5%92%8C%20Adadelta/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.RMSProp%20%E5%92%8C%20Adadelta/</id>
    <published>2024-08-12T23:02:27.000Z</published>
    <updated>2024-11-13T14:14:37.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RMSProp-和-Adadelta-算法"><a href="#RMSProp-和-Adadelta-算法" class="headerlink" title="RMSProp 和 Adadelta 算法"></a>RMSProp 和 Adadelta 算法</h1><p>​    在 <a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a> 中，RMSProp 和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad 的基础上做了改进，以适应深度学习中的大规模参数优化需求。</p><h2 id="RMSProp-算法"><a href="#RMSProp-算法" class="headerlink" title="RMSProp 算法"></a>RMSProp 算法</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>RMSProp 对 AdaGrad 进行改进，通过引入 <strong>衰减率</strong> 来调整二阶动量的累积。这样可以 <strong>避免</strong> AdaGrad 中 <strong>学习率减小过快</strong> 的问题。</p><p>AdaGrad 的二阶动量计算公式如下：  </p><script type="math/tex; mode=display">v_t = v_{t-1} + g_t^2</script><p>而 RMSProp 采用了带有衰减率的计算方式：  </p><script type="math/tex; mode=display">v_t = \beta v_{t-1} + (1 - \beta) g_t^2</script><p>其中，$\beta$ 是衰减率系数。</p><span id="more"></span><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>优点：</strong>  </p><ul><li><strong>自动调整学习率</strong>，避免学习率过大或过小的问题</li><li><strong>加速收敛速度</strong></li><li><strong>简单适用</strong>，适用于各种优化问题</li></ul><p><strong>缺点：</strong>  </p><ul><li>在处理稀疏特征时不够优秀</li><li>需要调整的超参数较多（衰减率 $\beta$ i 和学习率 $\alpha$ ）</li><li>收敛速度可能不如某些更先进的 <a href="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a></li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 RMSProp 优化器</span></span><br><span class="line">optimizer = torch.optim.RMSprop([w, b], lr=learning_rate, alpha=beta)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with RMSProp&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Adadelta-算法"><a href="#Adadelta-算法" class="headerlink" title="Adadelta 算法"></a>Adadelta 算法</h2><h3 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h3><p>Adadelta 是对 RMSProp 的进一步改进，旨在 <strong>自动调整学习率</strong>，避免手动调参。它通过计算梯度和权重更新量的累积值来调整学习率，使得训练过程更加稳定。</p><p>Adadelta 的公式如下：</p><ol><li>梯度的累积：  </li></ol><script type="math/tex; mode=display">E [g^2] _t = \rho E [g^2]_{t-1} + (1 - \rho) g_t^2</script><ol><li>权重更新量的累积：  </li></ol><script type="math/tex; mode=display">E [\Delta x^2] _t = \rho E [\Delta x^2]_{t-1} + (1 - \rho) (\Delta x_t)^2</script><ol><li>更新参数：  </li></ol><script type="math/tex; mode=display">\Delta x_t = -\frac{\sqrt{E [\Delta x^2]_{t-1} + \epsilon}}{\sqrt{E [g^2]_t + \epsilon}} g_t</script><script type="math/tex; mode=display">\theta_{t+1} = \theta_t + \Delta x_t</script><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>优点：</strong>  </p><ul><li><strong>自动调整学习率</strong>，避免学习率过大或过小的问题</li><li>避免出现学习率饱和现象，使得训练更加稳定</li></ul><p><strong>缺点：</strong>  </p><ul><li>可能收敛较慢</li><li>需要维护梯度和权重更新量的累积值，增加了空间复杂度</li></ul><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">1.0</span>  <span class="comment"># Adadelta 不需要传统的学习率</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">rho = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">1e-6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 Adadelta 优化器</span></span><br><span class="line">optimizer = torch.optim.Adadelta([w, b], rho=rho, eps=epsilon)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adadelta&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;RMSProp-和-Adadelta-算法&quot;&gt;&lt;a href=&quot;#RMSProp-和-Adadelta-算法&quot; class=&quot;headerlink&quot; title=&quot;RMSProp 和 Adadelta 算法&quot;&gt;&lt;/a&gt;RMSProp 和 Adadelta 算法&lt;/h1&gt;&lt;p&gt;​    在 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt; 中，RMSProp 和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad 的基础上做了改进，以适应深度学习中的大规模参数优化需求。&lt;/p&gt;
&lt;h2 id=&quot;RMSProp-算法&quot;&gt;&lt;a href=&quot;#RMSProp-算法&quot; class=&quot;headerlink&quot; title=&quot;RMSProp 算法&quot;&gt;&lt;/a&gt;RMSProp 算法&lt;/h2&gt;&lt;h3 id=&quot;基本思想&quot;&gt;&lt;a href=&quot;#基本思想&quot; class=&quot;headerlink&quot; title=&quot;基本思想&quot;&gt;&lt;/a&gt;基本思想&lt;/h3&gt;&lt;p&gt;RMSProp 对 AdaGrad 进行改进，通过引入 &lt;strong&gt;衰减率&lt;/strong&gt; 来调整二阶动量的累积。这样可以 &lt;strong&gt;避免&lt;/strong&gt; AdaGrad 中 &lt;strong&gt;学习率减小过快&lt;/strong&gt; 的问题。&lt;/p&gt;
&lt;p&gt;AdaGrad 的二阶动量计算公式如下：  &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
v_t = v_{t-1} + g_t^2&lt;/script&gt;&lt;p&gt;而 RMSProp 采用了带有衰减率的计算方式：  &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
v_t = &#92;beta v_{t-1} + (1 - &#92;beta) g_t^2&lt;/script&gt;&lt;p&gt;其中，$&#92;beta$ 是衰减率系数。&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Momentum</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/</id>
    <published>2024-08-12T22:02:27.000Z</published>
    <updated>2024-11-13T14:14:31.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="动量法（Momentum）"><a href="#动量法（Momentum）" class="headerlink" title="动量法（Momentum）"></a>动量法（Momentum）</h1><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><p>在 <a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a> 的优化过程中，梯度下降法（Gradient Descent, GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在 <strong>收敛速度慢</strong>、<strong>容易陷入局部最小值</strong> 以及在 <strong>高维空间中震荡较大</strong> 的问题。为了解决这些问题，人们提出了动量法（Momentum）。</p><h2 id="动量法的概念"><a href="#动量法的概念" class="headerlink" title="动量法的概念"></a>动量法的概念</h2><p>动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：<strong>在运动方向上保持运动的趋势，从而加速收敛</strong>。</p><h2 id="动量法在深度学习中的应用"><a href="#动量法在深度学习中的应用" class="headerlink" title="动量法在深度学习中的应用"></a>动量法在深度学习中的应用</h2><p>在深度学习中，动量法通过记录 <strong>梯度的增量</strong> 并将其与 <strong>当前梯度相加</strong>，来 <strong>平滑梯度下降</strong> 的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。</p><p>动量法的更新公式如下：  </p><script type="math/tex; mode=display">m_t = \beta m_{t-1} + \nabla L(w_t)</script><script type="math/tex; mode=display">w_{t+1} = w_t - \alpha m_t</script><p>其中：  </p><ul><li>$m_t$ 是动量项，记录了之前梯度的累积。  </li><li>$\beta$ 是动量参数，控制 <strong>动量项的衰减</strong>，一般取值为 0.9。  </li><li>$\nabla L(w_t)$ 是当前参数的梯度。</li><li>$\alpha$ 是学习率。</li></ul><span id="more"></span><h2 id="动量法的优点"><a href="#动量法的优点" class="headerlink" title="动量法的优点"></a>动量法的优点</h2><ol><li><strong>加速收敛</strong>：动量法通过积累之前的梯度信息，使得优化过程更为顺畅，避免了曲折路径，提高了收敛速度。  </li><li><strong>跳过局部最小值</strong>：由于动量的累积作用，可以帮助优化算法跳过一些局部最小值，找到更优的解。  </li><li><p><strong>减少振荡</strong>：动量法可以有效减小学习过程中梯度震荡的现象，使得模型的训练更加稳定。</p><h2 id="动量法的缺点"><a href="#动量法的缺点" class="headerlink" title="动量法的缺点"></a>动量法的缺点</h2></li><li><p><strong>计算复杂度增加</strong>：由于需要维护动量项，会导致计算复杂度的增加</p></li><li><strong>参数调节</strong>：动量法引入了新的超参数（动量系数 $\beta$ ），需要在实际应用中进行调节</li></ol><h2 id="动量法的改进及变种"><a href="#动量法的改进及变种" class="headerlink" title="动量法的改进及变种"></a>动量法的改进及变种</h2><p>​    在动量法的基础上，还有一些改进和变种，如 Nesterov 加速梯度（Nesterov Accelerated <a href="https://so.csdn.net/so/search?q=Gradient&amp;spm=1001.2101.3001.7020">Gradient</a>, NAG）、RMSprop、Adam 等。这些方法在动量法的基础上进一步优化了收敛速度和稳定性。</p><h2 id="实验代码示例"><a href="#实验代码示例" class="headerlink" title="实验代码示例"></a>实验代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据生成</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">X = torch.randn(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span> * X.squeeze() + <span class="number">2</span> + torch.randn(<span class="number">1000</span>) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同梯度下降方法的比较</span></span><br><span class="line">methods = &#123;</span><br><span class="line">    <span class="string">&#x27;SGD&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>),</span><br><span class="line">    <span class="string">&#x27;Momentum&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">losses = &#123;method: [] <span class="keyword">for</span> method <span class="keyword">in</span> methods&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> method_name, optimizer_fn <span class="keyword">in</span> methods.items():</span><br><span class="line">    model = LinearModel()</span><br><span class="line">    optimizer = optimizer_fn(model.parameters())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(X)</span><br><span class="line">        loss = criterion(outputs.squeeze(), y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        losses[method_name].append(loss.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line"><span class="keyword">for</span> method_name, loss_values <span class="keyword">in</span> losses.items():</span><br><span class="line">    plt.plot(loss_values, label=method_name)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Loss Curve Comparison&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112029254.png" alt="image-20240811202901191"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>动量法通过引入动量项，显著提高了 <a href="https://so.csdn.net/so/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">梯度下降法</a> 的收敛速度和稳定性。尽管在实际应用中引入了额外的计算开销，但其在许多深度学习任务中的表现优异，已经成为常用的优化方法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;动量法（Momentum）&quot;&gt;&lt;a href=&quot;#动量法（Momentum）&quot; class=&quot;headerlink&quot; title=&quot;动量法（Momentum）&quot;&gt;&lt;/a&gt;动量法（Momentum）&lt;/h1&gt;&lt;h2 id=&quot;背景知识&quot;&gt;&lt;a href=&quot;#背景知识&quot; class=&quot;headerlink&quot; title=&quot;背景知识&quot;&gt;&lt;/a&gt;背景知识&lt;/h2&gt;&lt;p&gt;在 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt; 的优化过程中，梯度下降法（Gradient Descent, GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在 &lt;strong&gt;收敛速度慢&lt;/strong&gt;、&lt;strong&gt;容易陷入局部最小值&lt;/strong&gt; 以及在 &lt;strong&gt;高维空间中震荡较大&lt;/strong&gt; 的问题。为了解决这些问题，人们提出了动量法（Momentum）。&lt;/p&gt;
&lt;h2 id=&quot;动量法的概念&quot;&gt;&lt;a href=&quot;#动量法的概念&quot; class=&quot;headerlink&quot; title=&quot;动量法的概念&quot;&gt;&lt;/a&gt;动量法的概念&lt;/h2&gt;&lt;p&gt;动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：&lt;strong&gt;在运动方向上保持运动的趋势，从而加速收敛&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;动量法在深度学习中的应用&quot;&gt;&lt;a href=&quot;#动量法在深度学习中的应用&quot; class=&quot;headerlink&quot; title=&quot;动量法在深度学习中的应用&quot;&gt;&lt;/a&gt;动量法在深度学习中的应用&lt;/h2&gt;&lt;p&gt;在深度学习中，动量法通过记录 &lt;strong&gt;梯度的增量&lt;/strong&gt; 并将其与 &lt;strong&gt;当前梯度相加&lt;/strong&gt;，来 &lt;strong&gt;平滑梯度下降&lt;/strong&gt; 的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。&lt;/p&gt;
&lt;p&gt;动量法的更新公式如下：  &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
m_t = &#92;beta m_{t-1} + &#92;nabla L(w_t)&lt;/script&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
w_{t+1} = w_t - &#92;alpha m_t&lt;/script&gt;&lt;p&gt;其中：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$m_t$ 是动量项，记录了之前梯度的累积。  &lt;/li&gt;
&lt;li&gt;$&#92;beta$ 是动量参数，控制 &lt;strong&gt;动量项的衰减&lt;/strong&gt;，一般取值为 0.9。  &lt;/li&gt;
&lt;li&gt;$&#92;nabla L(w_t)$ 是当前参数的梯度。&lt;/li&gt;
&lt;li&gt;$&#92;alpha$ 是学习率。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>mini-batch GD</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3.Mini-batch%20GD/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3.Mini-batch%20GD/</id>
    <published>2024-08-12T21:02:27.000Z</published>
    <updated>2024-11-13T14:14:28.508Z</updated>
    
    <content type="html"><![CDATA[<h1 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a>小批量梯度下降法（Mini-batch Gradient Descent）</h1><p>在 <a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020">深度学习模型</a> 的训练过程中，梯度下降法是最常用的优化算法之一。我们前面介绍了梯度下降法（Batch Gradient Descent）和随机梯度下降法（Stochastic Gradient Descent），两者各有优缺点。为了在 <strong>计算速度</strong> 和 <strong>收敛稳定性</strong> 之间找到 <strong>平衡</strong>，<strong>小批量梯度下降法</strong>（Mini-batch Gradient Descent）应运而生。下面我们详细介绍其基本思想、优缺点，并通过代码实现来比较三种梯度下降法。</p><h2 id="小批量梯度下降法的基本思想"><a href="#小批量梯度下降法的基本思想" class="headerlink" title="小批量梯度下降法的基本思想"></a>小批量梯度下降法的基本思想</h2><p>​    小批量梯度下降法在每次迭代中，使用 <strong>一小部分随机样本（称为小批量）</strong> 来计算梯度，并更新参数值。具体来说，算法步骤如下：</p><ol><li><p>初始化参数 $w$ 和 $b$</p></li><li><p>在每次迭代中，从训练集中随机抽取 $m$ 个样本。</p></li><li>使用这 $m$ 个样本计算损失函数的梯度</li><li>更新参数 $w$ 和 $b$</li></ol><p>其梯度计算公式如下：  </p><script type="math/tex; mode=display">w_{t+1}= w_{t}-\alpha \cdot \frac{1}{m}\sum_{i = 1}^m{\nabla _w}L(w_{t}, b_{t}, x_i, y_i),\\b_{t+1}= b_{t}-\alpha \cdot \frac{1}{m}\sum_{i = 1}^m{\nabla _b}L(w_{t}, b_{t}, x_i, y_i),</script><p>其中，$\alpha$ 是学习率， $m$ 是小批量的大小。</p><ul><li>当 $m=1$ 时，Mini-batch Gradient Descent 转换为 SGD</li><li>当 $m=n$ 时，Mini-batch Gradient Descent 转换为 GD</li></ul><span id="more"></span><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li><strong>计算速度快</strong>：与批量梯度下降法相比，每次迭代只需计算小批量样本的梯度，速度更快。  </li><li><strong>减少振荡</strong>：与 <a href="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a> 相比，梯度的计算更加稳定，减少了参数更新时的振荡。  </li><li><strong>控制灵活</strong>：可以调整小批量的大小，使得训练速度和精度之间达到平衡。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li><strong>需要调整学习率和小批量大小</strong>：学习率决定每次更新的步长，小批量大小决定每次计算梯度使用的样本数量。  </li><li><strong>内存消耗</strong>：小批量大小的选择受限于内存容量，尤其在使用 GPU 运算时，需要选择合适的小批量大小。</li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面通过代码实现和比较三种梯度下降法的执行效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span>*X + <span class="number">2</span> + np.random.randn(<span class="number">1000</span>, <span class="number">1</span>) * <span class="number">0.1</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 转换为 tensor</span></span><br><span class="line">X_tensor = torch.tensor(X, dtype=torch.float32)</span><br><span class="line">y_tensor = torch.tensor(y, dtype=torch.float32)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 封装为数据集</span></span><br><span class="line">dataset = TensorDataset(X_tensor, y_tensor)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearRegressionModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义梯度下降法的批量大小</span></span><br><span class="line">batch_sizes = [<span class="number">1000</span>, <span class="number">1</span>, <span class="number">128</span>]</span><br><span class="line">batch_labels = [<span class="string">&#x27;Batch Gradient Descent&#x27;</span>, <span class="string">&#x27;Stochastic Gradient Descent&#x27;</span>, <span class="string">&#x27;Mini-batch Gradient Descent&#x27;</span>]</span><br><span class="line">colors = [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储损失值</span></span><br><span class="line">losses = &#123;label: [] <span class="keyword">for</span> label <span class="keyword">in</span> batch_labels&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> batch_size, label, color <span class="keyword">in</span> <span class="built_in">zip</span>(batch_sizes, batch_labels, colors):</span><br><span class="line">    model = LinearRegressionModel()</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">    </span><br><span class="line">    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs), desc=label):</span><br><span class="line">        epoch_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch_x, batch_y <span class="keyword">in</span> data_loader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(batch_x)</span><br><span class="line">            loss = criterion(outputs, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        losses[label].append(epoch_loss / <span class="built_in">len</span>(data_loader))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制损失值变化曲线</span></span><br><span class="line"><span class="keyword">for</span> label, color <span class="keyword">in</span> <span class="built_in">zip</span>(batch_labels, colors):</span><br><span class="line">    plt.plot(losses[label], color=color, label=label)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>运行上述代码后，会显示三种梯度下降法在每个迭代周期（epoch）中的损失变化曲线。可以看到：</p><p><img src="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112012585.png" alt="image-20240811201255444"></p><ol><li>批量梯度下降法：损失曲线平滑，但训练速度较慢。  </li><li>随机梯度下降法：训练速度快，但损失曲线波动较大。  </li><li>小批量梯度下降法：在训练速度和损失曲线的稳定性之间达到了平衡，效果较为理想。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    小批量梯度下降法结合了批量梯度下降法和随机梯度下降法的优点，是深度学习中常用的 <a href="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a>。通过调整小 <strong>批量大小</strong> 和 <strong>学习率</strong>，可以在 <strong>训练速度</strong> 和 <strong>收敛稳定性</strong> 之间找到最佳平衡。在实际应用中，小批量梯度下降法由于其较高的效率和较好的收敛效果，被广泛应用于各类深度学习模型的训练中。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;小批量梯度下降法（Mini-batch-Gradient-Descent）&quot;&gt;&lt;a href=&quot;#小批量梯度下降法（Mini-batch-Gradient-Descent）&quot; class=&quot;headerlink&quot; title=&quot;小批量梯度下降法（Mini-batch Gradient Descent）&quot;&gt;&lt;/a&gt;小批量梯度下降法（Mini-batch Gradient Descent）&lt;/h1&gt;&lt;p&gt;在 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习模型&lt;/a&gt; 的训练过程中，梯度下降法是最常用的优化算法之一。我们前面介绍了梯度下降法（Batch Gradient Descent）和随机梯度下降法（Stochastic Gradient Descent），两者各有优缺点。为了在 &lt;strong&gt;计算速度&lt;/strong&gt; 和 &lt;strong&gt;收敛稳定性&lt;/strong&gt; 之间找到 &lt;strong&gt;平衡&lt;/strong&gt;，&lt;strong&gt;小批量梯度下降法&lt;/strong&gt;（Mini-batch Gradient Descent）应运而生。下面我们详细介绍其基本思想、优缺点，并通过代码实现来比较三种梯度下降法。&lt;/p&gt;
&lt;h2 id=&quot;小批量梯度下降法的基本思想&quot;&gt;&lt;a href=&quot;#小批量梯度下降法的基本思想&quot; class=&quot;headerlink&quot; title=&quot;小批量梯度下降法的基本思想&quot;&gt;&lt;/a&gt;小批量梯度下降法的基本思想&lt;/h2&gt;&lt;p&gt;​    小批量梯度下降法在每次迭代中，使用 &lt;strong&gt;一小部分随机样本（称为小批量）&lt;/strong&gt; 来计算梯度，并更新参数值。具体来说，算法步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;初始化参数 $w$ 和 $b$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在每次迭代中，从训练集中随机抽取 $m$ 个样本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;使用这 $m$ 个样本计算损失函数的梯度&lt;/li&gt;
&lt;li&gt;更新参数 $w$ 和 $b$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其梯度计算公式如下：  &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
w_{t+1}= w_{t}-&#92;alpha &#92;cdot &#92;frac{1}{m}&#92;sum_{i = 1}^m{&#92;nabla _w}L(w_{t}, b_{t}, x_i, y_i),
&#92;&#92;
b_{t+1}= b_{t}-&#92;alpha &#92;cdot &#92;frac{1}{m}&#92;sum_{i = 1}^m{&#92;nabla _b}L(w_{t}, b_{t}, x_i, y_i),&lt;/script&gt;&lt;p&gt;其中，$&#92;alpha$ 是学习率， $m$ 是小批量的大小。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当 $m=1$ 时，Mini-batch Gradient Descent 转换为 SGD&lt;/li&gt;
&lt;li&gt;当 $m=n$ 时，Mini-batch Gradient Descent 转换为 GD&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SGD</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2.SGD/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2.SGD/</id>
    <published>2024-08-12T20:02:27.000Z</published>
    <updated>2024-11-13T14:14:25.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="随机梯度下降法（SGD）"><a href="#随机梯度下降法（SGD）" class="headerlink" title="随机梯度下降法（SGD）"></a>随机梯度下降法（SGD）</h1><p>​    在 <a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a> 中，梯度下降法（Gradient Descent）是最常用的模型参数优化方法。然而，传统的梯度下降法（Full Batch Learning）存在一些缺点，例如训练时间过长和容易陷入局部最小值。为了解决这些问题，随机梯度下降法（Stochastic Gradient Descent，简称 SGD）应运而生。</p><h2 id="传统梯度下降法的问题"><a href="#传统梯度下降法的问题" class="headerlink" title="传统梯度下降法的问题"></a>传统梯度下降法的问题</h2><ol><li><strong>训练时间长</strong>：传统梯度下降法需要使用所有训练数据来计算梯度，因此数据量大时耗时严重。  </li><li><strong>容易陷入局部最小值</strong>：复杂的损失函数可能会导致算法在局部最小值附近来回震荡，无法快速收敛。  </li><li><strong>对初始值敏感</strong>：初始值选择不当可能导致算法被卡在局部最小值。</li></ol><h2 id="随机梯度下降法-的基本思想"><a href="#随机梯度下降法-的基本思想" class="headerlink" title="随机梯度下降法 的基本思想"></a><a href="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a> 的基本思想</h2><p>SGD 每次迭代仅 <strong>使用一个样本</strong> 的 <strong>损失值</strong> 来计算梯度，而不是全数据集损失值的求和平均来计算梯度。</p><span id="more"></span><h2 id="SGD-的优缺点"><a href="#SGD-的优缺点" class="headerlink" title="SGD 的优缺点"></a>SGD 的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li><strong>速度快</strong>：每次迭代只需计算一个样本的梯度，速度比传统方法快很多。  </li><li><strong>避免局部最小值</strong>：因为每次更新参数只使用一个样本，随机性使得算法不容易陷入局部最小值。</li><li><strong>更易实现和调整</strong>：每个样本的梯度可以分别计算，并行处理更加高效。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li><strong>收敛不稳定</strong>：每次迭代梯度都会有噪声，可能导致收敛不稳定。</li><li><strong>方差较大</strong>：每次更新参数只使用一个样本的梯度，可能导致算法方差较大，难以收敛。</li></ol><h2 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h2><p>为了提高 SGD 的收敛性，可以使用动态学习率。常见的动态学习率策略包括：</p><ol><li><p><strong>反比例学习率</strong>：初始学习率随着迭代次数增加而减小。  </p></li><li><p><strong>反比例平方学习率</strong>：类似反比例学习率，但减小速度更快。  </p></li></ol><script type="math/tex; mode=display">\alpha_t = \frac{\alpha_0}{1 + k \cdot t^2}</script><ol><li><strong>指数衰减学习率</strong>：学习率以指数形式衰减。</li></ol><script type="math/tex; mode=display">\alpha_t = \alpha_0 \cdot e^{-\lambda t}</script><h2 id="实现示例"><a href="#实现示例" class="headerlink" title="实现示例"></a>实现示例</h2><p>下面是使用 Python 实现随机梯度下降法的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stochastic_gradient_descent</span>(<span class="params">X, y, lr=<span class="number">0.01</span>, epochs=<span class="number">1000</span></span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line">    w = np.zeros(n)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            random_index = np.random.randint(m)</span><br><span class="line">            xi = X[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">            gradient_w = <span class="number">2</span> * xi.T.dot(xi.dot(w) + b - yi)</span><br><span class="line">            gradient_b = <span class="number">2</span> * (xi.dot(w) + b - yi)</span><br><span class="line">            w = w - lr * gradient_w</span><br><span class="line">            b = b - lr * gradient_b</span><br><span class="line">        <span class="comment"># 可选：动态学习率调整</span></span><br><span class="line">        lr = lr / (<span class="number">1</span> + epoch / epochs)</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">y = np.array([<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用 SGD 函数</span></span><br><span class="line">w, b = stochastic_gradient_descent(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;权重:&quot;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;偏置:&quot;</span>, b)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SGD 通过每次迭代使用一个 <strong>随机样本</strong> 来计算梯度，从而加快了计算速度并避免陷入局部最小值。动态学习率的使用可以进一步提高 SGD 的收敛性。在实际应用中，SGD 已成为深度学习领域最常用的优化算法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;随机梯度下降法（SGD）&quot;&gt;&lt;a href=&quot;#随机梯度下降法（SGD）&quot; class=&quot;headerlink&quot; title=&quot;随机梯度下降法（SGD）&quot;&gt;&lt;/a&gt;随机梯度下降法（SGD）&lt;/h1&gt;&lt;p&gt;​    在 &lt;a href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt; 中，梯度下降法（Gradient Descent）是最常用的模型参数优化方法。然而，传统的梯度下降法（Full Batch Learning）存在一些缺点，例如训练时间过长和容易陷入局部最小值。为了解决这些问题，随机梯度下降法（Stochastic Gradient Descent，简称 SGD）应运而生。&lt;/p&gt;
&lt;h2 id=&quot;传统梯度下降法的问题&quot;&gt;&lt;a href=&quot;#传统梯度下降法的问题&quot; class=&quot;headerlink&quot; title=&quot;传统梯度下降法的问题&quot;&gt;&lt;/a&gt;传统梯度下降法的问题&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;训练时间长&lt;/strong&gt;：传统梯度下降法需要使用所有训练数据来计算梯度，因此数据量大时耗时严重。  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;容易陷入局部最小值&lt;/strong&gt;：复杂的损失函数可能会导致算法在局部最小值附近来回震荡，无法快速收敛。  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对初始值敏感&lt;/strong&gt;：初始值选择不当可能导致算法被卡在局部最小值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;随机梯度下降法-的基本思想&quot;&gt;&lt;a href=&quot;#随机梯度下降法-的基本思想&quot; class=&quot;headerlink&quot; title=&quot;随机梯度下降法 的基本思想&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;随机梯度下降法&lt;/a&gt; 的基本思想&lt;/h2&gt;&lt;p&gt;SGD 每次迭代仅 &lt;strong&gt;使用一个样本&lt;/strong&gt; 的 &lt;strong&gt;损失值&lt;/strong&gt; 来计算梯度，而不是全数据集损失值的求和平均来计算梯度。&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>GD</title>
    <link href="https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1.GD/"/>
    <id>https://luyicui.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1.GD/</id>
    <published>2024-08-12T19:02:27.000Z</published>
    <updated>2024-11-13T14:14:22.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度下降法（Gradient-Descent）"><a href="#梯度下降法（Gradient-Descent）" class="headerlink" title="梯度下降法（Gradient Descent）"></a>梯度下降法（Gradient Descent）</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>​    在深度学习中，<a href="https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">损失函数</a> 的求解是一个关键步骤。损失函数通常没有解析解，因此需要通过最优化算法来逼近求解。其中，梯度下降法是最常用的优化算法之一。本文将详细介绍梯度下降法的基本概念、理论基础、及其在深度学习中的应用。</p><h2 id="梯度下降法的基本概念"><a href="#梯度下降法的基本概念" class="headerlink" title="梯度下降法的基本概念"></a>梯度下降法的基本概念</h2><p>梯度下降法（Gradient Descent）是一种基于一阶导数的优化算法，用于最小化目标函数。在深度学习中，目标函数通常是损失函数，其目的是通过调整参数来使损失最小化。</p><h3 id="损失函数的定义"><a href="#损失函数的定义" class="headerlink" title="损失函数的定义"></a>损失函数的定义</h3><p>假设损失函数 $L$ 是参数 $W$ 的函数：$L(W)$，我们的目标是找到参数 $W$ 使得 $L(W)$ 最小化。</p><span id="more"></span><h3 id="梯度的定义"><a href="#梯度的定义" class="headerlink" title="梯度的定义"></a>梯度的定义</h3><p>梯度是损失函数的导数，表示函数在某一点处的最陡下降方向。对于参数 $W$ 的每个分量 $w_i$ ，梯度表示为：</p><script type="math/tex; mode=display">\nabla L(W)=\left [\frac{\partial L}{\partial w_1},\frac{\partial L}{\partial w_2},\ldots,\frac{\partial L}{\partial w_n}\right]</script><blockquote><p>  <strong>梯度的维度</strong> = <strong>参数的个数</strong></p></blockquote><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>梯度下降法通过以下步骤更新参数：</p><script type="math/tex; mode=display">w_{t+1}= w_{t}-\alpha \cdot \frac{1}{n}\sum_{i = 1}^n{\nabla _w}L(w_{t}, b_{t}, x_i, y_i),\\b_{t+1}= b_{t}-\alpha \cdot \frac{1}{n}\sum_{i = 1}^n{\nabla _b}L(w_{t}, b_{t}, x_i, y_i),</script><p>其中，$\alpha$ 是学习率（Learning Rate），决定了每次更新的步长；$n$ 是样本大小。</p><h2 id="梯度下降法的应用"><a href="#梯度下降法的应用" class="headerlink" title="梯度下降法的应用"></a>梯度下降法的应用</h2><h3 id="简单示例：二次损失函数"><a href="#简单示例：二次损失函数" class="headerlink" title="简单示例：二次损失函数"></a>简单示例：二次损失函数</h3><p>为了便于理解，我们假设损失函数是一个简单的二次函数</p><script type="math/tex; mode=display">L(W) = W^2</script><p>梯度为：</p><script type="math/tex; mode=display">\nabla L(W) = 2W</script><p>根据梯度下降法的更新规则，参数更新为：</p><script type="math/tex; mode=display">W_{t+1} = W_t - \alpha \cdot 2W_t = W_t(1 - 2\alpha)</script><h3 id="高维度情况下的梯度下降"><a href="#高维度情况下的梯度下降" class="headerlink" title="高维度情况下的梯度下降"></a>高维度情况下的梯度下降</h3><p>​    在实际应用中，损失函数往往是高维度的。梯度下降法可以扩展到高维度情况，其中 <strong>梯度是一个向量，表示每个参数</strong> 的导数。我们将梯度表示为一个向量，并对每个参数进行更新。</p><h3 id="学习率的选择"><a href="#学习率的选择" class="headerlink" title="学习率的选择"></a>学习率的选择</h3><p>学习率  $\alpha$ 对梯度下降法的收敛速度和稳定性有重大影响。选择合适的学习率非常重要。</p><ul><li>如果学习率过大，算法可能会在最小值附近来回 <strong>震荡</strong>；</li><li>如果学习率过小，算法的 <strong>收敛速度会非常慢</strong>。</li></ul><h2 id="梯度下降法的变体"><a href="#梯度下降法的变体" class="headerlink" title="梯度下降法的变体"></a>梯度下降法的变体</h2><p>在实际应用中，梯度下降法有多种变体，以提高收敛速度和稳定性。常见的变体包括：</p><ul><li><strong>随机梯度下降法（SGD）</strong>：每次迭代使用一个或几个样本来更新参数，而不是使用整个训练集。这种方法可以显著加快计算速度。  </li><li><strong>动量法（Momentum）</strong>：在每次更新时，加入之前更新的动量，以加速收敛。  </li><li><strong>自适应学习率</strong> 方法：例如 Adagrad、RMSprop、Adam 等，通过 <strong>动态调整学习率</strong> 来提高收敛效果。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>梯度下降法是深度学习中最常用的优化算法之一。通过计算损失函数的梯度，确定参数的更新方向和步长，不断逼近损失函数的最小值。<strong>选择合适的学习率和初始点是梯度下降法（GD）成功的关键。</strong> 理解梯度下降法的基本概念和应用，对于深入学习 <a href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">深度学习算法</a> 有重要意义。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;梯度下降法（Gradient-Descent）&quot;&gt;&lt;a href=&quot;#梯度下降法（Gradient-Descent）&quot; class=&quot;headerlink&quot; title=&quot;梯度下降法（Gradient Descent）&quot;&gt;&lt;/a&gt;梯度下降法（Gradient Descent）&lt;/h1&gt;&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;​    在深度学习中，&lt;a href=&quot;https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;损失函数&lt;/a&gt; 的求解是一个关键步骤。损失函数通常没有解析解，因此需要通过最优化算法来逼近求解。其中，梯度下降法是最常用的优化算法之一。本文将详细介绍梯度下降法的基本概念、理论基础、及其在深度学习中的应用。&lt;/p&gt;
&lt;h2 id=&quot;梯度下降法的基本概念&quot;&gt;&lt;a href=&quot;#梯度下降法的基本概念&quot; class=&quot;headerlink&quot; title=&quot;梯度下降法的基本概念&quot;&gt;&lt;/a&gt;梯度下降法的基本概念&lt;/h2&gt;&lt;p&gt;梯度下降法（Gradient Descent）是一种基于一阶导数的优化算法，用于最小化目标函数。在深度学习中，目标函数通常是损失函数，其目的是通过调整参数来使损失最小化。&lt;/p&gt;
&lt;h3 id=&quot;损失函数的定义&quot;&gt;&lt;a href=&quot;#损失函数的定义&quot; class=&quot;headerlink&quot; title=&quot;损失函数的定义&quot;&gt;&lt;/a&gt;损失函数的定义&lt;/h3&gt;&lt;p&gt;假设损失函数 $L$ 是参数 $W$ 的函数：$L(W)$，我们的目标是找到参数 $W$ 使得 $L(W)$ 最小化。&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://luyicui.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
