<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>弘毅 の blog</title>
  
  
  <link href="https://cuiluyi.github.io/atom.xml" rel="self"/>
  
  <link href="https://cuiluyi.github.io/"/>
  <updated>2024-12-03T12:41:19.120Z</updated>
  <id>https://cuiluyi.github.io/</id>
  
  <author>
    <name>弘毅</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9014.%E5%93%8D%E5%BA%94%E5%BC%8F%E5%B8%83%E5%B1%80%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9014.%E5%93%8D%E5%BA%94%E5%BC%8F%E5%B8%83%E5%B1%80%E3%80%91/</id>
    <published>2024-12-03T12:34:45.945Z</published>
    <updated>2024-12-03T12:41:19.120Z</updated>
    
    <content type="html"><![CDATA[<h1 id="响应式布局">【14.响应式布局】</h1><h2 id="media查询">media查询</h2><p>当屏幕宽度满足特定条件时应用css。</p><p>例如：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">@media</span>(min-width: <span class="number">768px</span>) &#123;</span><br><span class="line">    <span class="selector-class">.container</span> &#123;</span><br><span class="line">        <span class="attribute">width</span>: <span class="number">960px</span>;</span><br><span class="line">        <span class="attribute">background-color</span>: lightblue;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="bootstrap">Bootstrap</h2><ul><li><a href="https://v5.bootcss.com/">bootstrap地址</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;响应式布局&quot;&gt;【14.响应式布局】&lt;/h1&gt;
&lt;h2 id=&quot;media查询&quot;&gt;media查询&lt;/h2&gt;
&lt;p&gt;当屏幕宽度满足特定条件时应用css。&lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;figure class=&quot;highlight css&quot;&gt;&lt;table&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9013.flex%E5%B8%83%E5%B1%80%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9013.flex%E5%B8%83%E5%B1%80%E3%80%91/</id>
    <published>2024-12-03T12:34:45.921Z</published>
    <updated>2024-12-03T12:41:02.110Z</updated>
    
    <content type="html"><![CDATA[<h1 id="flex布局">【13.flex布局】</h1><p><code>flex</code>CSS简写属性设置了弹性项目如何增大或缩小以适应其弹性容器中可用的空间。</p><h2 id="flex-direction"><code>flex-direction</code></h2><p>CSS <code>flex-direction</code> 属性指定了内部元素是如何在 flex容器中布局的，定义了主轴的方向(正方向或反方向)。</p><h2 id="取值">取值：</h2><ul><li><code>row</code>：flex容器的主轴被定义为与文本方向相同。主轴起点和主轴终点与内容方向相同。</li><li><code>row-reverse</code>：表现和row相同，但是置换了主轴起点和主轴终点。</li><li><code>column</code>：flex容器的主轴和块轴相同。主轴起点与主轴终点和书写模式的前后点相同</li><li><code>column-reverse</code>：表现和column相同，但是置换了主轴起点和主轴终点</li></ul><h2 id="flex-wrap"><code>flex-wrap</code></h2><p>CSS 的 <code>flex-wrap</code> 属性指定 flex元素单行显示还是多行显示。如果允许换行，这个属性允许你控制行的堆叠方向。</p><h2 id="取值-1">取值：</h2><ul><li><code>nowrap</code>：默认值。不换行。</li><li><code>wrap</code>：换行，第一行在上方。</li><li><code>wrap-reverse</code>：换行，第一行在下方。</li></ul><h2 id="flex-flow"><code>flex-flow</code></h2><p>CSS <code>flex-flow</code> 属性是 <code>flex-direction</code> 和<code>flex-wrap</code> 的简写。默认值为：<code>row nowrap</code>。</p><h2 id="justify-content"><code>justify-content</code></h2><p>CSS <code>justify-content</code>属性定义了浏览器之间，如何分配顺着弹性容器主轴(或者网格行轴)的元素之间及其周围的空间。</p><h2 id="取值-2">取值：</h2><ul><li><code>flex-start</code>：默认值。左对齐。</li><li><code>flex-end</code>：右对齐。</li><li><code>space-between</code>：左右两段对齐。</li><li><code>space-around</code>：在每行上均匀分配弹性元素。相邻元素间距离相同。每行第一个元素到行首的距离和每行最后一个元素到行尾的距离将会是相邻元素之间距离的一半。</li><li><code>space-evenly</code>：flex项都沿着主轴均匀分布在指定的对齐容器中。相邻flex项之间的间距，主轴起始位置到第一个flex项的间距，主轴结束位置到最后一个flex项的间距，都完全一样。</li></ul><h2 id="align-items"><code>align-items</code></h2><p>CSS<code>align-items</code>属性将所有直接子节点上的align-self值设置为一个组。align-self属性设置项目在其包含块中在交叉轴方向上的对齐方式。</p><h2 id="取值-3">取值：</h2><ul><li><code>flex-start</code>：元素向主轴起点对齐。</li><li><code>flex-end</code>：元素向主轴终点对齐。</li><li><code>center</code>：元素在侧轴居中。</li><li><code>stretch</code>：弹性元素被在侧轴方向被拉伸到与容器相同的高度或宽度。</li></ul><h2 id="align-content"><code>align-content</code></h2><p>CSS 的 <code>align-content</code>属性设置了浏览器如何沿着弹性盒子布局的纵轴和网格布局的主轴在内容项之间和周围分配空间。</p><h2 id="取值-4">取值：</h2><ul><li><code>flex-start</code>：所有行从垂直轴起点开始填充。第一行的垂直轴起点边和容器的垂直轴起点边对齐。接下来的每一行紧跟前一行。</li><li><code>flex-end</code>：所有行从垂直轴末尾开始填充。最后一行的垂直轴终点和容器的垂直轴终点对齐。同时所有后续行与前一个对齐。</li><li><code>center</code>：所有行朝向容器的中心填充。每行互相紧挨，相对于容器居中对齐。容器的垂直轴起点边和第一行的距离相等于容器的垂直轴终点边和最后一行的距离。</li><li><code>stretch</code>：拉伸所有行来填满剩余空间。剩余空间平均地分配给每一行。</li></ul><h2 id="order"><code>order</code></h2><p>定义<code>flex</code>项目的顺序，值越小越靠前。</p><h2 id="flex-grow"><code>flex-grow</code></h2><p>CSS 属性 <code>flex-grow</code> CSS 设置 flex 项主尺寸 的 flex增长系数。</p><p>负值无效，默认为 0。</p><h2 id="flex-shrink"><code>flex-shrink</code></h2><p>CSS <code>flex-shrink</code> 属性指定了 flex 元素的收缩规则。flex元素仅在默认宽度之和大于容器的时候才会发生收缩，其收缩的大小是依据flex-shrink 的值。</p><p>负值无效，默认为1。</p><h2 id="flex-basis"><code>flex-basis</code></h2><p>CSS 属性 flex-basis 指定了 flex 元素在主轴方向上的初始大小。</p><h2 id="取值-5">取值：</h2><p>width 值可以是 <code>&lt;length&gt;</code>;该值也可以是一个相对于其父弹性盒容器主轴尺寸的百分数。负值是不被允许的。默认为 auto。</p><h2 id="flex"><code>flex</code></h2><p><code>flex-grow</code>、<code>flex-shrink</code>、<code>flex-basis</code>的缩写。</p><h2 id="常用取值">常用取值：</h2><ul><li><code>auto</code>：<code>flex: 1 1 auto</code></li><li><code>none</code>：<code>flex: 0 0 auto</code></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;flex布局&quot;&gt;【13.flex布局】&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;flex&lt;/code&gt;
CSS简写属性设置了弹性项目如何增大或缩小以适应其弹性容器中可用的空间。&lt;/p&gt;
&lt;h2 id=&quot;flex-direction&quot;&gt;&lt;code&gt;flex-directi</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9012.%E6%B5%AE%E5%8A%A8%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9012.%E6%B5%AE%E5%8A%A8%E3%80%91/</id>
    <published>2024-12-03T12:34:45.849Z</published>
    <updated>2024-12-03T12:40:43.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="浮动">【12.浮动】</h1><h2 id="float"><code>float</code></h2><p><code>float</code>CSS属性指定一个元素应沿其容器的左侧或右侧放置，允许文本和内联元素环绕它。该元素从网页的正常流动(文档流)中移除，尽管仍然保持部分的流动性（与绝对定位相反）。</p><p>由于float意味着使用块布局，它在某些情况下修改display 值的计算值：</p><ul><li><code>display</code>为<code>inline</code>或<code>inline-block</code>时，使用<code>float</code>后会统一变成<code>block</code>。</li></ul><h2 id="取值">取值：</h2><ul><li><code>left</code>：表明元素必须浮动在其所在的块容器左侧的关键字。</li><li><code>right</code>：表明元素必须浮动在其所在的块容器右侧的关键字。</li></ul><h2 id="clear"><code>clear</code></h2><p>有时，你可能想要强制元素移至任何浮动元素下方。比如说，你可能希望某个段落与浮动元素保持相邻的位置，但又希望这个段落从头开始强制独占一行。此时可以使用<code>clear</code>。</p><h2 id="取值-1">取值：</h2><ul><li><code>left</code>：清除左侧浮动。</li><li><code>right</code>：清除右侧浮动。</li><li><code>both</code>：清除左右两侧浮动</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;浮动&quot;&gt;【12.浮动】&lt;/h1&gt;
&lt;h2 id=&quot;float&quot;&gt;&lt;code&gt;float&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;float&lt;/code&gt;
CSS属性指定一个元素应沿其容器的左侧或右侧放置，允许文本和内联元素环绕它。该元素从网页的正常流动(文</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9011.%E4%BD%8D%E7%BD%AE%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9011.%E4%BD%8D%E7%BD%AE%E3%80%91/</id>
    <published>2024-12-03T12:34:45.821Z</published>
    <updated>2024-12-03T12:40:29.356Z</updated>
    
    <content type="html"><![CDATA[<h1 id="位置">【11.位置】</h1><h2 id="position"><code>position</code></h2><p><code>CSS position</code>属性用于指定一个元素在文档中的定位方式。</p><h2 id="定位类型">定位类型：</h2><ul><li>定位元素（positioned element）是其计算后位置属性为 relative,absolute, fixed 或 sticky的一个元素（换句话说，除static以外的任何东西）。</li><li>相对定位元素（relatively positioned element）是计算后位置属性为relative 的元素。</li><li>绝对定位元素（absolutely positioned element）是计算后位置属性为absolute 或 fixed 的元素。</li><li>粘性定位元素（stickily positioned element）是计算后位置属性为 sticky的元素。</li></ul><h2 id="取值">取值：</h2><ul><li><code>static</code>：该关键字指定元素使用正常的布局行为，即元素在文档常规流中当前的布局位置。此时top, right, bottom, left 和 z-index 属性无效。</li><li><code>relative</code>：该关键字下，元素先放置在未添加定位时的位置，再在不改变页面布局的前提下调整元素位置（因此会在此元素未添加定位时所在位置留下空白）。top,right, bottom, left等调整元素相对于初始位置的偏移量。</li><li><code>absolute</code>：元素会被移出正常文档流，并不为元素预留空间，通过指定元素相对于最近的非static定位祖先元素的偏移，来确定元素位置。绝对定位的元素可以设置外边距（margins），且不会与其他边距合并。</li><li><code>fixed</code>：元素会被移出正常文档流，并不为元素预留空间，而是通过指定元素相对于屏幕视口（viewport）的位置来指定元素位置。元素的位置在屏幕滚动时不会改变。</li><li><code>sticky</code>：元素根据正常文档流进行定位，然后相对它的最近滚动祖先（nearestscrolling ancestor）和 containing block (最近块级祖先 nearestblock-level ancestor)，包括table-related元素，基于top, right, bottom, 和left的值进行偏移。偏移值不会影响任何其他元素的位置。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;位置&quot;&gt;【11.位置】&lt;/h1&gt;
&lt;h2 id=&quot;position&quot;&gt;&lt;code&gt;position&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;CSS position&lt;/code&gt;属性用于指定一个元素在文档中的定位方式。&lt;/p&gt;
&lt;h2 id=&quot;定位类型&quot;&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9010.%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%9010.%E7%9B%92%E5%AD%90%E6%A8%A1%E5%9E%8B%E3%80%91/</id>
    <published>2024-12-03T12:34:44.420Z</published>
    <updated>2024-12-03T12:40:07.163Z</updated>
    
    <content type="html"><![CDATA[<h1 id="盒子模型">【10.盒子模型】</h1><h2 id="box-sizing"><code>box-sizing</code></h2><p>CSS 中的 <code>box-sizing</code> 属性定义了 user agent应该如何计算一个元素的总宽度和总高度。</p><ul><li><code>content-box</code>：是默认值，设置<code>border</code>和<code>padding</code>均会增加元素的宽高。</li><li><code>border-box</code>：设置<code>border</code>和<code>padding</code>不会改变元素的宽高，而是挤占内容区域。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;盒子模型&quot;&gt;【10.盒子模型】&lt;/h1&gt;
&lt;h2 id=&quot;box-sizing&quot;&gt;&lt;code&gt;box-sizing&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;CSS 中的 &lt;code&gt;box-sizing&lt;/code&gt; 属性定义了 user agent
应该如何计算一个</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%909.%E5%86%85%E8%BE%B9%E8%B7%9D%E4%B8%8E%E5%A4%96%E8%BE%B9%E8%B7%9D%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%909.%E5%86%85%E8%BE%B9%E8%B7%9D%E4%B8%8E%E5%A4%96%E8%BE%B9%E8%B7%9D%E3%80%91/</id>
    <published>2024-12-03T12:34:44.393Z</published>
    <updated>2024-12-03T12:39:50.436Z</updated>
    
    <content type="html"><![CDATA[<h1 id="内边距与外边距">【9.内边距与外边距】</h1><h2 id="margin"><code>margin</code></h2><p><code>margin</code>属性为给定元素设置所有四个（上下左右）方向的外边距属性。</p><ul><li>可以接受1~4个值（上、右、下、左的顺序）</li><li>可以分别指明四个方向：<code>margin-top</code>、<code>margin-right</code>、<code>margin-bottom</code>、<code>margin-left</code></li><li>可取值<ul><li><code>length</code>：固定值</li><li><code>percentage</code>：相对于包含块的宽度，以百分比值为外边距。</li><li><code>auto</code>：让浏览器自己选择一个合适的外边距。有时，在一些特殊情况下，该值可以使元素居中。</li></ul></li><li>外边距重叠<ul><li>块的上外边距(margin-top)和下外边距(margin-bottom)有时合并(折叠)为单个边距，其大小为单个边距的最大值(或如果它们相等，则仅为其中一个)，这种行为称为边距折叠。</li><li>父元素与后代元素：父元素没有上边框和padding时，后代元素的<code>margin-top</code>会溢出，溢出后父元素的<code>margin-top</code>会与后代元素取最大值。</li></ul></li></ul><h2 id="padding"><code>padding</code></h2><p><code>padding</code> CSS 简写属性控制元素所有四条边的内边距区域。</p><ul><li>可以接受1~4个值（上、右、下、左的顺序）</li><li>可以分别指明四个方向：<code>padding-top</code>、<code>padding-right</code>、<code>padding-bottom</code>、<code>padding-left</code></li><li>可取值<ul><li><code>length</code>：固定值</li><li><code>percentage</code>：相对于包含块的宽度，以百分比值为内边距。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;内边距与外边距&quot;&gt;【9.内边距与外边距】&lt;/h1&gt;
&lt;h2 id=&quot;margin&quot;&gt;&lt;code&gt;margin&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;margin&lt;/code&gt;属性为给定元素设置所有四个（上下左右）方向的外边距属性。&lt;/p&gt;
&lt;ul&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%908.%E5%85%83%E7%B4%A0%E5%B1%95%E7%A4%BA%E6%A0%BC%E5%BC%8F%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%908.%E5%85%83%E7%B4%A0%E5%B1%95%E7%A4%BA%E6%A0%BC%E5%BC%8F%E3%80%91/</id>
    <published>2024-12-03T12:34:44.369Z</published>
    <updated>2024-12-03T12:39:31.329Z</updated>
    
    <content type="html"><![CDATA[<h1 id="元素展示格式">【8.元素展示格式】</h1><h2 id="display"><code>display</code></h2><ul><li><code>block</code>：<ul><li>独占一行</li><li>width、height、margin、padding均可控制</li><li>width默认100%。</li></ul></li><li><code>inline</code>：<ul><li>可以共占一行</li><li>width与height无效，水平方向的margin与padding有效，竖直方向的margin与padding无效</li><li>width默认为本身内容宽度</li></ul></li><li><code>inline-block</code><ul><li>可以共占一行</li><li>width、height、margin、padding均可控制</li><li>width默认为本身内容宽度</li></ul></li></ul><h2 id="white-space"><code>white-space</code></h2><p><code>white-space</code> CSS 属性是用来设置如何处理元素中的 空白(en-US)。</p><h2 id="text-overflow"><code>text-overflow</code></h2><p><code>text-overflow</code> CSS属性确定如何向用户发出未显示的溢出内容信号。它可以被剪切，显示一个省略号或显示一个自定义字符串。</p><h2 id="overflow"><code>overflow</code></h2><p>CSS属性 <code>overflow</code> 定义当一个元素的内容太大而无法适应块级格式化上下文 时候该做什么。它是 <code>overflow-x</code>和<code>overflow-y</code>的 简写属性 。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;元素展示格式&quot;&gt;【8.元素展示格式】&lt;/h1&gt;
&lt;h2 id=&quot;display&quot;&gt;&lt;code&gt;display&lt;/code&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;block&lt;/code&gt;：
&lt;ul&gt;
&lt;li&gt;独占一行&lt;/li&gt;
&lt;li&gt;width、h</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%907.%E8%BE%B9%E6%A1%86%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%907.%E8%BE%B9%E6%A1%86%E3%80%91/</id>
    <published>2024-12-03T12:34:44.306Z</published>
    <updated>2024-12-03T12:39:16.376Z</updated>
    
    <content type="html"><![CDATA[<h1 id="边框">【7.边框】</h1><h2 id="border-style"><code>border-style</code></h2><p><code>border-style</code> 是一个 CSS简写属性，用来设定元素所有边框的样式。</p><h2 id="border-width"><code>border-width</code></h2><p><code>border-width</code>属性可以设置盒子模型的边框宽度。</p><h2 id="border-color"><code>border-color</code></h2><p>CSS属性<code>border-color</code>是一个用于设置元素四个边框颜色的快捷属性：<code>border-top-color</code>, <code>border-right-color</code>,<code>border-bottom-color</code>, <code>border-left-color</code></p><h2 id="border-radius"><code>border-radius</code></h2><p>CSS 属性 <code>border-radius</code>允许你设置元素的外边框圆角。当使用一个半径时确定一个圆形，当使用两个半径时确定一个椭圆。这个(椭)圆与边框的交集形成圆角效果。</p><h2 id="border-collapse"><code>border-collapse</code></h2><p><code>border-collapse</code> CSS属性是用来决定表格的边框是分开的还是合并的。在分隔模式下，相邻的单元格都拥有独立的边框。在合并模式下，相邻单元格共享边框。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;边框&quot;&gt;【7.边框】&lt;/h1&gt;
&lt;h2 id=&quot;border-style&quot;&gt;&lt;code&gt;border-style&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;border-style&lt;/code&gt; 是一个 CSS
简写属性，用来设定元素所有边框的样式。&lt;/p&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%906.%E8%83%8C%E6%99%AF%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%906.%E8%83%8C%E6%99%AF%E3%80%91/</id>
    <published>2024-12-03T12:34:44.278Z</published>
    <updated>2024-12-03T12:38:57.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">【6.背景】</h1><h2 id="background-color"><code>background-color</code></h2><p>CSS属性中的<code>background-color</code>会设置元素的背景色,属性的值为颜色值或关键字”transparent”二者选其一。</p><h2 id="background-image"><code>background-image</code></h2><p>CSS <code>background-image</code>属性用于为一个元素设置一个或者多个背景图像。</p><ul><li>渐变色：linear-gradient(rgba(0, 0, 255, 0.5), rgba(255, 255, 0,0.5))</li></ul><h2 id="background-size"><code>background-size</code></h2><p><code>background-size</code>设置背景图片大小。图片可以保有其原有的尺寸，或者拉伸到新的尺寸，或者在保持其原有比例的同时缩放到元素的可用空间的尺寸。</p><h2 id="background-repeat"><code>background-repeat</code></h2><p><code>background-repeat</code> CSS属性定义背景图像的重复方式。背景图像可以沿着水平轴，垂直轴，两个轴重复，或者根本不重复。</p><h2 id="background-position"><code>background-position</code></h2><p><code>background-position</code> 为背景图片设置初始位置。</p><h2 id="background-attachment"><code>background-attachment</code></h2><p><code>background-attachment</code> CSS属性决定背景图像的位置是在视口内固定，或者随着包含它的区块滚动。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;【6.背景】&lt;/h1&gt;
&lt;h2 id=&quot;background-color&quot;&gt;&lt;code&gt;background-color&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;CSS属性中的&lt;code&gt;background-color&lt;/code&gt;会设置元素的背景色,
属性</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%905.%E5%AD%97%E4%BD%93%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%905.%E5%AD%97%E4%BD%93%E3%80%91/</id>
    <published>2024-12-03T12:34:42.839Z</published>
    <updated>2024-12-03T12:38:40.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="字体">【5.字体】</h1><h2 id="font-size"><code>font-size</code></h2><p><code>font-size</code> CSS属性指定字体的大小。因为该属性的值会被用于计算em和ex长度单位，定义该值可能改变其他元素的大小。</p><h2 id="font-style"><code>font-style</code></h2><p><code>font-style</code> CSS 属性允许你选择 <code>font-family</code>字体下的 italic 或 oblique 样式。</p><h2 id="font-weight"><code>font-weight</code></h2><p><code>font-weight</code> CSS 属性指定了字体的粗细程度。一些字体只提供 normal 和 bold 两种值。</p><h2 id="font-family"><code>font-family</code></h2><p>CSS 属性 <code>font-family</code>允许您通过给定一个有先后顺序的，由字体名或者字体族名组成的列表来为选定的元素设置字体。属性值用逗号隔开。浏览器会选择列表中第一个该计算机上有安装的字体，或者是通过<span class="citation" data-cites="font-face">@font-face</span>指定的可以直接下载的字体。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;字体&quot;&gt;【5.字体】&lt;/h1&gt;
&lt;h2 id=&quot;font-size&quot;&gt;&lt;code&gt;font-size&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;font-size&lt;/code&gt; CSS
属性指定字体的大小。因为该属性的值会被用于计算em和ex长度单位，定义该</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%904.%E6%96%87%E6%9C%AC%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%904.%E6%96%87%E6%9C%AC%E3%80%91/</id>
    <published>2024-12-03T12:34:42.815Z</published>
    <updated>2024-12-03T12:38:23.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文本">【4.文本】</h1><h2 id="text-align"><code>text-align</code></h2><p><code>text-align</code>CSS属性定义行内内容（例如文字）如何相对它的块父元素对齐。text-align并不控制块元素自己的对齐，只控制它的行内内容的对齐。</p><h2 id="line-height"><code>line-height</code></h2><p><code>line-height</code> CSS属性用于设置多行元素的空间量，如多行文本的间距。对于块级元素，它指定元素行盒（lineboxes）的最小高度。对于非替代的 inline 元素，它用于计算行盒（linebox）的高度。</p><ul><li><p>补充知识点：长度单位</p><table><thead><tr class="header"><th>单位</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>px</td><td>设备上的像素点</td></tr><tr class="even"><td>%</td><td>相对于父元素的百分比</td></tr><tr class="odd"><td>em</td><td>相对于当前元素的字体大小</td></tr><tr class="even"><td>rem</td><td>相对于根元素的字体大小</td></tr><tr class="odd"><td>vw</td><td>相对于视窗宽度的百分比</td></tr><tr class="even"><td>vh</td><td>相对于视窗高度的百分比</td></tr></tbody></table></li></ul><h2 id="letter-spacing"><code>letter-spacing</code></h2><p>CSS 的 <code>letter-spacing</code> 属性用于设置文本字符的间距。</p><h2 id="text-indent"><code>text-indent</code></h2><p><code>text-indent</code>属性能定义一个块元素首行文本内容之前的缩进量。</p><h2 id="text-decoration"><code>text-decoration</code></h2><p><code>text-decoration</code> 这个 CSS属性是用于设置文本的修饰线外观的（下划线、上划线、贯穿线/删除线 或闪烁）它是 <code>text-decoration-line</code>,<code>text-decoration-color</code>, <code>text-decoration-style</code>,和新出现的 <code>text-decoration-thickness</code> 属性的缩写。</p><h2 id="text-shadow"><code>text-shadow</code></h2><p><code>text-shadow</code>为文字添加阴影。可以为文字与<code>text-decorations</code>添加多个阴影，阴影值之间用逗号隔开。每个阴影值由元素在X和Y方向的偏移量、模糊半径和颜色值组成。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;文本&quot;&gt;【4.文本】&lt;/h1&gt;
&lt;h2 id=&quot;text-align&quot;&gt;&lt;code&gt;text-align&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;text-align&lt;/code&gt;
CSS属性定义行内内容（例如文字）如何相对它的块父元素对齐。text-al</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%903.%E9%A2%9C%E8%89%B2%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%903.%E9%A2%9C%E8%89%B2%E3%80%91/</id>
    <published>2024-12-03T12:34:42.790Z</published>
    <updated>2024-12-03T12:38:08.287Z</updated>
    
    <content type="html"><![CDATA[<h1 id="颜色">【3.颜色】</h1><h2 id="预定义的颜色值">预定义的颜色值</h2><p>black、white、red、green、blue、lightblue等。</p><h2 id="进制表示法">16进制表示法</h2><p>使用6位16进制数表示颜色，例如：<code>#ADD8E6</code>。</p><p>其中第1-2位表示红色，第3-4位表示绿色，第5-6位表示蓝色。</p><p>简写方式：<code>#ABC</code>，等价于<code>#AABBCC</code>。</p><h2 id="rgb表示法">RGB表示法</h2><p><code>rgb(173, 216, 230)</code>。</p><p>其中第一个数表示红色，第二个数表示绿色，第三个数表示蓝色。</p><h2 id="rgba表示法">RGBA表示法</h2><p><code>rgba(173, 216, 230, 0.5)</code>。</p><p>前三个数同上，第四个数表示透明度。</p><h2 id="取色方式">取色方式</h2><ul><li>网页里的颜色，可以在chrome的调试模式下获取</li><li>其他颜色可以使用QQ的截图软件：<ul><li>直接按<code>c</code>键，可以复制rgb颜色值</li><li>按住<code>shift</code>再按<code>c</code>键，可以复制16进制颜色值</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;颜色&quot;&gt;【3.颜色】&lt;/h1&gt;
&lt;h2 id=&quot;预定义的颜色值&quot;&gt;预定义的颜色值&lt;/h2&gt;
&lt;p&gt;black、white、red、green、blue、lightblue等。&lt;/p&gt;
&lt;h2 id=&quot;进制表示法&quot;&gt;16进制表示法&lt;/h2&gt;
&lt;p&gt;使用6位1</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%902.%E9%80%89%E6%8B%A9%E5%99%A8%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%902.%E9%80%89%E6%8B%A9%E5%99%A8%E3%80%91/</id>
    <published>2024-12-03T12:34:42.767Z</published>
    <updated>2024-12-03T12:37:51.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="选择器">【2.选择器】</h1><h2 id="标签选择器">标签选择器</h2><p>选择所有<code>div</code>标签：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span> &#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">200px</span>;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">200px</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: gray;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="id选择器">ID选择器</h2><p>选择ID为<code>rect-1</code>的标签：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#rect-1</span> &#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">200px</span>;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">200px</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: gray;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="类选择器">类选择器</h2><p>选择所有<code>rectangle</code>类的标签：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.rectangle</span> &#123;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">200px</span>;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">200px</span>;</span><br><span class="line">    <span class="attribute">background-color</span>: gray;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="伪类选择器">伪类选择器</h2><p>伪类用于定义元素的特殊状态。</p><p>链接伪类选择器：</p><ul><li><code>:link</code>：链接访问前的样式</li><li><code>:visited</code>：链接访问后的样式</li><li><code>:hover</code>：鼠标悬停时的样式</li><li><code>:active</code>：鼠标点击后长按时的样式</li><li><code>:focus</code>：聚焦后的样式</li></ul><p>位置伪类选择器：</p><ul><li><code>:nth-child(n)</code>：选择是其父标签第n个子元素的所有元素。</li></ul><p>目标伪类选择器：</p><ul><li>:target：当url指向该元素时生效。</li></ul><h2 id="复合选择器">复合选择器</h2><p>由两个及以上基础选择器组合而成的选择器。</p><ul><li><code>element1, element2</code>：同时选择元素<code>element1</code>和元素<code>element2</code>。</li><li><code>element.class</code>：选则包含某类的<code>element</code>元素。</li><li><code>element1 + element2</code>：选择紧跟<code>element1</code>的<code>element2</code>元素。</li><li><code>element1 element2</code>：选择<code>element1</code>内的所有<code>element2</code>元素。</li><li><code>element1 &gt; element2</code>：选择父标签是<code>element1</code>的所有<code>element2</code>元素。</li></ul><h2 id="通配符选择器">通配符选择器</h2><ul><li><code>*</code>：选择所有标签</li><li><code>[attribute]</code>：选择具有某个属性的所有标签</li><li><code>[attribute=value]</code>：选择<code>attribute</code>值为<code>value</code>的所有标签</li></ul><h2 id="伪元素选择器">伪元素选择器</h2><p>将特定内容当做一个元素，选择这些元素的选择器被称为伪元素选择器。</p><ul><li><code>::first-letter</code>：选择第一个字母</li><li><code>::first-line</code>：选择第一行</li><li><code>::selection</code>：选择已被选中的内容</li><li><code>::after</code>：可以在元素后插入内容</li><li><code>::before</code>：可以在元素前插入内容</li></ul><h2 id="样式渲染优先级">样式渲染优先级</h2><ul><li>权重大小，越具体的选择器权重越大：<code>!important</code> &gt;行内样式 &gt; ID选择器 &gt; 类与伪类选择器 &gt; 标签选择器 &gt;通用选择器</li><li>权重相同时，后面的样式会覆盖前面的样式</li><li>继承自父元素的权重最低</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;选择器&quot;&gt;【2.选择器】&lt;/h1&gt;
&lt;h2 id=&quot;标签选择器&quot;&gt;标签选择器&lt;/h2&gt;
&lt;p&gt;选择所有&lt;code&gt;div&lt;/code&gt;标签：&lt;/p&gt;
&lt;figure class=&quot;highlight css&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gu</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%901.%E6%A0%B7%E5%BC%8F%E5%AE%9A%E4%B9%89%E6%96%B9%E5%BC%8F%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/12/03/[object%20Object]/%E5%89%8D%E7%AB%AF/css/%E3%80%901.%E6%A0%B7%E5%BC%8F%E5%AE%9A%E4%B9%89%E6%96%B9%E5%BC%8F%E3%80%91/</id>
    <published>2024-12-03T12:34:42.737Z</published>
    <updated>2024-12-03T12:37:26.106Z</updated>
    
    <content type="html"><![CDATA[<h1 id="样式定义方式">【1.样式定义方式】</h1><h2 id="行内样式表inline-style-sheet">行内样式表（inline stylesheet）</h2><p>直接定义在标签的<code>style</code>属性中。</p><ul><li>作用范围：仅对当前标签产生影响。</li></ul><p>例如：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;/images/mountain.jpg&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;&quot;</span> <span class="attr">style</span>=<span class="string">&quot;width: 300px; height: 200px;&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="内部样式表internal-style-sheet">内部样式表（internal stylesheet）</h2><p>定义在<code>style</code>标签中，通过选择器影响对应的标签。</p><ul><li>作用范围：可以对同一个页面中的多个元素产生影响。</li></ul><h2 id="外部样式表external-style-sheet">外部样式表（external stylesheet）</h2><p>定义在css样式文件中，通过选择器影响对应的标签。可以用link标签引入某些页面。</p><ul><li>作用范围：可以对多个页面产生影响。</li></ul><h2 id="注释">注释</h2><p>注意不能使用//。</p><p>只有：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 注释 */</span> </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;样式定义方式&quot;&gt;【1.样式定义方式】&lt;/h1&gt;
&lt;h2 id=&quot;行内样式表inline-style-sheet&quot;&gt;行内样式表（inline style
sheet）&lt;/h2&gt;
&lt;p&gt;直接定义在标签的&lt;code&gt;style&lt;/code&gt;属性中。&lt;/p&gt;
&lt;u</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>【二叉树】</title>
    <link href="https://cuiluyi.github.io/2024/08/23/[object%20Object]/%E7%AE%97%E6%B3%95/%E3%80%90%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91/"/>
    <id>https://cuiluyi.github.io/2024/08/23/[object%20Object]/%E7%AE%97%E6%B3%95/%E3%80%90%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%91/</id>
    <published>2024-08-22T17:02:27.000Z</published>
    <updated>2024-11-13T14:13:34.984Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二叉树">二叉树</h1><h2 id="存储">存储</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r;<span class="comment">// l[i] 和 r[i] 分别存储节点 i 的左、右孩子编号</span></span><br></pre></td></tr></table></figure><blockquote><p>不定义为 <code>int l[N], r[N]</code> 的原因是：二叉树的结点个数最大为N，但是结点权值可以大于N，此时就会导致段错误，而定义成哈希表就避免了很多麻烦</p></blockquote><h2 id="非递归遍历">非递归遍历</h2><h3 id="结论">结论</h3><p>【<strong>结论</strong>】用 <strong>栈</strong> 模拟实现<strong>中序遍历</strong>，<font color='red'> <strong>Push</strong></font> 操作的数据过程是 <font color='blue'> <strong>先序</strong></font> 遍历，<font color='red'> <strong>Pop</strong> </font>操作的数据过程是 <font color='blue'> <strong>中序</strong> </font>遍历</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408232259283.png"alt="树的遍历.png" /><figcaption aria-hidden="true">树的遍历.png</figcaption></figure><p>​ 如图所示，⊗ 是先序遍历，☆ 是中序遍历，△ 是后序遍历。我们发现：树的<strong>前序、中序、后序</strong> 实际上都是将整棵树以<strong>上图所示的路线</strong> 跑了 <spanclass="math inline">\(1\)</span> 遍，每个结点都碰到了 <spanclass="math inline">\(3\)</span> 次，三者唯一不同之处在于<strong>访问节点的时机不同</strong></p><ul><li><strong>先序</strong> 遍历在第 <spanclass="math inline">\(1\)</span> 次碰到结点时访问</li><li><strong>中序</strong> 遍历在第 <spanclass="math inline">\(2\)</span> 次碰到结点时访问</li><li><strong>后序</strong> 遍历在第 <spanclass="math inline">\(3\)</span> 次碰到结点时访问</li></ul><h3 id="例题">例题</h3><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8792912/">AcWing1576. 再次树遍历 - AcWing</a></li></ul><span id="more"></span><h2 id="层序遍历">层序遍历</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> q[N];</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">bfs</span><span class="params">(<span class="type">int</span> root)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> hh = <span class="number">0</span>, tt = <span class="number">-1</span>;</span><br><span class="line">    q[++ tt] = root;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(hh &lt;= tt)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">auto</span> t = q[hh ++];</span><br><span class="line">        <span class="keyword">if</span>(l.count(t))<span class="comment">// 存在左孩子</span></span><br><span class="line">            q[++ tt] = l[t];</span><br><span class="line">        <span class="keyword">if</span>(r.count(t))<span class="comment">// 存在右孩子</span></span><br><span class="line">            q[++ tt] = r[t];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bfs(root);</span><br><span class="line"><span class="comment">// 输出层序序列</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, q[i]);</span><br></pre></td></tr></table></figure><h2 id="后序中序建树">后序中序建树</h2><h3 id="模板">模板</h3><ul><li><strong>时间复杂度</strong>：<spanclass="math inline">\(O(n)\)</span></li></ul><blockquote><p>注意：前提是二叉树中<strong>节点编号或权值互不相同</strong>，我们才能用<strong>哈希表</strong>记录中序序列各节点对应的下标，从而将时间复杂度优化为 <spanclass="math inline">\(O(n)\)</span>。如果<strong>二叉树节点编号或权值可能重复</strong>，则只能遍历搜索位置，此时时间复杂度为<span class="math inline">\(O(n^2)\)</span></p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> in[N], post[N];<span class="comment">// n 个节点的中序序列、后序序列</span></span><br><span class="line"><span class="type">int</span> pre[N], cnt;<span class="comment">// 建图的同时记录 n 个节点的前序序列</span></span><br><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r, pos;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">build</span><span class="params">(<span class="type">int</span> il, <span class="type">int</span> ir, <span class="type">int</span> pl, <span class="type">int</span> pr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> root = post[pr];</span><br><span class="line">    <span class="type">int</span> k = pos[root];<span class="comment">// 优化时间复杂度 O(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 记录前序序列 */</span></span><br><span class="line">    <span class="comment">// pre[cnt ++] = root;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(il &lt; k)</span><br><span class="line">        l[root] = build(il, k - <span class="number">1</span>, pl, pl + (k - <span class="number">1</span> - il));</span><br><span class="line">    <span class="keyword">if</span>(k &lt; ir)</span><br><span class="line">        r[root] = build(k + <span class="number">1</span>, ir, pl + (k - <span class="number">1</span> - il) + <span class="number">1</span>, pr - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;post[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;in[i]);</span><br><span class="line">        pos[in[i]] = i;        <span class="comment">// 哈希表记录每个数在中序遍历的下标</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// root 为二叉树的根节点</span></span><br><span class="line">    <span class="type">int</span> root = build(<span class="number">0</span>, n - <span class="number">1</span>, <span class="number">0</span>, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题-1">例题</h3><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8790598/">AcWing1497. 树的遍历 - AcWing</a></li><li><ahref="https://www.acwing.com/activity/content/code/content/8790623/">AcWing1620. Z 字形遍历二叉树 - AcWing</a></li></ul><h2 id="中序建树">中序建树</h2><h3 id="模板-1">模板</h3><ul><li><strong>时间复杂度</strong>：<spanclass="math inline">\(O(n)\)</span></li></ul><blockquote><p>注意：前提是二叉树中每个节点的权值互不相同，我们才能用哈希表记录中序序列各节点对应的下标</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> in[N], pre[N];<span class="comment">// n 个节点的中序序列、前序序列</span></span><br><span class="line"><span class="type">int</span> post[N], cnt;<span class="comment">// 建图的同时记录 n 个节点的后序序列</span></span><br><span class="line"><span class="built_in">unordered_map</span>&lt;<span class="type">int</span>, <span class="type">int</span>&gt; l, r, pos;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">build</span><span class="params">(<span class="type">int</span> il, <span class="type">int</span> ir, <span class="type">int</span> pl, <span class="type">int</span> pr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> root = pre[pl];</span><br><span class="line">    <span class="type">int</span> k = pos[root];<span class="comment">// 优化时间复杂度 O(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(il &lt; k)</span><br><span class="line">        l[root] = build(il, k - <span class="number">1</span>, pl + <span class="number">1</span>, pl + <span class="number">1</span> + k - <span class="number">1</span> - il);</span><br><span class="line">    <span class="keyword">if</span>(k &lt; ir)</span><br><span class="line">        r[root] = build(k + <span class="number">1</span>, ir, pl + <span class="number">1</span> + k - <span class="number">1</span> - il + <span class="number">1</span>, pr);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 记录后序序列 */</span></span><br><span class="line">    <span class="comment">// post[cnt ++] = root;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;pre[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;in[i]);</span><br><span class="line">        pos[in[i]] = i;        <span class="comment">// 哈希表记录每个数在中序遍历的下标</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// root 为二叉树的根节点</span></span><br><span class="line">    <span class="type">int</span> root = build(<span class="number">0</span>, n - <span class="number">1</span>, <span class="number">0</span>, n - <span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="例题-2">例题</h3><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8790607/">AcWing1631. 后序遍历 - AcWing</a></li><li><a href="https://www.acwing.com/solution/content/251933/">AcWing2019 清华软院 T2. 二叉树算权 - AcWing</a></li></ul><h2 id="前序和后序">前序和后序</h2><p>已知二叉树的前序序列和后序序列，<strong>无法唯一确定</strong>这个二叉树，但是我们可以确定每个子树的形状和个数，仅仅是子树的位置不能确定</p><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8793054/">AcWing1609. 前序和后序遍历 - AcWing</a></li><li><a href="https://www.acwing.com/solution/content/251849/">AcWing3486. 前序和后序 - AcWing</a></li></ul><h1 id="完全二叉树">完全二叉树</h1><h2 id="存储-1">存储</h2><p>完全二叉树采用 <strong>数组</strong> 存储</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br></pre></td></tr></table></figure><h2 id="性质">性质</h2><p>完全二叉树的性质如下：</p><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408230026116.jpeg"alt="7476a7cbe9d1fbd33df03cf13ab37b4" /><figcaptionaria-hidden="true">7476a7cbe9d1fbd33df03cf13ab37b4</figcaption></figure><ol type="1"><li><strong><font color='red'> 从 1 号单元开始存储树节点</font></strong>（0 号单元存节点的个数）</li><li>节点 <span class="math inline">\(i\)</span> 左子树是 <spanclass="math inline">\(2i\)</span>，右子树是 <spanclass="math inline">\(2i+1\)</span>，根节点是 <spanclass="math inline">\(\lfloor i/2 \rfloor\)</span>，左兄弟是 <spanclass="math inline">\(i-1\)</span>，右兄弟 <spanclass="math inline">\(i+1\)</span></li><li>第 <span class="math inline">\(d\)</span> 层最多有 <spanclass="math inline">\(2^{d-1}\)</span>，<strong>起始节点</strong> 编号为<span class="math inline">\(2^{d-1}\)</span>（<spanclass="math inline">\(d\)</span> 从 <spanclass="math inline">\(1\)</span> 开始）</li><li>如果完全二叉树一共有 <span class="math inline">\(n\)</span>个节点，则 <strong>非</strong> 叶子节点为 <spanclass="math inline">\(T[0…n/2]\)</span>，叶节点为 <spanclass="math inline">\(T[n/2+1,…,n]\)</span></li></ol><h2 id="例题-3">例题</h2><ul><li><a href="https://www.acwing.com/solution/content/251776/">AcWing1240. 完全二叉树的权值 - AcWing</a></li></ul><h1 id="二叉搜索树bst">二叉搜索树(BST)</h1><p>==<strong><font color='blue'> 二叉搜索树 </font></strong> ==<strong><font color='blue'> 二叉查找树 </font></strong> （Binary SearchTree） == <strong><font color='blue'> 二叉排序树 </font></strong>（Binary Sort Tree）==</p><ul><li>若它的左子树不空，则 <strong>左子树</strong> 上<strong>所有</strong> 结点的值均 <font color='red'><strong>小于</strong> </font> 根结点的值;</li><li>若它的右子树不空，则 <strong>右子树</strong> 上<strong>所有</strong> 结点的值均 <font color='red'><strong>大于等于</strong> </font> 根结点的值;</li><li>它的左、右子树也都分别是 <strong>二又搜索树</strong></li></ul><blockquote><p>注意：上述定义在不同题目中，等号的位置可能不一样（即也有可能左子树均小于等于根节点，右子树均大于根节点）</p></blockquote><h2 id="性质-1">性质</h2><ul><li><strong>二叉排序树</strong> 的 <strong><font color='red'> 中序遍历</font></strong> 是 <strong><font color='gree'> 递增 </font></strong>序列</li></ul><blockquote><p>在构造二叉排序树时，若关键字序列有序，则二叉排序树的高度最大</p></blockquote><h2 id="例题-4">例题</h2><ul><li><ahref="https://www.acwing.com/activity/content/code/content/8790764/">AcWing1527. 判断二叉搜索树 - AcWing</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;二叉树&quot;&gt;二叉树&lt;/h1&gt;
&lt;h2 id=&quot;存储&quot;&gt;存储&lt;/h2&gt;
&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;unordered_map&lt;/span&gt;&amp;lt;&lt;span class=&quot;type&quot;&gt;int&lt;/span&gt;, &lt;span class=&quot;type&quot;&gt;int&lt;/span&gt;&amp;gt; l, r;	&lt;span class=&quot;comment&quot;&gt;// l[i] 和 r[i] 分别存储节点 i 的左、右孩子编号&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;不定义为 &lt;code&gt;int l[N], r[N]&lt;/code&gt; 的原因是：二叉树的结点个数最大为
N，但是结点权值可以大于
N，此时就会导致段错误，而定义成哈希表就避免了很多麻烦&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;非递归遍历&quot;&gt;非递归遍历&lt;/h2&gt;
&lt;h3 id=&quot;结论&quot;&gt;结论&lt;/h3&gt;
&lt;p&gt;【&lt;strong&gt;结论&lt;/strong&gt;】用 &lt;strong&gt;栈&lt;/strong&gt; 模拟实现
&lt;strong&gt;中序遍历&lt;/strong&gt;，&lt;font color=&#39;red&#39;&gt; &lt;strong&gt;Push&lt;/strong&gt;
&lt;/font&gt; 操作的数据过程是 &lt;font color=&#39;blue&#39;&gt; &lt;strong&gt;先序&lt;/strong&gt;
&lt;/font&gt; 遍历，&lt;font color=&#39;red&#39;&gt; &lt;strong&gt;Pop&lt;/strong&gt; &lt;/font&gt;
操作的数据过程是 &lt;font color=&#39;blue&#39;&gt; &lt;strong&gt;中序&lt;/strong&gt; &lt;/font&gt;
遍历&lt;/p&gt;
&lt;figure&gt;
&lt;img
src=&quot;https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408232259283.png&quot;
alt=&quot;树的遍历.png&quot; /&gt;
&lt;figcaption aria-hidden=&quot;true&quot;&gt;树的遍历.png&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;​ 如图所示，⊗ 是先序遍历，☆ 是中序遍历，△ 是后序遍历。我们发现：树的
&lt;strong&gt;前序、中序、后序&lt;/strong&gt; 实际上都是将整棵树以
&lt;strong&gt;上图所示的路线&lt;/strong&gt; 跑了 &lt;span
class=&quot;math inline&quot;&gt;&#92;(1&#92;)&lt;/span&gt; 遍，每个结点都碰到了 &lt;span
class=&quot;math inline&quot;&gt;&#92;(3&#92;)&lt;/span&gt; 次，三者唯一不同之处在于
&lt;strong&gt;访问节点的时机不同&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;先序&lt;/strong&gt; 遍历在第 &lt;span
class=&quot;math inline&quot;&gt;&#92;(1&#92;)&lt;/span&gt; 次碰到结点时访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中序&lt;/strong&gt; 遍历在第 &lt;span
class=&quot;math inline&quot;&gt;&#92;(2&#92;)&lt;/span&gt; 次碰到结点时访问&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后序&lt;/strong&gt; 遍历在第 &lt;span
class=&quot;math inline&quot;&gt;&#92;(3&#92;)&lt;/span&gt; 次碰到结点时访问&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;例题&quot;&gt;例题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a
href=&quot;https://www.acwing.com/activity/content/code/content/8792912/&quot;&gt;AcWing
1576. 再次树遍历 - AcWing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="算法" scheme="https://cuiluyi.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam详解</title>
    <link href="https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <id>https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/8.Adam%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/</id>
    <published>2024-08-13T02:02:27.000Z</published>
    <updated>2024-11-13T14:14:43.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adam-算法详解">Adam 算法详解</h1><p>Adam 算法在 RMSProp 算法基础上对 <ahref="https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;spm=1001.2101.3001.7020">小批量</a>随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。</p><blockquote><p>所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。adam算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶moment 和二阶moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了 Adagrad算法的处理稀疏型数据，又结合了 RMSProp 算法的可以处理非稳态的数据。</p></blockquote><p>小tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个<ahref="https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;vd_source=12c80a98ec9426002a2f54318421082c">优化算法系列合集</a>，个人觉得比较容易听懂</p><h2 id="算法">算法</h2><p>Adam 算法使用了动量变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> ​和 RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量 <spanclass="math inline">\(\boldsymbol{s}_t\)</span> ​，并在时间步 <spanclass="math inline">\(0\)</span> 将它们中每个元素初始化为 <spanclass="math inline">\(0\)</span>。给定超参数 <spanclass="math inline">\(0 \leq \beta_1 &lt; 1\)</span> （算法作者建议设为<span class="math inline">\(0.9\)</span>），时间步 <spanclass="math inline">\(t\)</span> 的动量变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> ​即小批量随机梯度 <spanclass="math inline">\(\boldsymbol{g}_t\)</span> ​的指数加权移动平均：</p><p><span class="math display">\[\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1)\boldsymbol{g}_t\]</span> 和 RMSProp 算法中一样，给定超参数 <spanclass="math inline">\(0 \leq \beta_2 &lt; 1\)</span> （算法作者建议设为0.999）</p><p>将小批量随机梯度按元素平方后的项 <spanclass="math inline">\(\boldsymbol{g}_t \odot \boldsymbol{g}_t\)</span>​做指数加权移动平均得到 <spanclass="math inline">\(\boldsymbol{s}_t\)</span>​： <spanclass="math display">\[\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2)\boldsymbol{g}_t \odot \boldsymbol{g}_t\]</span> 由于我们将 <spanclass="math inline">\(\boldsymbol{v}_0\)</span> 和 <spanclass="math inline">\(\boldsymbol{s}_0\)</span> 中的元素都初始化为 <spanclass="math inline">\(0\)</span></p><p>在时间步 <span class="math inline">\(t\)</span> 我们得到 <spanclass="math inline">\(\boldsymbol{v}_t = (1-\beta_1) \sum_{i=1}^t\beta_1^{t-i}\boldsymbol{g}_i\)</span>。将过去各时间步小批量随机梯度的权值相加，得到<span class="math inline">\((1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 -\beta_1^t\)</span>。需要注意的是，当 <spanclass="math inline">\(t\)</span>较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 <spanclass="math inline">\(\beta_1 = 0.9\)</span> 时，<spanclass="math inline">\(\boldsymbol{v}_1 =0.1\boldsymbol{g}_1\)</span>。为了消除这样的影响，对于任意时间步 <spanclass="math inline">\(t\)</span> ，我们可以将 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> 再除以 <spanclass="math inline">\(1 -\beta_1^t\)</span>，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作 <strong>偏差修正</strong>。在 Adam 算法中，我们对变量 <spanclass="math inline">\(\boldsymbol{v}_t\)</span> 和 <spanclass="math inline">\(\boldsymbol{s}_t\)</span> 均作偏差修正： <spanclass="math display">\[\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}\]</span></p><p><span class="math display">\[\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}\]</span></p><p>接下来，Adam 算法使用以上偏差修正后的变量 $_t $ 和 <spanclass="math inline">\(\hat{\boldsymbol{s}}_t\)</span>，将模型参数中每个元素的学习率通过按元素运算重新调整：</p><p><span class="math display">\[\boldsymbol{g}_t&#39; \leftarrow \frac{\eta\hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}\]</span> 其中 <span class="math inline">\(\eta\)</span> 是学习率，<spanclass="math inline">\(\epsilon\)</span>是为了维持数值稳定性而添加的常数，如 <spanclass="math inline">\(10^{-8}\)</span> 。和 AdaGrad 算法、RMSProp算法以及 AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用<span class="math inline">\(\boldsymbol{g}_t&#39;\)</span>​迭代自变量：</p><p><span class="math display">\[\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t&#39;\]</span></p><span id="more"></span><h2 id="从零开始实现">从零开始实现</h2><p>我们按照 Adam 算法中的公式实现该算法。其中时间步 t t t 通过<code>hyperparams</code> 参数传入 <code>adam</code> 函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">features, labels = d2l.get_data_ch7()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_adam_states</span>():</span><br><span class="line">    v_w, v_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    s_w, s_b = torch.zeros((features.shape[<span class="number">1</span>], <span class="number">1</span>), dtype=torch.float32), torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adam</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    beta1, beta2, eps = <span class="number">0.9</span>, <span class="number">0.999</span>, <span class="number">1e-6</span></span><br><span class="line">    <span class="keyword">for</span> p, (v, s) <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1</span> - beta1) * p.grad.data</span><br><span class="line">        s[:] = beta2 * s + (<span class="number">1</span> - beta2) * p.grad.data**<span class="number">2</span></span><br><span class="line">        v_bias_corr = v / (<span class="number">1</span> - beta1 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        s_bias_corr = s / (<span class="number">1</span> - beta2 ** hyperparams[<span class="string">&#x27;t&#x27;</span>])</span><br><span class="line">        p.data -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)</span><br><span class="line">    hyperparams[<span class="string">&#x27;t&#x27;</span>] += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>使用学习率为 0.01 的 Adam 算法来训练模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch7(adam, init_adam_states(), &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;t&#x27;</span>: <span class="number">1</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.245370, 0.065155 sec per epoch</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202411132136580.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="简洁实现">简洁实现</h2><p>通过名称为“Adam”的优化器实例，我们便可使用 PyTorch 提供的 Adam算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_pytorch_ch7(torch.optim.Adam, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;, features, labels)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss: 0.242066, 0.056867 sec per epoch</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202411132136204.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="小结">小结</h2><ul><li>Adam 算法在 RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。</li><li>Adam 算法使用了偏差修正。</li></ul><h2 id="参考文献">参考文献</h2><p>[1] Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochasticoptimization. arXiv preprint arXiv: 1412.6980.</p><hr /><blockquote><p>注：除代码外本节与原书此节基本相同，<ahref="https://zh.d2l.ai/chapter_optimization/adam.html">原书传送门</a></p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;adam-算法详解&quot;&gt;Adam 算法详解&lt;/h1&gt;
&lt;p&gt;Adam 算法在 RMSProp 算法基础上对 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E5%B0%8F%E6%89%B9%E9%87%8F&amp;amp;spm=1001.2101.3001.7020&quot;&gt;小批量&lt;/a&gt;
随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。adam
算法是一种基于“momentum”思想的随机梯度下降优化方法，通过迭代更新之前每次计算梯度的一阶
moment 和二阶
moment，并计算滑动平均值，后用来更新当前的参数。这种思想结合了 Adagrad
算法的处理稀疏型数据，又结合了 RMSProp 算法的可以处理非稳态的数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;小
tips：跟我一样基础不太好的看起来比较难以理解，建议搭配视频食用，可参考这个
&lt;a
href=&quot;https://www.bilibili.com/video/BV1HP4y1g7xN/?spm_id_from=pageDriver&amp;amp;vd_source=12c80a98ec9426002a2f54318421082c&quot;&gt;优化算法系列合集&lt;/a&gt;，个人觉得比较容易听懂&lt;/p&gt;
&lt;h2 id=&quot;算法&quot;&gt;算法&lt;/h2&gt;
&lt;p&gt;Adam 算法使用了动量变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; ​和 RMSProp
算法中小批量随机梯度按元素平方的指数加权移动平均变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_t&#92;)&lt;/span&gt; ​，并在时间步 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0&#92;)&lt;/span&gt; 将它们中每个元素初始化为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0&#92;)&lt;/span&gt;。给定超参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0 &#92;leq &#92;beta_1 &amp;lt; 1&#92;)&lt;/span&gt; （算法作者建议设为
&lt;span class=&quot;math inline&quot;&gt;&#92;(0.9&#92;)&lt;/span&gt;），时间步 &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; 的动量变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; ​即小批量随机梯度 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{g}_t&#92;)&lt;/span&gt; ​的指数加权移动平均：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{v}_t &#92;leftarrow &#92;beta_1 &#92;boldsymbol{v}_{t-1} + (1 - &#92;beta_1)
&#92;boldsymbol{g}_t
&#92;]&lt;/span&gt; 和 RMSProp 算法中一样，给定超参数 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0 &#92;leq &#92;beta_2 &amp;lt; 1&#92;)&lt;/span&gt; （算法作者建议设为
0.999）&lt;/p&gt;
&lt;p&gt;将小批量随机梯度按元素平方后的项 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{g}_t &#92;odot &#92;boldsymbol{g}_t&#92;)&lt;/span&gt;
​做指数加权移动平均得到 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_t&#92;)&lt;/span&gt;​： &lt;span
class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{s}_t &#92;leftarrow &#92;beta_2 &#92;boldsymbol{s}_{t-1} + (1 - &#92;beta_2)
&#92;boldsymbol{g}_t &#92;odot &#92;boldsymbol{g}_t
&#92;]&lt;/span&gt; 由于我们将 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_0&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_0&#92;)&lt;/span&gt; 中的元素都初始化为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(0&#92;)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在时间步 &lt;span class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; 我们得到 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t = (1-&#92;beta_1) &#92;sum_{i=1}^t
&#92;beta_1^{t-i}
&#92;boldsymbol{g}_i&#92;)&lt;/span&gt;。将过去各时间步小批量随机梯度的权值相加，得到
&lt;span class=&quot;math inline&quot;&gt;&#92;((1-&#92;beta_1) &#92;sum_{i=1}^t &#92;beta_1^{t-i} = 1 -
&#92;beta_1^t&#92;)&lt;/span&gt;。需要注意的是，当 &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt;
较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_1 = 0.9&#92;)&lt;/span&gt; 时，&lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_1 =
0.1&#92;boldsymbol{g}_1&#92;)&lt;/span&gt;。为了消除这样的影响，对于任意时间步 &lt;span
class=&quot;math inline&quot;&gt;&#92;(t&#92;)&lt;/span&gt; ，我们可以将 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; 再除以 &lt;span
class=&quot;math inline&quot;&gt;&#92;(1 -
&#92;beta_1^t&#92;)&lt;/span&gt;，从而使过去各时间步小批量随机梯度权值之和为
1。这也叫作 &lt;strong&gt;偏差修正&lt;/strong&gt;。在 Adam 算法中，我们对变量 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{v}_t&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{s}_t&#92;)&lt;/span&gt; 均作偏差修正： &lt;span
class=&quot;math display&quot;&gt;&#92;[
&#92;hat{&#92;boldsymbol{v}}_t &#92;leftarrow &#92;frac{&#92;boldsymbol{v}_t}{1 - &#92;beta_1^t}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;hat{&#92;boldsymbol{s}}_t &#92;leftarrow &#92;frac{&#92;boldsymbol{s}_t}{1 - &#92;beta_2^t}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;接下来，Adam 算法使用以上偏差修正后的变量 $_t $ 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;hat{&#92;boldsymbol{s}}_t&#92;)&lt;/span&gt;
，将模型参数中每个元素的学习率通过按元素运算重新调整：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{g}_t&amp;#39; &#92;leftarrow &#92;frac{&#92;eta
&#92;hat{&#92;boldsymbol{v}}_t}{&#92;sqrt{&#92;hat{&#92;boldsymbol{s}}_t} + &#92;epsilon}
&#92;]&lt;/span&gt; 其中 &lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;eta&#92;)&lt;/span&gt; 是学习率，&lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;epsilon&#92;)&lt;/span&gt;
是为了维持数值稳定性而添加的常数，如 &lt;span
class=&quot;math inline&quot;&gt;&#92;(10^{-8}&#92;)&lt;/span&gt; 。和 AdaGrad 算法、RMSProp
算法以及 AdaDelta
算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用
&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;boldsymbol{g}_t&amp;#39;&#92;)&lt;/span&gt;
​迭代自变量：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;boldsymbol{x}_t &#92;leftarrow &#92;boldsymbol{x}_{t-1} - &#92;boldsymbol{g}_t&amp;#39;
&#92;]&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Adam</title>
    <link href="https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/"/>
    <id>https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/7.Adam/</id>
    <published>2024-08-13T01:02:27.000Z</published>
    <updated>2024-11-13T14:14:39.956Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adam-算法">Adam 算法</h1><p>​ 接下来，我们将介绍目前常用的梯度下降法中的王者——Adam算法。Adam（Adaptive MomentEstimation）是目前深度学习中最常用的优化算法之一。Adam 算法的核心思想是<strong>利用梯度一阶动量和二阶动量来动态自适应调整学习率</strong>，既保持了<strong>Momentum 收敛速度快</strong> 的优点，又结合了 <strong>RMSProp自适应学习率</strong> 的优点</p><h2 id="基本思想">基本思想</h2><p>Adam 算法通过计算梯度的 <strong>一阶动量</strong>（即<strong>梯度的指数加权移动平均）</strong> 和梯度的<strong>二阶动量</strong>（即<strong>梯度平方的指数加权移动平均</strong>）来<strong>动态调整</strong> 每个参数的<strong>学习率</strong>。具体公式如下：</p><ol type="1"><li>梯度的一阶动量：</li></ol><p><span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t\]</span></p><ol start="2" type="1"><li>梯度的二阶动量：</li></ol><p><span class="math display">\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p><ol start="3" type="1"><li>偏差修正：</li></ol><p><span class="math display">\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}  \]</span></p><p><span class="math display">\[\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\]</span></p><ol start="4" type="1"><li>更新参数：</li></ol><p><span class="math display">\[\theta_{t+1} = \theta_t - \frac{\alpha \hat{m}_t}{\sqrt{\hat{v}_t} +\epsilon}\]</span></p><p>其中：<br />- <span class="math inline">\(\beta_1\)</span> 和 <spanclass="math inline">\(\beta_2\)</span> 分别是 <strong>动量</strong> 和<strong>均方根动量</strong> 的衰减率，常用值为 <spanclass="math inline">\(\beta_1 = 0.9\)</span> 和 <spanclass="math inline">\(\beta_2 = 0.999\)</span> - <spanclass="math inline">\(\epsilon\)</span>是一个很小的常数，用于防止分母为零，常用值为 <spanclass="math inline">\(10^{-8}\)</span></p><span id="more"></span><h2 id="优缺点">优缺点</h2><p><strong>优点</strong>：</p><ul><li><strong>自适应调整学习率</strong>：根据一阶动量和二阶动量动态调整每个参数的学习率，使得训练过程更加稳定。</li><li><strong>收敛速度快</strong>：结合动量法的 <strong>加速特性</strong>和 RMSProp 的 <strong>平稳特性</strong>，能够快速收敛到最优解。</li><li>能处理 <strong>稀疏梯度</strong>，适用于大规模数据和参数。</li></ul><p><strong>缺点</strong>：</p><ul><li>对于某些特定问题，Adam 可能会出现不稳定的收敛行为。<br /></li><li>参数较多：Adam 算法需要调整的 <strong>超参数较多</strong>（例如<span class="math inline">\(\beta_1\)</span> , <spanclass="math inline">\(\beta_2\)</span> , <spanclass="math inline">\(\epsilon\)</span>），调参复杂度高。</li></ul><h2 id="代码实现">代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义Adam优化器</span></span><br><span class="line">optimizer = torch.optim.Adam([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adam&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="adam-与其他算法的比较">Adam 与其他算法的比较</h2><p>Adam 算法集成了 SGD、动量法、Adagrad、Adadelta等多种优化算法的优点，具有快速收敛和稳定的特点。以下是它与其他算法的对比：</p><ol type="1"><li>SGD：基本的随机梯度下降法，收敛速度较慢，易陷入局部最优。</li><li>动量法：在 SGD基础上加入一阶动量，加速收敛，但仍然可能陷入局部最优。</li><li>Adagrad：自适应学习率，但对历史梯度的累积会导致学习率不断减小，后期训练缓慢。</li><li>RMSProp：改进了Adagrad，通过引入衰减系数解决学习率不断减小的问题。</li><li>Adam：结合动量法和 RMSProp的优点，具有快速收敛和稳定的特点，是目前最常用的优化算法。</li></ol><h2 id="小结">小结</h2><p>Adam 算法作为一种自适应的梯度下降优化算法，结合了动量法和 RMSProp的优点，能够有效地加速模型的收敛，同时保持稳定性。它通过计算一阶和二阶动量来动态调整学习率，使得模型在训练过程中能够快速收敛，并适应不同的优化问题。尽管Adam需要调整的超参数较多，但其优越的性能使得它成为深度学习中最广泛使用的优化算法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;adam-算法&quot;&gt;Adam 算法&lt;/h1&gt;
&lt;p&gt;​ 接下来，我们将介绍目前常用的梯度下降法中的王者——Adam
算法。Adam（Adaptive Moment
Estimation）是目前深度学习中最常用的优化算法之一。Adam 算法的核心思想是
&lt;strong&gt;利用梯度一阶动量和二阶动量来动态自适应调整学习率&lt;/strong&gt;，既保持了
&lt;strong&gt;Momentum 收敛速度快&lt;/strong&gt; 的优点，又结合了 &lt;strong&gt;RMSProp
自适应学习率&lt;/strong&gt; 的优点&lt;/p&gt;
&lt;h2 id=&quot;基本思想&quot;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;Adam 算法通过计算梯度的 &lt;strong&gt;一阶动量&lt;/strong&gt;（即
&lt;strong&gt;梯度的指数加权移动平均）&lt;/strong&gt; 和梯度的
&lt;strong&gt;二阶动量&lt;/strong&gt;（即
&lt;strong&gt;梯度平方的指数加权移动平均&lt;/strong&gt;）来
&lt;strong&gt;动态调整&lt;/strong&gt; 每个参数的
&lt;strong&gt;学习率&lt;/strong&gt;。具体公式如下：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;梯度的一阶动量：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
m_t = &#92;beta_1 m_{t-1} + (1 - &#92;beta_1) g_t
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;梯度的二阶动量：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = &#92;beta_2 v_{t-1} + (1 - &#92;beta_2) g_t^2
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&quot;3&quot; type=&quot;1&quot;&gt;
&lt;li&gt;偏差修正：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;hat{m}_t = &#92;frac{m_t}{1 - &#92;beta_1^t}  
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;hat{v}_t = &#92;frac{v_t}{1 - &#92;beta_2^t}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&quot;4&quot; type=&quot;1&quot;&gt;
&lt;li&gt;更新参数：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;theta_{t+1} = &#92;theta_t - &#92;frac{&#92;alpha &#92;hat{m}_t}{&#92;sqrt{&#92;hat{v}_t} +
&#92;epsilon}
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中：&lt;br /&gt;
- &lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;beta_1&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_2&#92;)&lt;/span&gt; 分别是 &lt;strong&gt;动量&lt;/strong&gt; 和
&lt;strong&gt;均方根动量&lt;/strong&gt; 的衰减率，常用值为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_1 = 0.9&#92;)&lt;/span&gt; 和 &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;beta_2 = 0.999&#92;)&lt;/span&gt; - &lt;span
class=&quot;math inline&quot;&gt;&#92;(&#92;epsilon&#92;)&lt;/span&gt;
是一个很小的常数，用于防止分母为零，常用值为 &lt;span
class=&quot;math inline&quot;&gt;&#92;(10^{-8}&#92;)&lt;/span&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>AdaGrad</title>
    <link href="https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.AdaGrad/"/>
    <id>https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5.AdaGrad/</id>
    <published>2024-08-13T00:02:27.000Z</published>
    <updated>2024-11-13T14:14:34.313Z</updated>
    
    <content type="html"><![CDATA[<h1 id="adagrad-算法">AdaGrad 算法</h1><p>在前面我们讲解了 <ahref="https://so.csdn.net/so/search?q=%E5%8A%A8%E9%87%8F%E6%B3%95&amp;spm=1001.2101.3001.7020">动量法</a>（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的AdaGrad 算法。二阶动量的出现，才意味着真正的<strong>自适应学习率</strong> 优化算法时代的到来。</p><h2 id="adagrad-算法的基本思想">AdaGrad 算法的基本思想</h2><p>我们先回顾一下传统的 <ahref="https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">随机梯度下降法</a>（SGD）及其各种变种。它们都是以<strong>同样的学习率</strong> 来更新 <strong>每一个参数</strong>的。但深度神经网络往往包含大量参数，这些参数并不总是<strong>均匀更新</strong> 的。有些参数更新得频繁，有些则很少更新。</p><ul><li>对于 <strong>经常更新</strong>的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的<strong>学习率小一些</strong></li><li>对于 <strong>偶尔更新</strong>的参数，我们了解的信息较少，希望从每一个样本中多学一些，即<strong>学习率大一些</strong></li></ul><p>要动态度量历史更新的频率，我们引入<strong>二阶动量</strong>。二阶动量通过将每一位各自的历史梯度的<strong>平方</strong> 叠加起来来计算。具体公式如下：</p><p><span class="math display">\[v_t = v_{t-1} + g_t^2\]</span></p><p>其中，<span class="math inline">\(g_t\)</span> 是当前的梯度。</p><span id="more"></span><h2 id="算法流程">算法流程</h2><ol type="1"><li><strong>计算当前梯度 <span class="math inline">\(g_t\)</span></strong>：</li></ol><p><span class="math display">\[g_t = \nabla f(w_t)\]</span></p><ol start="2" type="1"><li><strong>更新二阶动量 <span class="math inline">\(v_t\)</span></strong>：</li></ol><p><span class="math display">\[v_t =  v_{t-1} + g_t^2\]</span></p><ol start="3" type="1"><li><strong>计算当前时刻的下降梯度</strong>：</li></ol><p><span class="math display">\[w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t} + \epsilon} g_t\]</span></p><p>其中，<span class="math inline">\(\alpha\)</span> 是学习率，<spanclass="math inline">\(\epsilon\)</span> 是一个小的平滑项，防止分母为0。</p><h2 id="稀疏特征处理">稀疏特征处理</h2><p>AdaGrad 算法主要针对 <strong>稀疏特征</strong>进行了优化。<strong>稀疏特征</strong>在很多样本中只出现少数几次，在训练模型时，这些稀疏特征的更新很少，但每次更新可能带来较大影响。AdaGrad通过调整每个特征的学习率，针对这种情况进行了优化。</p><h3 id="优缺点">优缺点</h3><p><strong>优点</strong>：</p><ol type="1"><li><strong>有效处理稀疏特征</strong>：自动调整每个参数的学习率，使得稀疏特征的更新更少。<br /></li><li><strong>加速收敛</strong>：在自动调整学习率的同时，使得模型在训练过程中更快收敛。</li></ol><p><strong>缺点</strong>：<br />1.<strong>学习率逐渐减小</strong>：每次迭代中学习率都会减小，导致训练后期学习率变得非常小，从而使收敛速度变慢。<br />2.<strong>固定调整方式</strong>：对于不同参数，学习率调整方式是固定的，无法根据不同任务自动调整。</p><h2 id="代码实现">代码实现</h2><p>下面是一个简单的 PyTorch 实现 AdaGrad 算法的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成一些数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 AdaGrad 优化器</span></span><br><span class="line">optimizer = torch.optim.Adagrad([w, b], lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112049596.png"alt="image-20240811204910528" /><figcaption aria-hidden="true">image-20240811204910528</figcaption></figure><h2 id="总结">总结</h2><p>本节我们介绍了一种新的梯度下降算法变体——AdaGrad。与动量法相比，它最大的改进在于<strong>使用二阶动量来动态调整学习率</strong>，能够记住历史上的梯度信息，以动态调整学习率。其主要优点是能够处理稀疏特征问题，但也有学习率逐渐减小和调整方式固定的缺点。</p><p>到目前为止，我们一共讲了五种梯度下降算法。AdaGrad 是 2011年提出的，而动量法在 1993 年提出，SGD 在 1951年提出。通过时间轴的对比，我们可以看出人们在不断研究和改进梯度下降算法，从最早的梯度下降法到SGD，再到动量法、小批量梯度下降，最后到 2011 年的 AdaGrad。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;adagrad-算法&quot;&gt;AdaGrad 算法&lt;/h1&gt;
&lt;p&gt;在前面我们讲解了 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E5%8A%A8%E9%87%8F%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;动量法&lt;/a&gt;（Momentum），也就是动量随机梯度下降法。它使用了一阶动量。然而，我们同时也提到了二阶动量。使用二阶动量的梯度下降算法的改进版就是本节要讲的
AdaGrad 算法。二阶动量的出现，才意味着真正的
&lt;strong&gt;自适应学习率&lt;/strong&gt; 优化算法时代的到来。&lt;/p&gt;
&lt;h2 id=&quot;adagrad-算法的基本思想&quot;&gt;AdaGrad 算法的基本思想&lt;/h2&gt;
&lt;p&gt;我们先回顾一下传统的 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;amp;spm=1001.2101.3001.7020&quot;&gt;随机梯度下降法&lt;/a&gt;（SGD）及其各种变种。它们都是以
&lt;strong&gt;同样的学习率&lt;/strong&gt; 来更新 &lt;strong&gt;每一个参数&lt;/strong&gt;
的。但深度神经网络往往包含大量参数，这些参数并不总是
&lt;strong&gt;均匀更新&lt;/strong&gt; 的。有些参数更新得频繁，有些则很少更新。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;strong&gt;经常更新&lt;/strong&gt;
的参数，我们已经积累了大量关于它的知识，希望它不被新的单个样本影响太大，也就是说希望对这些参数的
&lt;strong&gt;学习率小一些&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对于 &lt;strong&gt;偶尔更新&lt;/strong&gt;
的参数，我们了解的信息较少，希望从每一个样本中多学一些，即
&lt;strong&gt;学习率大一些&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要动态度量历史更新的频率，我们引入
&lt;strong&gt;二阶动量&lt;/strong&gt;。二阶动量通过将每一位各自的历史梯度的
&lt;strong&gt;平方&lt;/strong&gt; 叠加起来来计算。具体公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = v_{t-1} + g_t^2
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;span class=&quot;math inline&quot;&gt;&#92;(g_t&#92;)&lt;/span&gt; 是当前的梯度。&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>RMSProp 和 Adadelta</title>
    <link href="https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.RMSProp%20%E5%92%8C%20Adadelta/"/>
    <id>https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6.RMSProp%20%E5%92%8C%20Adadelta/</id>
    <published>2024-08-12T23:02:27.000Z</published>
    <updated>2024-11-13T14:14:37.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="rmsprop-和-adadelta-算法">RMSProp 和 Adadelta 算法</h1><p>​ 在 <ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>中，RMSProp 和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad的基础上做了改进，以适应深度学习中的大规模参数优化需求。</p><h2 id="rmsprop-算法">RMSProp 算法</h2><h3 id="基本思想">基本思想</h3><p>RMSProp 对 AdaGrad 进行改进，通过引入 <strong>衰减率</strong>来调整二阶动量的累积。这样可以 <strong>避免</strong> AdaGrad 中<strong>学习率减小过快</strong> 的问题。</p><p>AdaGrad 的二阶动量计算公式如下：</p><p><span class="math display">\[v_t = v_{t-1} + g_t^2\]</span> 而 RMSProp 采用了带有衰减率的计算方式：</p><p><span class="math display">\[v_t = \beta v_{t-1} + (1 - \beta) g_t^2\]</span> 其中，<span class="math inline">\(\beta\)</span>是衰减率系数。</p><span id="more"></span><h3 id="优缺点">优缺点</h3><p><strong>优点：</strong></p><ul><li><strong>自动调整学习率</strong>，避免学习率过大或过小的问题</li><li><strong>加速收敛速度</strong></li><li><strong>简单适用</strong>，适用于各种优化问题</li></ul><p><strong>缺点：</strong></p><ul><li>在处理稀疏特征时不够优秀</li><li>需要调整的超参数较多（衰减率 <spanclass="math inline">\(\beta\)</span> i 和学习率 <spanclass="math inline">\(\alpha\)</span> ）</li><li>收敛速度可能不如某些更先进的 <ahref="https://so.csdn.net/so/search?q=%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95&amp;spm=1001.2101.3001.7020">优化算法</a></li></ul><h3 id="代码实现">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 RMSProp 优化器</span></span><br><span class="line">optimizer = torch.optim.RMSprop([w, b], lr=learning_rate, alpha=beta)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with RMSProp&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="adadelta-算法">Adadelta 算法</h2><h3 id="基本思想-1">基本思想</h3><p>Adadelta 是对 RMSProp 的进一步改进，旨在<strong>自动调整学习率</strong>，避免手动调参。它通过计算梯度和权重更新量的累积值来调整学习率，使得训练过程更加稳定。</p><p>Adadelta 的公式如下：</p><ol type="1"><li>梯度的累积：</li></ol><p><span class="math display">\[E [g^2] _t = \rho E [g^2]_{t-1} + (1 - \rho) g_t^2\]</span></p><ol start="2" type="1"><li>权重更新量的累积：</li></ol><p><span class="math display">\[E [\Delta x^2] _t = \rho E [\Delta x^2]_{t-1} + (1 - \rho) (\Deltax_t)^2\]</span></p><ol start="3" type="1"><li>更新参数：</li></ol><p><span class="math display">\[\Delta x_t = -\frac{\sqrt{E [\Delta x^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}} g_t  \]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_t + \Delta x_t\]</span></p><h3 id="优缺点-1">优缺点</h3><p><strong>优点：</strong><br />- <strong>自动调整学习率</strong>，避免学习率过大或过小的问题 -避免出现学习率饱和现象，使得训练更加稳定</p><p><strong>缺点：</strong></p><ul><li>可能收敛较慢</li><li>需要维护梯度和权重更新量的累积值，增加了空间复杂度</li></ul><h3 id="代码实现-1">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">learning_rate = <span class="number">1.0</span>  <span class="comment"># Adadelta 不需要传统的学习率</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">rho = <span class="number">0.9</span></span><br><span class="line">epsilon = <span class="number">1e-6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机生成数据</span></span><br><span class="line">x = torch.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span> + torch.randn(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = torch.randn(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义 Adadelta 优化器</span></span><br><span class="line">optimizer = torch.optim.Adadelta([w, b], rho=rho, eps=epsilon)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 记录损失</span></span><br><span class="line">losses = []</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_pred = x * w + b</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = torch.mean((y_pred - y) ** <span class="number">2</span>)</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 清空梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可视化训练过程</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(epochs), losses)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Loss with Adadelta&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;rmsprop-和-adadelta-算法&quot;&gt;RMSProp 和 Adadelta 算法&lt;/h1&gt;
&lt;p&gt;​ 在 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt;
中，RMSProp 和 Adadelta 是两种常见的优化算法。它们都是在 AdaGrad
的基础上做了改进，以适应深度学习中的大规模参数优化需求。&lt;/p&gt;
&lt;h2 id=&quot;rmsprop-算法&quot;&gt;RMSProp 算法&lt;/h2&gt;
&lt;h3 id=&quot;基本思想&quot;&gt;基本思想&lt;/h3&gt;
&lt;p&gt;RMSProp 对 AdaGrad 进行改进，通过引入 &lt;strong&gt;衰减率&lt;/strong&gt;
来调整二阶动量的累积。这样可以 &lt;strong&gt;避免&lt;/strong&gt; AdaGrad 中
&lt;strong&gt;学习率减小过快&lt;/strong&gt; 的问题。&lt;/p&gt;
&lt;p&gt;AdaGrad 的二阶动量计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = v_{t-1} + g_t^2
&#92;]&lt;/span&gt; 而 RMSProp 采用了带有衰减率的计算方式：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
v_t = &#92;beta v_{t-1} + (1 - &#92;beta) g_t^2
&#92;]&lt;/span&gt; 其中，&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;beta&#92;)&lt;/span&gt;
是衰减率系数。&lt;/p&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Momentum</title>
    <link href="https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/"/>
    <id>https://cuiluyi.github.io/2024/08/13/[object%20Object]/AI/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4.Momentum/</id>
    <published>2024-08-12T22:02:27.000Z</published>
    <updated>2024-11-13T14:14:31.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="动量法momentum">动量法（Momentum）</h1><h2 id="背景知识">背景知识</h2><p>在 <ahref="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>的优化过程中，梯度下降法（Gradient Descent,GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在<strong>收敛速度慢</strong>、<strong>容易陷入局部最小值</strong> 以及在<strong>高维空间中震荡较大</strong>的问题。为了解决这些问题，人们提出了动量法（Momentum）。</p><h2 id="动量法的概念">动量法的概念</h2><p>动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：<strong>在运动方向上保持运动的趋势，从而加速收敛</strong>。</p><h2 id="动量法在深度学习中的应用">动量法在深度学习中的应用</h2><p>在深度学习中，动量法通过记录 <strong>梯度的增量</strong> 并将其与<strong>当前梯度相加</strong>，来 <strong>平滑梯度下降</strong>的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。</p><p>动量法的更新公式如下：</p><p><span class="math display">\[m_t = \beta m_{t-1} + \nabla L(w_t)\]</span> <span class="math display">\[w_{t+1} = w_t - \alpha m_t\]</span></p><p>其中：</p><ul><li><span class="math inline">\(m_t\)</span>是动量项，记录了之前梯度的累积。<br /></li><li><span class="math inline">\(\beta\)</span> 是动量参数，控制<strong>动量项的衰减</strong>，一般取值为 0.9。<br /></li><li><span class="math inline">\(\nabla L(w_t)\)</span>是当前参数的梯度。</li><li><span class="math inline">\(\alpha\)</span> 是学习率。</li></ul><span id="more"></span><h2 id="动量法的优点">动量法的优点</h2><ol type="1"><li><p><strong>加速收敛</strong>：动量法通过积累之前的梯度信息，使得优化过程更为顺畅，避免了曲折路径，提高了收敛速度。<br /></p></li><li><p><strong>跳过局部最小值</strong>：由于动量的累积作用，可以帮助优化算法跳过一些局部最小值，找到更优的解。<br /></p></li><li><p><strong>减少振荡</strong>：动量法可以有效减小学习过程中梯度震荡的现象，使得模型的训练更加稳定。## 动量法的缺点</p></li><li><p><strong>计算复杂度增加</strong>：由于需要维护动量项，会导致计算复杂度的增加</p></li><li><p><strong>参数调节</strong>：动量法引入了新的超参数（动量系数 <spanclass="math inline">\(\beta\)</span>），需要在实际应用中进行调节</p></li></ol><h2 id="动量法的改进及变种">动量法的改进及变种</h2><p>​ 在动量法的基础上，还有一些改进和变种，如 Nesterov 加速梯度（NesterovAccelerated <ahref="https://so.csdn.net/so/search?q=Gradient&amp;spm=1001.2101.3001.7020">Gradient</a>,NAG）、RMSprop、Adam等。这些方法在动量法的基础上进一步优化了收敛速度和稳定性。</p><h2 id="实验代码示例">实验代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据生成</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">X = torch.randn(<span class="number">1000</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span> * X.squeeze() + <span class="number">2</span> + torch.randn(<span class="number">1000</span>) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同梯度下降方法的比较</span></span><br><span class="line">methods = &#123;</span><br><span class="line">    <span class="string">&#x27;SGD&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>),</span><br><span class="line">    <span class="string">&#x27;Momentum&#x27;</span>: <span class="keyword">lambda</span> params: optim.SGD(params, lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">losses = &#123;method: [] <span class="keyword">for</span> method <span class="keyword">in</span> methods&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> method_name, optimizer_fn <span class="keyword">in</span> methods.items():</span><br><span class="line">    model = LinearModel()</span><br><span class="line">    optimizer = optimizer_fn(model.parameters())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(X)</span><br><span class="line">        loss = criterion(outputs.squeeze(), y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        losses[method_name].append(loss.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line"><span class="keyword">for</span> method_name, loss_values <span class="keyword">in</span> losses.items():</span><br><span class="line">    plt.plot(loss_values, label=method_name)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&#x27;Loss Curve Comparison&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><imgsrc="https://tianchou.oss-cn-beijing.aliyuncs.com/img/202408112029254.png"alt="image-20240811202901191" /><figcaption aria-hidden="true">image-20240811202901191</figcaption></figure><h2 id="结论">结论</h2><p>动量法通过引入动量项，显著提高了 <ahref="https://so.csdn.net/so/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&amp;spm=1001.2101.3001.7020">梯度下降法</a>的收敛速度和稳定性。尽管在实际应用中引入了额外的计算开销，但其在许多深度学习任务中的表现优异，已经成为常用的优化方法之一。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;动量法momentum&quot;&gt;动量法（Momentum）&lt;/h1&gt;
&lt;h2 id=&quot;背景知识&quot;&gt;背景知识&lt;/h2&gt;
&lt;p&gt;在 &lt;a
href=&quot;https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;amp;spm=1001.2101.3001.7020&quot;&gt;深度学习&lt;/a&gt;
的优化过程中，梯度下降法（Gradient Descent,
GD）是最基本的方法。然而，基本的梯度下降法在实际应用中存在
&lt;strong&gt;收敛速度慢&lt;/strong&gt;、&lt;strong&gt;容易陷入局部最小值&lt;/strong&gt; 以及在
&lt;strong&gt;高维空间中震荡较大&lt;/strong&gt;
的问题。为了解决这些问题，人们提出了动量法（Momentum）。&lt;/p&gt;
&lt;h2 id=&quot;动量法的概念&quot;&gt;动量法的概念&lt;/h2&gt;
&lt;p&gt;动量（Momentum）最初是一个物理学概念，表示物体的质量与速度的乘积。它的方向与速度的方向相同，并遵循动量守恒定律。尽管深度学习中的动量与物理学中的动量并不完全相同，但它们都强调了一个概念：&lt;strong&gt;在运动方向上保持运动的趋势，从而加速收敛&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;动量法在深度学习中的应用&quot;&gt;动量法在深度学习中的应用&lt;/h2&gt;
&lt;p&gt;在深度学习中，动量法通过记录 &lt;strong&gt;梯度的增量&lt;/strong&gt; 并将其与
&lt;strong&gt;当前梯度相加&lt;/strong&gt;，来 &lt;strong&gt;平滑梯度下降&lt;/strong&gt;
的路径。这意味着在每一步的迭代中，不仅考虑当前的梯度，还考虑之前梯度的累积效果。&lt;/p&gt;
&lt;p&gt;动量法的更新公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
m_t = &#92;beta m_{t-1} + &#92;nabla L(w_t)
&#92;]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;&#92;[
w_{t+1} = w_t - &#92;alpha m_t
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(m_t&#92;)&lt;/span&gt;
是动量项，记录了之前梯度的累积。&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;beta&#92;)&lt;/span&gt; 是动量参数，控制
&lt;strong&gt;动量项的衰减&lt;/strong&gt;，一般取值为 0.9。&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;nabla L(w_t)&#92;)&lt;/span&gt;
是当前参数的梯度。&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;&#92;(&#92;alpha&#92;)&lt;/span&gt; 是学习率。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="优化算法" scheme="https://cuiluyi.github.io/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
